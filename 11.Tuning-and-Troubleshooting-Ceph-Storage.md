| 编号  | 系统         | 角色                           | IP             | 主机名                 | Ceph版本 |
| --- | ---------- | ---------------------------- | -------------- | ------------------- | ------ |
| 1   | Centos 8.5 | bootstrap，mon，mgr，osd，iscsi  | 192.168.31.185 | ceph01.xiaohui.cn   | Quincy |
| 2   | Centos 8.5 | mon，mgr，osd，rgw，mds          | 192.168.31.129 | ceph02.xiaohui.cn   | Quincy |
| 3   | Centos 8.5 | mon，mgr，rgw，mds              | 192.168.31.25  | ceph03.xiaohui.cn   | Quincy |
| 4   | Centos 8.5 | second ceph cluster，rgwsite2 | 192.168.31.34  | rgwsite2.xiaohui.cn | Quincy |

作者：李晓辉

联系方式：

1. 微信：Lxh_Chat

2. QQ：939958092

3. 邮箱：[xiaohui_li@foxmail.com](mailto:xiaohui_li@foxmail.com) 

# 优化红帽Ceph存储性能

## 定义性能调优

性能调优是裁减系统配置的过程，以便特定的关键应用程序具有最佳的响应时间或吞吐量。Ceph集群的性能调优有三个指标:延迟、IOPS(每秒输入输出操作)和吞吐量。

**延迟**

磁盘延迟和响应时间是同一件事，这是一种常见的误解。磁盘延迟是设备的一个函数，但是响应时间是整个服务器的一个函数，对于使用旋转盘片的硬盘驱动器，磁盘延迟有两个组成部分:

**寻道时间:** 在盘片上将磁头定位到正确轨道所花费的时间，通常为0.2到0.8毫秒

**旋转延迟:** 轨道上正确的起始扇区从磁头下经过所需要的额外时间，通常为几毫秒。在磁头定位好之后，驱动器就可以开始从盘片传输数据了。在这一点上，顺序数据传输速率很重要。对于固态硬盘(ssd)，等效的度量是存储设备的随机访问延迟，通常小于一毫秒。对于非易失性存储器表示驱动器(NVMes)，存储驱动器的随机访问延迟通常以微秒为单位

**每秒操作 (IOPS)**

系统每秒能处理的读写请求数与存储设备的能力和应用有关。当应用程序发出I/O请求时，操作系统将请求传输给设备，并等待直到请求完成。作为参考，使用旋转盘片的硬盘的IOPS在50到200之间，ssd的IOPS在数千到数十万之间，NVMes的IOPS在数十万左右

**吞吐量**

吞吐量指的是系统每秒可以读取或写入的实际字节数。块的大小和数据传输速率会影响吞吐量。磁盘块大小越高，延迟因素衰减得越多。数据传输速率越高，磁盘将数据从其表面传输到缓冲区的速度就越快

作为参考值，使用旋转盘片的硬盘的吞吐量约为150mb /s, ssd约为500mbps, NVMes约为2000mb /s。可以测量网络和整个系统的吞吐量，从远程客户端到服务器

### 调优目标

使用的硬件决定了系统和Ceph集群的性能限制

性能调优的目标是尽可能高效地使用硬件

一个常见的现象是，调优一个特定子系统可能会对另一个子系统的性能产生负面影响。例如，可以以高吞吐量为代价来优化系统以获得低延迟，因此，在开始调优之前，建立与Ceph集群的预期工作负载一致的目标:

**IOPS优化**

块设备上的工作负载通常是IOPS密集型的，例如OpenStack虚拟机上运行的数据库。典型的部署需要高性能SAS驱动器来存储ssd或NVMe设备上的日志

**吞吐量的优化**

RADOS网关上的工作负载通常是吞吐量密集型的。对象可以存储大量的数据，比如音频和视频内容

**容量优化**

需要以尽可能低的成本存储大量数据的工作负载通常以性能换取价格。选择更便宜和更慢的SATA驱动器是这种工作负载的解决方案

根据工作负载的不同，调优目标应该包括:

1. 减少时延

2. 增加设备侧IOPS

3. 增加块大小

## 优化Ceph性能

下面的部分描述了调优Ceph的推荐实践

### Ceph部署

正确规划Ceph集群部署是很重要的。MONs的性能对于整个集群的性能至关重要。对于大型部署，MONs应该位于专用节点上。为了保证仲裁的正确性，需要奇数个MONs

Ceph设计用于处理大量数据，如果使用正确的硬件并正确地调优集群，则可以提高性能

在集群安装之后，开始持续监视集群，以排除故障并安排维护活动。尽管Ceph具有显著的自愈能力，但许多类型的故障事件都需要快速通知和人工干预。如果出现性能问题，请在磁盘、网络和硬件级别进行故障排除。然后，继续诊断RADOS块设备和Ceph RADOS网关

#### OSD的建议

在写BlueStore块数据库和WAL (write-ahead log)时，为了提高效率，建议使用ssd盘或NVMes盘。OSD的数据、块数据库和WAL可以配置在相同的存储设备上，也可以通过对这些组件使用单独的设备来进行非配置

在典型的部署中，osd使用具有高延迟的传统旋转磁盘，因为它们提供了令人满意的指标，以更低的每兆字节成本满足定义的目标。默认情况下，BlueStore osd将数据、块数据库和WAL放在同一个块设备上。但是，可以通过为块数据库和WAL使用单独的低延迟ssd或NVMe设备来最大化效率。多个块数据库和WALs可以共享同一个SSD或NVMe设备，降低存储基础设施成本

考虑以下SSD规格对预期工作负载的影响:

1. 支持仪式数量的平均故障间隔时间(MTBF)

2. IOPS功能

3. 数据传送速率

4. BUS/ SSD几个功能

当一个承载日志的SSD或NVMe设备失效时，每个使用它来承载日志的OSD也将不可用。在决定在同一存储设备上放置多少块数据库或WALs时，请考虑这一点

#### Ceph RADOS网关的建议

RADOS网关上的工作负载通常是吞吐量密集型的。作为对象存储的音频和视频材料可能很大。但是，桶索引池通常显示更I/O密集型的工作负载模式。将索引池存储在SSD设备上。

RADOS网关为每个桶维护一个索引。默认情况下，Ceph将这个索引存储在一个RADOS对象中。当一个桶存储超过100,000个对象时，单个索引对象成为瓶颈，索引性能下降。

Ceph可以在多个RADOS对象或分片中保存大索引。通过设置rgw_override_bucket_index_max可启用该特性。建议为每桶中预期的对象数除以100,000

随着索引的增长，Ceph必须定期重新共享桶。Red Hat Ceph Storage提供了桶索引自动重分片功能。rgw_dynamic_resharding参数(默认为true)控制该特性

#### CephFS的建议

保存目录结构和其他索引的元数据池可能成为CephFS瓶颈。为了最大限度地减少这种限制，元数据池使用SSD设备

每个MDS在内存中为不同类型的项(如inode)维护一个缓存。Ceph通过mds_cache_memory_limit参数限制这个缓存的大小。其默认值(以绝对字节表示)为4gb

## 放置组代数

由于某些OSD节点上不必要的CPU和RAM活动，集群中pg的总数可能会影响整体性能。Red Hat建议在将集群投入生产之前验证每个池的PG分配。还要考虑回填和回收的具体测试，对客户端I/O请求的影响，有两个重要的值:

1. 集群中pg的总数

2. 特定池中可使用的pg数量

使用这个公式来估算一个特定池中可使用的pg数量:

放置组总数= (0SDs * 100) /副本数量

应用每个池的公式可以获得集群的pg总数。Red Hat建议每个OSD 100至200 pg

**Splitting PGs**

Ceph支持增加或减少池中pg的数量。如果在创建池时不指定该值，则创建池时使用默认值8pg，这个值非常低

pg_autoscale_mode属性允许Ceph做出建议，并自动调整pg_num和pgp_num参数。在创建新池时，默认启用此选项。pg_num参数定义特定对象的pg数量。pgp_num参数定义CRUSH算法考虑放置的pg的数量。

红帽建议你增加放置组的数量，直到你达到理想的pg数量。大量增加pg的数量会导致集群性能下降，因为会产生预期的数据，重新定位和再平衡是密集的。

使用ceph osd pool set命令通过设置参数pg_num手动增加或减少pg数量。在禁用pg_autoscale模式选项的情况下，手动增加池中的pg数量时，应该只增加少量的增量

将放置组的总数设置为2的幂可以更好地在osd中分布pg。增加pgp_num参数会自动增加pgp_num参数，但会逐步增加，以减少对集群性能的影响

**PG合并**

Red Hat Ceph Storage可以将两个PG合并成一个更大的PG，减少PG的总数。当池中的pg数量过大且性能下降时，合并可能很有用。因为合并是一个复杂的过程，所以每次只合并一个PG，以尽量减少对集群性能的影响

**PG Auto-scaling**

如前所述，PG自动缩放功能允许Ceph做出建议，并自动调整PG的数量。该特性在创建池时默认启用。对于现有的池，使用这个命令配置自动伸缩:

```bash
[admin@node -)$ ceph osd pool set pool-name pg_autoscale_mode mode
```

将模式参数设置为off以禁用它，设置为on以启用它，并允许Ceph自动调整pg的数量，或在必须调整pg数量时发出警报。

查看自动缩放模块提供的信息:

```bash
[admin@node -)$ ceph osd pool autoscale-status
```

## 设计集群架构

在设计Ceph集群时，考虑扩展选择，以匹配未来的数据需求，并使用正确的网络大小和体系结构促进足够的吞吐量

### 可扩展性

可以通过两种方式扩展集群存储:

1. 通过向集群中添加更多节点向外扩展

2. 通过向现有节点添加更多资源进行扩展

扩展要求节点可以接受更多的CPU和RAM资源，以处理磁盘数量和磁盘大小的增加。向外扩展需要添加具有类似资源和容量的节点，以匹配集群的现有节点，从而实现平衡操作

### 网络的最佳实践

连接Ceph集群中节点的网络对于良好的性能至关重要，因为所有客户端和I/O集群操作都使用它。红帽公司推荐以下做法:

1. 为了提高性能和提供更好的故障排除隔离，OSD流量和客户端流量使用单独的网络

2. 存储集群的网络容量不小于10gb。1gb组网不适合生产环境

3. 根据集群和客户端流量以及存储的数据量评估网络大小

4. 强烈建议进行网络监控

5. 在可能的情况下，使用单独的网卡连接到网络，或者使用单独的端口

![](https://gitee.com/cnlxh/ceph/raw/master/images/tuning/tuning-tuning-cephperf-network.svg)

Ceph守护进程自动绑定到正确的接口，例如将MONs绑定到公共网络，将osd绑定到公共网络和集群网络

## 手动控制PG的主OSD

使用主亲和性设置来影响Ceph选择一个特定的OSD作为放置组的主OSD。设置越高，一个OSD被选择为主OSD的可能性越大。可以通过配置集群来避免主OSD使用慢盘或控制器来缓解问题或瓶颈。ceph osd primary-affinity命令用于修改osd的主亲和性。亲和力是0和1之间的一个实数

```bash
[admin@node -)$ ceph osd primary-affinity osd-number affinity 
```

### OSD恢复与回填

当Ceph在集群中添加或移除一个OSD时，Ceph会对pg进行重新平衡，使用新的OSD或重新创建存储在被移除的OSD中的副本。这些回填和恢复操作会产生较高的集群网络流量负载，从而影响性能。

为避免集群性能下降，请调整回填和恢复操作，使重新平衡与集群正常操作之间的关系。Ceph提供参数来限制回填和回收操作的1/0和网络活动。

以下列表包括其中一些参数:

| 参数                          | 定义                 |
| --------------------------- | ------------------ |
| osd_recovery_op_ priority   | 恢复操作优先级            |
| osd_recovery_max_active     | 每个OSD的最大并发恢复请求数    |
| osd_recovery_threads        | 用于数据恢复的线程数         |
| osd_max_backfills           | 单个OSD的最大回填数        |
| osd_backfill_scan_min       | 每次回填扫描的最小对象数       |
| osd_backfill_scan_max       | 每次回填扫描的最大对象数       |
| osd_backfill_full_ratio     | 向OSD回填请求的阈值        |
| osd_backfill_retry_interval | 在重新尝试回填请求之前，需要等待几秒 |

## 配置硬件

对集群的预期工作负载使用现实的指标，构建集群的硬件配置，以提供足够的性能，但保持成本尽可能低。Red Hat建议将这些硬件配置用于以下三个性能优先级:

**IOPS优化**

1. 每个NVMe设备使用两个osd

2. NVMe驱动器将数据、块数据库和WAL配置在同一个存储设备上

3. 假设有一个2ghz的CPU，每个NVMe使用10个核，或者每个SSD使用2个核

4. 分配16gb内存作为基准，每个OSD加5gb内存

5. 每2个osd使用10gbe网卡

**吞吐量的优化**

1. 每个硬盘使用一个OSD

2. 将块数据库和WAL放置在ssd或nvme上

3. 使用至少7200 RPM的硬盘驱动器

4. 假设有一个2ghz的CPU，每个HDD使用半个核

5. 分配16gb内存作为基准，每个OSD加5gb内存

6. 每12个osd使用10个GbE网卡

**容量优化**

1. 每个硬盘使用一个OSD

2. hdd将数据、块数据库和WAL配置在同一个存储设备上

3. 使用至少7200 RPM的硬盘驱动器

4. 假设有一个2ghz的CPU，每个HDD使用半个核

5. 分配16gb内存作为基准，每个OSD加5gb内存

6. 每12个osd使用10个GbE网卡

## 使用Ceph性能工具进行调优

性能工具提供基准度量来检查集群的性能问题

### 性能计数器和指标

每个Ceph守护进程都维护一组内部计数器和仪表。有几个工具可以访问这些计数器:

**仪表板插件**

Dashboard插件公开了一个可以在端口8443上访问的web界面。“集群OSD”菜单提供OSD的基本实时统计信息，如读字节数、写字节数、读操作次数、写操作次数等。使用ceph mgr模块Enable Dashboard命令启用Dashboard插件。如果您使用cephadm bootstrap命令引导集群，那么默认情况下仪表板是启用的

**Manager (MGR) Prometheus 插件**

该插件在端口9283上公开性能指标，供外部Prometheus服务器收集。Prometheus是一个开源的系统监视和警报工具

**ceph命令行工具**

ceph命令具有查看指标和更改守护进程参数的选项

## 性能压力工具

Red Hat Ceph Storage提供了对Ceph集群进行压力测试和基准测试的工具。

### RADOS bench命令

RADOS bench是一个简单的测试RADOS对象存储的工具。它在集群上执行写和读测试，并提供统计数据。该命令的通用语法如下:

```bash
[admin@node -]$ rados -p pool-name bench seconds write|seq|rand -b objsize -t concurrency
```

以下是该工具的常用参数:

1. seq和rand测试是顺序和随机读取基准测试。这些测试要求首先运行写入基准测试，并使用--no-cleanup选项。默认情况下，RADOS平台会删除为编写测试创建的对象。--no-cleanup选项将保留这些对象，这对于在相同的对象上执行多个测试非常有用

2. 默认的对象大小是4mb

3. 默认并发数为16

使用--no-cleanup选项，在运行rados bench命令后，必须手动删除池中保留的数据

例如，rados bench命令提供的吞吐量、IOPS、时延信息如下:

```bash
[ceph: root@server /]# rados bench -p testbench 10 write --no-cleanup hints = 1
```

### RBD bench命令

RBD测试在为测试创建的现有映像上测量I/O的吞吐量和延迟

这些是默认值:

1. 如果不为size参数提供后缀，则该命令假定以字节为单位

2. 默认池名为rbd

3. -io-size的默认值是4096字节

4. --io-threads的默认值为16

5. --io-total的默认值是1gb

6. --io-模式的默认值是seq

例如，rbd bench命令提供的信息包括吞吐量和延迟:

```bash
[ceph: root@server /]# rbd bench --io-type write testimage --pool=testbench 
```

# 对象存储集群性能调优

## 维护OSD性能

良好的客户端性能要求在其物理限度内使用OSDs。为了保持OSD性能，评估以下调优机会:

1. 调优OSD使用的BlueStore后端，以便将对象存储在物理设备上

2. 调整自动数据擦洗和深度擦洗的时间表

3. 调整异步快照修整(删除已删除的快照)时间表

4. 控制当OSDs失败或添加或替换时，回填和恢复操作发生的速度

## 在Ceph BlueStore上存储数据

OSD守护进程的默认后端对象存储是BlueStore。以下列表描述了使用BlueStore的一些主要特性:

**直接管理存储设备**

BlueStore消耗原始块设备或分区。这简化了存储设备的管理，因为不需要其他抽象层，比如本地文件系统

**高效的写时复制**

Ceph块设备和Ceph文件系统快照依赖于在BlueStore中高效实现的写时复制克隆机制。这将为常规快照和依赖克隆实现高效两阶段提交的erasure-coded 池带来高效的I/O

**没有大型双重写入**

BlueStore首先将任何新数据写入块设备上未分配的空间，然后提交一个Roeks DB事务，该事务更新对象元数据以引用磁盘的新区域

**多设备支持**

BlueStore可以使用多个块设备来存储数据、元数据和预写日志

在BlueStore中，原始分区以bluestore_min_alloc_size变量指定的大小块管理。对于hdd和ssd，bluestore_min_alloc_size默认设置为4096，相当于4 KB。如果要写入原始分区的数据小于块大小，那么它将被填充为0。如果块大小不适合工作负载(例如编写许多小对象)，则可能会导致浪费未使用的空间

Red Hat建议设置bluestore_ min_alloc_size变量来匹配最小的通用写操作，以避免浪费未使用的空间。例如，如果客户端经常写入4 KB的对象，那么在OSD节点上配置设置，例如bluestore_min_alloc_size = 4096。如果之前用bluestore_min_alloc_size_ssd或bluestore_min_a lloc_size_hdd变量设置了bluetore_min_alloc_size_hdd变量，那么设置bluetore_min_alloc_size变量将覆盖HOD或SSD的特定设置

使用ceph config命令设置bluestore_min_alloc_size变量的值:

```bash
[root@node -]# ceph config set osd.ID bluestore_min_alloc_size_device-type value 
```

## BlueStore碎片化工具

随着时间的推移，OSD的空闲空间会变得碎片化。分片正常，但分片过多会降低OSD性能。使用BlueStore时，使用BlueStore碎片化工具检查碎片级别。BlueStore碎片化工具生成一个BlueStore OSD的碎片级别分数。碎片化评分在0 ~ 1之间，0为无碎片化，1为重度碎片化。

作为参考，O和0.7之间的值被认为是小的和可接受的碎片，0.7和0.9之间的分数是可观的，但仍然是安全的碎片，高于0.9的分数表明严重的碎片导致性能问题。

使用BlueStore碎片化工具查看碎片评分:

```bash
[root@node -]# ceph daemon osd.ID bluestore allocator score block
```

## 维护数据一致性和擦洗

osd负责验证数据一致性，使用轻洗净和深洗净。轻擦洗验证对象的存在、校验和和大小。深度擦洗读取数据并重新计算和验证对象的校验和。默认情况下，Red Hat Ceph Storage每天执行轻度擦洗，每周执行深度擦洗。但是，Ceph可以在任何时候开始擦洗操作，这可能会影响集群的性能。可以使用ceph osd set nose rub和ceph osd unset nose rub命令启用或禁用集群级光擦除。尽管清除操作会影响性能，但Red Hat建议启用该特性，因为它可以维护数据完整性。Red Hat建议设置刷洗参数，以将刷洗限制在工作负载最低的已知时间段内

默认配置允许在白天的任何时间轻擦洗

### 轻擦洗

通过在ceph的[osd]部分添加参数来调整轻刷洗过程。例如，使用osd_scrub_begin_hour参数来设置时间

扫描开始，从而避免在工作负载高峰期间进行轻度扫描。轻扫描特性有以下调优参数:osd_scrub_begin_hour = begin_ hour:: begin_ hour参数指定开始扫描的时间。有效值从0到23。如果该值设置为0，且osd_scrub_end_hour也为0，则全天都允许擦洗

**osd_scrub_end_hour = end_hour**

end hour参数指定停止擦洗的时间。取值范围为0 ~ 23。如果该值设置为0，并且osd_scrub_begin_hour也为0，那么全天都允许扫描。

**osd_scrub_load_threshold**

如果系统负载低于阈值(由get loadavg () / number online CPUs参数定义)，则执行擦洗。默认值为0.5

**osd scrub_min_interval**

如果负载低于osd scrub_ load_threshold参数中设置的阈值，执行刷洗的频率不超过该参数中定义的秒数。缺省值为1天

**osd_scrub_interval_randomize_ratio**

在参数“osd_scrub min interval ”中定义的值上添加一个随机延迟。默认值为0.5

**osd_scrub_ max_interval**

无论负载如何，在执行擦洗之前，不要等待超过这个时间。缺省值为7天

**osd_scrub_priority**

通过该参数设置擦洗操作的优先级。缺省值为5。该值相对于osd_client_op_priority的值，后者的默认优先级更高，值为63

### 深层擦洗

ceph osd set nodeep-scrub和ceph osd unset nodeep-scrub命令用于在集群级别启用和禁用深度扫描。可以通过将深度扫描参数添加到ceph配置文件的[osd]部分来配置深度扫描参数。与轻擦洗参数一样，对深擦洗配置的任何更改都会影响集群性能。以下参数是调优深度擦洗最关键的参数:

**osd_deep_scrub_interval**

深度擦洗的间隔时间。缺省值为7天

**osd_scrub_sleep**

在深度刷洗硬盘读取之间引入暂停。增加该值可以降低擦洗操作的速度，并降低对客户端操作的影响。缺省值为0

可以使用一个外部调度程序来实现轻扫描和深扫描，使用以下命令:

ceph pg dump命令在last _SCRUB和last_DEEP_SCRUB列中显示最后一次轻擦洗和深擦洗

ceph pg scrub pg-id命令在特定的pg上安排深度磨砂

ceph pg deep-scrub pg-id命令在一个特定的pg上安排深磨砂

使用ceph osd pool set pool-name parameter value命令设置指定池的参数

### 池擦写参数

还可以使用这些池参数在池级控制轻擦洗和深擦洗:

**noscrub**

如果设置为true, Ceph不会轻擦洗池。默认值为false

**nodeep-scrub**

如果设置为true, Ceph不会深度擦洗池。默认值为false

**scrub_min_interval**

擦洗的次数不要超过该参数中定义的秒数。如果设置为默认0，则Ceph使用osd_scrub_min_interva l全局配置参数

**scrub_ max_interval**

在擦洗池之前，等待的时间不要超过该参数中定义的周期。如果设置为默认0,Ceph使用osd_scrub_max_interval全局配置参数

**deep_ scrub_interval**

深度擦洗的间隔时间。如果设置为默认0,Ceph将使用Osd_deep_scrub_interval全局配置参数

## 裁剪快照和osd

快照在pool和RBD级别都是可用的。当删除快照时，Ceph将快照数据的删除安排为异步操作，称为快照修整

为了减少快照修整过程对集群的影响，可以在删除每个快照对象后设置暂停。通过使用osd snap_trim_sleep参数配置此暂停，该参数是允许下一次快照微调操作之前等待的秒数。该参数的默认值为0。请根据实际环境设置，联系红帽技术支持进行设置

使用osd_snap_trim_priority参数控制快照修剪进程，该参数的默认值为5

## 控制回填和恢复

为了限制回填和恢复操作对集群的影响，保持集群的性能，有必要对回填和恢复操作进行控制

当有新的OSD加入集群，或者当一个OSD死亡，Ceph将其pg重新分配给其他OSD时，就会进行回填。当发生这样的事件时，Ceph会在可用的osd上创建对象副本

当Ceph OSD变得不可访问并恢复在线时，就会发生恢复，例如由于短时间的中断。OSD进入恢复模式，获取最新的数据副本

可以通过以下参数管理回填和回收操作:

**osd_rnax_backfills**

控制每个OSD的最大回填次数。缺省值为1

**osd_recovery_rnax_active**

控制每个OSD的最大并发恢复次数。缺省值为3

**osd_recovery_op_priority**

设置恢复优先级。取值范围为1 ~ 63。数字越高，优先级越高。缺省值为3

# 集群与客户端故障处理

## 开始故障排除

支持Ceph集群的硬件随着时间的推移会出现故障。集群中的数据变得碎片化，需要维护。应该在集群中执行一致的监视和故障排除，以保持集群处于健康状态

### 识别问题

在对Ceph问题进行故障排除时，第一步是确定是哪个Ceph组件导致了问题。有时，可以在ceph运行状况detailorceph运行状况status命令提供的信息中找到此组件。其他时候，必须进一步调查以发现问题。验证集群的状态，以帮助确定是单个故障还是整个节点故障

以下故障排除清单建议接下来的步骤:

1. 确定导致问题的Ceph组件

2. 为标识的组件设置调试日志并查看日志

3. 验证拥有一个受支持的配置

4. 确定是否有缓慢或卡住的操作

### 故障排除集群健康

Red Hat Ceph Storage持续运行各种健康状况检查，以监控集群的健康状况。当运行状况检查失败时，集群运行状况状态将更改为HEALTH_WARN或HEALTH_ERR，具体取决于运行状况检查失败的严重程度和影响。Red Hat Ceph Storage还将健康检查警告和错误记录到集群日志中。ceph status和ceph health命令显示集群运行状况状态。当集群健康状态为HEAL TH_WARN或HEAL TH_ERR时，使用ceph health detail命令查看健康检查消息，以便可以开始对问题进行故障排除

```bash
[ceph: root@node /]# ceph health detail
```

一些运行状况状态消息指示某个特定问题;其他人则提供了更一般的指示。例如，如果群集运行状况状态更改为HEALTH_ WARN，并且看到运行状况消息HEALTH_WARN 1 osds down Degraded data redundancy，那么这就是问题的明确指示

其他运行状况状态消息可能需要进一步的故障排除，因为它们可能指示几个可能的根本原因。例如，以下消息表示一个问题有多种可能的解决方案

```bash
[ceph: root@node /]# ceph health detail
```

可以通过更改指定池的pg num设置来解决这个问题，或者通过重新配置pg autosca ler模式设置从warn变为on，以便Ceph自动调整pg的数量

当集群性能健康状况检查失败时，Ceph会发送有关性能的健康状况消息。例如，OSD之间通过发送心跳ping消息来监控OSD守护进程的可用性。Ceph还使用OSD的ping响应时间来监控网络性能。一个失败的OSD ping消息可能意味着来自某个特定OSD的延迟，表明该OSD存在潜在的问题。多个OSD的ping消息失败可能是网络组件故障，如OSD主机间网络切换

### 屏蔽Ceph健康警报

可能希望暂时关闭一些集群警告，因为已经知道它们，而且还不需要修复它们。例如，如果你关闭一个OSD进行维护，那么集群会报告一个HEALTH_WARN状态。可以静音此警告消息，以便健康检查不会影响报告的整体状态。Ceph通过健康检查码指定健康检查警报。例如，前面的HEALTH_ WARN消息显示了POOL_TOO_FEW_PGS运行状况代码。要使运行状况警报消息静音，请使用ceph health命令

```bash
[ceph: root@node /)# ceph health mute health-code [duration]
```

健康代码是ceph health detail命令提供的代码。可选参数duration是静音运行状况消息的时间，以秒、分钟或小时为单位指定。可以使用ceph health unmute heal th-code取消健康信息的静音，当您健康消息设置静音时，如果健康状态进一步降级，Ceph将自动取消警报的静音。例如，如果集群报告一个OSD故障，将该警报设置为静音，如果另一个OSD故障，Ceph将自动移除静音。任何可测量的运行状况警报都将取消静音

## 配置日志记录

如果集群的某个特定区域出现问题，那么可以为该区域启用日志记录。例如，如果osd运行正常，但的元数据服务器没有正常运行，请为特定的元数据服务器实例启用调试日志记录。根据需要为每个子系统启用日志记录。向Ceph配置添加调试通常是在运行时临时完成的。如果在启动集群时遇到问题，可以将Ceph调试日志记录添加到Ceph配置数据库中。查看默认路径“/var/log/Ceph”下的Ceph日志文件。Ceph将日志存储在基于内存的缓存中。

### 理解Ceph日志

在运行时使用Ceph命令配置Ceph日志记录。如果在启动集群时遇到错误，那么可以更新Ceph配置数据库，以便它在启动期间进行日志记录。

您可以为集群中的每个子系统设置不同的日志记录级别。调试级别在1到20之间，其中1表示简洁，20表示详细。

Ceph不发送基于内存的日志到输出日志，除了以下情况:

1. 一个致命的信号出现了

2. 代码中的断言被触发

3. 你的请求

要对输出日志级别和内存级别使用不同的调试级别，请使用斜杠(/)字符。例如，debug_mon = 1/5设置ceph-mon守护进程的输出日志级别为1，内存日志级别为5

### 在运行时配置日志

要在运行时激活调试输出，请使用ceph tell命令

```bash
[ceph: root@node /]# ceph tell type.id config set debug_subsystem debug-level
```

type和id参数是Ceph守护进程的类型及其id。该子系统为需要修改调试级别的具体子系统

这个例子修改了Ceph组件间消息系统的OSD O调试级别:

```bash
[ceph: root@node /]# ceph tell osd.0 config set debug_ms 5 
```

在运行时查看配置设置如下:

```bash
[ceph: root@node /)# ceph tell osd.0 config show
```

### 在配置数据库中配置日志

配置子系统调试级别，以便它们在引导时记录到默认日志文件中。使用Ceph config set命令将调试设置添加到Ceph配置数据库中

例如，通过在Ceph配置数据库中设置以下参数，为特定的Ceph守护进程添加调试级别:

```bash
[ceph: root@node /]# ceph config set global debug_ms 1/5 
[ceph: root@node /)# ceph config set osd debug_ms 1 
[ceph: root@node /]# ceph config set osd debug_osd 1/5 
[ceph: root@node /]# ceph config set mon debug_mon 20 
```

### 设置日志文件轮转

Ceph组件的调试日志是资源密集型的，可以生成大量的数据。如果磁盘几乎满了，那么可以通过修改/etc/logrotate.d/ceph上的日志旋转配置来加速日志轮转，Cron作业调度器使用此文件调度日志轮换

可以在轮转频率之后添加一个大小设置，这样当日志文件达到指定的大小时就会进行轮转:

```bash
rotate 7 
weekly 
size size 
compress 
sharedscripts 
```

使用crontab命令添加一个检查/etc/logrotate.d/ceph文件

```bash
[ceph: root@node /]# crontab -e 
```

例如，可以指示Cron检查/etc/logrotate./ceph每30分钟

```bash
30 * * * * /usr/sbin/logrotate /etc/logrotate.d/ceph > /dev/null 2>&1
```

## 故障诊断网络问题

Ceph节点使用网络相互通信。当osd被报告为down时，网络问题可能是原因。带有时钟偏差的监视器是网络问题的常见原因。时钟歪斜，或计时歪斜，是同步数字电路系统中的一种现象，在这种现象中，相同的源时钟信号在不同的时间到达不同的元件。如果读数之间的差异与集群中配置的数据相差太远，那么就会出现时钟倾斜错误。该错误可能会导致丢包、延迟或带宽受限，影响集群的性能和稳定性

下面的网络故障排除清单建议下一步步骤:

1. 确保集群中的cluster_network和public_network参数包含正确的值。可以通过使用ceph config get mon cluster_network或ceph config get mon public_network命令检索它们的值，或通过检查ceph.conf文件

2. 检查所有网络接口是否正常

3. 验证Ceph节点，并验证它们能够使用它们的主机名相互连接，如果使用防火墙，确保Ceph节点能够在适当的端口上相互连接。打开适当的端口，如有必要，重新安装端口

4. 验证主机之间的网络连接是否有预期的延迟，并且没有丢包，例如，使用ping命令

5. 连接较慢的节点可能会减慢较快节点的速度。检查交换机间链路是否能够承受已连接节点的累计带宽

6. 验证NTP在集群节点中运行正常。例如，可以查看chronyc tracking命令提供的信息

## Ceph客户端故障处理

下面列出了客户端在访问Red Hat Ceph存储集群时遇到的最常见问题:

1. 客户机无法使用监视器(MONs)

2. 使用CLI导致的不正确或缺少命令行参数

3. /etc/ceph/ceph.conf不正确、丢失或不可访问

4. 密钥环文件不正确、丢失或不可访问

ceph-common包为rados、ceph、rbd和radosgw-admin命令提供bash选项卡补全。在shell提示符下输入命令时，可以通过按Tab键来访问选项和属性补全

### 启用和修改日志文件

在对客户端进行故障诊断时，请提高日志级别

在客户端系统中，可以通过ceph config set client debug_ms 1命令将debug_ms = 1参数添加到配置数据库中。Ceph客户端将调试信息保存在“/var/log/Ceph/Ceph-client.id.log的日志文件

大多数Ceph客户机命令，例如rados、Ceph或rbd，也接受- -debug -ms=1选项，以只执行日志级别增加的命令

### 启用客户端管理套接字

默认情况下，Ceph客户端在启动时创建一个UNIX域套接字。可以使用此套接字与客户机通信，以检索实时性能数据或动态获取或设置配置参数

在/var/run/ceph/fsid目录中，有该主机的admin套接字列表。允许每个OSD一个管理套接字，每个MON一个套接字，每个MGR一个套接字。管理员可以使用附带- -admin-daemon socket-patch选项的ceph命令查询通过的客户端套接字

```bash
[ceph: root@node /]# sudo ls -al /var/run/ceph/fsid 
```

下面的示例使用FUSE客户端挂载一个CephFS文件系统，获取性能计数器，并将debug_ms配置参数设置为1:

```bash
[root@host ~]# ceph-fuse -n client.admin /mnt/mountpoint 
[root@host ~]# ls /var/run/ceph/2ae6d05a-229a-11ec-925e-52540000fa0c ceph-client.admin.54240.94381967377904.asok 
[root@host ~]# ceph --admin-daemon /var/run/ceph/ceph-client.admi.54240.94381967377904.asok perf dump
[root@host ~]# ceph --admin-daemon /var/run/ceph/ceph-client.admin.54240.94381967377904.asok config show
[root@host -]# ceph --admin-daemon /var/run/ceph/ceph-client.admin.54240.94381967377904.asok config set debug_ms 5
[root@host ~]# ceph --admin-daemon /var/run/ceph/ceph-client.admin.54240.94381967377904.asok config show
```

### 比较Ceph版本和功能

早期版本的Ceph客户端可能无法从已安装的Ceph集群提供的特性中获益。例如，较早的客户机可能无法从erasure-coded池检索数据。因此，在升级Ceph集群时，还应该更新客户机。RADOS网关、用于CephFS的FUSE客户端、librbd或命令行，例如RADOS或RBD，都是Ceph客户端的例子。

在客户端，你可以通过Ceph versions命令找到正在运行的Ceph集群的版本:

```bash
[ceph: root@node /]# ceph versions
```

还可以使用ceph features命令列出支持的特性级别。如果无法升级客户端，ceph osd set-require-min -compat-client version-name指定ceph集群支持的最小客户端版本，使用这个最小的客户端设置，Ceph拒绝使用与当前客户端版本不兼容的特性

使用ceph osd命令验证集群所需的最小版本:

```bash
[ceph: root@node /]# ceph osd get-require-min-compat-client 
luminous 
```

### 使用Cephx

Red Hat Ceph Storage提供了用于加密认证的cepphx协议。如果启用了Cephx，那么Ceph将在默认的/etc/ceph/路径中查找密钥环。要么为所有组件启用Cephx，要么完全禁用它。Ceph不支持混合设置，比如为客户端启用cepphx，但为Ceph服务之间的通信禁用它。默认情况下，启用了Cephx，当客户端试图访问Ceph集群时，如果没有Cephx，就会收到错误消息

所有Ceph命令都作为客户端进行身份验证。默认为admin用户，但可以使用--name和--ID选项指定用户名或用户ID

Cephx的问题通常与以下方面有关:

1. 对钥匙圈或/etc/ceph/ceph.conf的权限不正确

2. 丢失钥匙圈和/etc/ceph/ceph.conf文件

3. 给定用户的cephx权限不正确或无效。使用ceph认证列表命令来识别问题

4. 不正确或拼写错误的用户名，也可以使用ceph auth list命令来验证

## Ceph监视器故障排除

可以通过ceph运行状况详细信息命令或查看ceph日志提供的信息来识别错误消息

以下是最常见的Ceph MON错误消息列表:

**mon.X is down (out of quorum)**

如果Ceph MON守护进程没有运行，则会出现一个错误，阻止该守护进程启动。例如，可能是守护进程有一个损坏的存储，或者/var分区已满。

如果Ceph MON守护进程正在运行，但被报告为关闭，那么原因取决于MON的状态。如果Ceph MON处于探测状态的时间长于预期，那么它就无法找到其他Ceph监视器。这个问题可能是由网络问题引起的，或者Ceph Monitor可能有一个过时的Ceph Monitor map (monmap)试图在不正确的IP地址上到达其他Ceph Monitor。

如果Ceph MON处于e lee ting状态的时间超过预期，那么它的时钟可能不会同步。如果状态从同步变为eleeting，那么这意味着Ceph MON生成映射的速度比同步进程能够处理的速度要快。

如果状态是leader or peon,，那么Ceph Mon已经达到法定人数，但集群的其他成员不承认法定人数。这个问题主要是由时钟故障引起的同步异常、网络故障或NTP同步异常

**clock skew**

这个错误消息表明MON的时钟可能没有被同步。mon_clock_drift_allowed参数控制集群在显示警告消息之前允许的时钟之间的最大差值。主要是由于时钟同步失败、网络故障或NTP同步异常等原因造成的

**mon.X store is getting too big**

当存储太大并延迟对客户端查询的响应时，Ceph MON会显示此警告消息

## Ceph OSDs故障处理

使用ceph status命令查看监视器的仲裁。如果群集显示健康状态，则群集可以组成仲裁。如果没有监视器仲裁，或者监视器状态出现错误，请首先解决监视器问题，然后继续验证网络

以下是最常见的Ceph OSD错误消息列表:

**full osds**

当集群达到mon_osd_full_ratio参数设置的容量时，Ceph返回HEALTH_ERR full osds消息。默认设置为0.95，即集群容量的95%。

使用ceph df命令确定已使用的原始存储的百分比，由% raw used列给出。如果裸存储占比超过70%，则可以删除不必要的数据，或者通过增加OSD来扩展集群来减少裸存储

**nearfull osds**

当集群达到由mon_osd_nearfull_ratio默认参数设置的容量时，Ceph返回nearfull osds消息。默认值为0.85，即集群容量的85%。

产生此警告消息的主要原因是:

1. 集群OSD间OSD数不均衡

2. 基于OSD数量、用例、每个OSD的目标pg数和OSD利用率，放置组计数不正确

3. 集群使用不成比例的CRUSH可调项

4. osd的后端存储几乎满了

要解决此问题:

1. 验证PG计数是否足够

2. 确认您使用了集群版本最优的CRUSH可调项，如果不是，请调整它们

3. 根据利用率修改osd的权重

4. 确定osd使用的磁盘上剩余的空间

**osds are down**

当osds down或flapping时，Ceph返回osds是down消息。该消息的主要原因是ceph-osd的某个进程故障，或者与其他osd的网络连接出现问题

## RADOS网关故障处理

可以排除Ceph RESTful接口和一些常见的RADOS网关问题

### 调试Ceph RESTful接口

radosgw守护进程是一个Ceph客户端，它位于Ceph集群和HTTP客户端之间。它包括自己的网络服务器Beast，它支持HTTP和HTTPS。

如果发生错误，应该查看/var /log/ceph/文件夹中的日志文件

要将日志记录到文件中，请将log_to_file参数设置为true。可以通过log_file和debug参数更新日志文件的位置和日志级别。还可以在Ceph配置数据库中启用rgw_enable_ops_ log和rgw_ enable_usage_ log参数，分别记录每次成功的RADOS网关操作和使用情况

```bash
[ceph: root@node /]# ceph config set client.rgw log_file /var/log/ceph/ceph-rgw-node.log 
[ceph: root@node /]# ceph config set client.rgw log_to_file true 
[ceph: root@node /]# ceph config set client.rgw debug_rgw 20 
[ceph: root@node /]# ceph config set client.rgw rgw_enable_ops_log true 
[ceph: root@node /)# ceph config set global rgw_enable_usage_log true
```

使用radosgw-admin log list命令查看调试日志。该命令提供可用的日志对象列表。使用radosgwadmin log show命令查看日志文件信息。若要直接从日志对象检索信息，请在对象ID中添加- -object参数。如果要检索具有时间戳的桶的信息，可以添加--bucket、--date和--bucket- ID参数，这些参数分别表示桶名、时间戳和桶ID

### 常见的RADOS网关问题

在RADOS网关中最常见的错误是客户端和RADOS网关之间的时间倾斜，因为S3协议使用日期和时间来签署每个请求。为了避免这个问题，在Ceph和客户节点上都使用NTP

可以通过在RADOS网关日志文件中查找HTTP状态行来验证RADOS网关请求完成的问题

RADOS网关是一个Ceph客户端，它将所有配置存储在RADOS对象中。保存配置数据的RADOS pg组必须处于active +clean状态。如果状态不是active+clean，那么如果主OSD无法提供数据服务，Ceph I/O请求就会挂起，HTTP客户端最终会超时。使用ceph健康详细信息命令识别未激活的pg

## CephFS故障排除

CephFS元数据服务器(MDS)维护一个与其客户端、FUSE或内核共享的缓存，以便MDS可以将其缓存的一部分委托给客户端。例如，访问inode的客户机可以在本地管理和缓存对该对象的更改。如果其他客户端也请求访问同一个节点，MDS可以请求第一个客户端用新的元数据更新服务器

为了保持缓存的一致性，MDS需要与客户端建立可靠的网络连接

Ceph可以自动断开或驱逐没有响应的客户端。发生这种情况时，未刷新的客户端数据将丢失

当客户端试图获得对CephFS的访问时，MDS请求具有当前能力的客户端释放它们。如果客户机没有响应，那么CephFS会在超时后显示一条错误消息。可以使用ceph fs set命令使用session_timeout属性配置超时时间。缺省值是60秒

session_autoc丢失属性控制退出。如果客户端与MDS的通信失败时间超过默认的300秒，则MDS将客户端逐出

Ceph暂时禁止被驱逐的客户端，以便他们不能重新连接。如果出现这种禁止，您必须重新引导客户端系统或卸载并重新挂载文件系统才能重新连接
