| 编号  | 系统         | 角色                    | IP             | 主机名              | Ceph版本 |
| --- | ---------- | --------------------- | -------------- | ---------------- | ------ |
| 1   | Centos 8.5 | bootstrap，mon，mgr，osd | 192.168.30.130 | host1.xiaohui.cn | Quincy |
| 2   | Centos 8.5 | mon，mgr，osd，rgw，mds   | 192.168.30.131 | host2.xiaohui.cn | Quincy |
| 3   | Centos 8.5 | mon，mgr，rgw，mds       | 192.168.30.132 | host3.xiaohui.cn | Quincy |

作者：李晓辉

联系方式：

1. 微信：Lxh_Chat

2. QQ：939958092

3. 邮箱：[xiaohui_li@foxmail.com](mailto:xiaohui_li@foxmail.com)

# 使用逻辑卷创建 BlueStore OSD

## BlueStore 简介

BlueStore 取代 FileStore 作为 OSD 的存储后端，FileStore 现已弃用，FileStore 将对象存储为块设备基础上的文件系统（通常是 XFS）中的文件。BlueStore 将对象直接存储在原始块设备上，免除了对文件系统层的需要，从而提高了读写操作速度

## 停止自动应用osd

前面我们已经添加了所有硬盘到集群中，而且添加了--all-available-devices选项，将会导致我们删除osd后再次重新创建，我们先停止集群自动创建osd

```bash
[ceph: root@host1 /]# ceph orch apply osd --all-available-devices --unmanaged=true
Scheduled osd.all-available-devices update...
```

## 添加OSD到集群

先查看现有osd情况

```bash
[ceph: root@host1 /]# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME       STATUS  REWEIGHT  PRI-AFF
-1         4.88297  root default                             
-5         1.95319      host host1                           
 1    ssd  0.48830          osd.1       up   1.00000  1.00000
 8    ssd  0.48830          osd.8       up   1.00000  1.00000
11    ssd  0.48830          osd.11      up   1.00000  1.00000
12    ssd  0.48830          osd.12      up   1.00000  1.00000
-7         1.95319      host host2                           
 2    ssd  0.48830          osd.2       up   1.00000  1.00000
 4    ssd  0.48830          osd.4       up   1.00000  1.00000
 7    ssd  0.48830          osd.7       up   1.00000  1.00000
10    ssd  0.48830          osd.10      up   1.00000  1.00000
-3         0.97659      host host3                           
 0    ssd  0.48830          osd.0       up   1.00000  1.00000
 3    ssd  0.48830          osd.3       up   1.00000  1.00000
```

将host3上的nvme0n4盘加进来

```bash
[ceph: root@host1 /]# ceph orch daemon add osd host3.xiaohui.cn:/dev/nvme0n4
Created osd(s) 5 on host 'host3.xiaohui.cn'
```

再次查看osd情况，发现在host3上多了一个osd

```bash
[ceph: root@host1 /]# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME       STATUS  REWEIGHT  PRI-AFF
-1         5.37126  root default                             
-5         1.95319      host host1                           
 1    ssd  0.48830          osd.1       up   1.00000  1.00000
 8    ssd  0.48830          osd.8       up   1.00000  1.00000
11    ssd  0.48830          osd.11      up   1.00000  1.00000
12    ssd  0.48830          osd.12      up   1.00000  1.00000
-7         1.95319      host host2                           
 2    ssd  0.48830          osd.2       up   1.00000  1.00000
 4    ssd  0.48830          osd.4       up   1.00000  1.00000
 7    ssd  0.48830          osd.7       up   1.00000  1.00000
10    ssd  0.48830          osd.10      up   1.00000  1.00000
-3         1.46489      host host3                           
 0    ssd  0.48830          osd.0       up   1.00000  1.00000
 3    ssd  0.48830          osd.3       up   1.00000  1.00000
 5    ssd  0.48830          osd.5       up   1.00000  1.00000
```

## 从集群中删除OSD

这里有一个很重要的问题，一般情况下我们不会主动删除某个osd，通常是它坏了，主动报告状态为down，这个问题是我们如何知道哪个osd是哪块盘，例如我明确知道host2上的/dev/nvme0n3坏了，那这是哪个osd呢，或者反过来，我就想删除某个osd，我如何知道它是哪块盘呢

从上面的信息我们看到host2上的osd分别为2 4 7 10，一共4个osd，现在我想删除osd 10，并把盘用来干别的

### 查询osd所在的硬盘名称

在host2上，一共有5个硬盘

```bash
[ceph: root@host2 /]# lsblk
NAME                                                                                                  MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sr0                                                                                                    11:0    1 10.1G  0 rom  
nvme0n1                                                                                               259:0    0  500G  0 disk 
|-nvme0n1p1                                                                                           259:1    0    1G  0 part /rootfs/boot
`-nvme0n1p2                                                                                           259:2    0  499G  0 part 
  |-cl-root                                                                                           253:0    0  494G  0 lvm  /rootfs
  `-cl-swap                                                                                           253:1    0    5G  0 lvm  [SWAP]
nvme0n2                                                                                               259:3    0  500G  0 disk 
`-ceph--63ca61f0--20ba--4246--92de--97daa50067a4-osd--block--0334d9de--bb00--46c5--ba4f--b1d3d97e30fa 253:4    0  500G  0 lvm  
nvme0n3                                                                                               259:4    0  500G  0 disk 
`-ceph--ebb6e66a--72a4--47bc--972c--1529dc7b522f-osd--block--1a66b118--0f36--4de9--a85e--34706bdce3d0 253:3    0  500G  0 lvm  
nvme0n4                                                                                               259:5    0  500G  0 disk 
`-ceph--3937117f--7c4d--4e93--b5d6--07f71a10b35a-osd--block--0582284b--19d8--412b--b3d2--4449a21df72d 253:2    0  500G  0 lvm  
nvme0n5                                                                                               259:6    0  500G  0 disk 
`-ceph--989f20d4--37c8--49fa--91bd--e503e052874f-osd--block--657250d1--cf76--415d--a187--b73280e5e695 253:5    0  500G  0 lvm  
```

```bash
[ceph: root@host2 /]# ceph-volume inventory

Device Path               Size         rotates available Model name
/dev/nvme0n1              500.00 GB    False   False     VMware Virtual NVMe Disk
/dev/nvme0n2              500.00 GB    False   False     VMware Virtual NVMe Disk
/dev/nvme0n3              500.00 GB    False   False     VMware Virtual NVMe Disk
/dev/nvme0n4              500.00 GB    False   False     VMware Virtual NVMe Disk
/dev/nvme0n5              500.00 GB    False   False     VMware Virtual NVMe Disk
[ceph: root@host2 /]# 
```

查询硬盘上对应了哪个osd，看到/dev/nvme0n5对应了osd10

```bash
[ceph: root@host2 /]# for disk in `ceph-volume inventory | grep /dev | awk '{print $1}'`;do ceph-volume inventory $disk | grep -e path -e "osd id";done
     path                      /dev/nvme0n1
     path                      /dev/nvme0n2
     osd id                    2
     path                      /dev/nvme0n3
     osd id                    4
     path                      /dev/nvme0n4
     osd id                    7
     path                      /dev/nvme0n5
     osd id                    10
```

### OSD暂停服务

```bash
[ceph: root@host2 /]# ceph osd out 10
marked out osd.10. 
```

### 停止OSD

```bash
[ceph: root@host2 /]# ceph osd stop osd.10
stop osd.10. 
```

### 擦除osd以重用

```bash
[ceph: root@host2 /]# ceph orch device zap host2.xiaohui.cn /dev/nvme0n5 --force
zap successful for /dev/nvme0n5 on host2.xiaohui.cn
```

此时在硬盘上的所有lvs都会消失

```bash
[ceph: root@host2 /]# lsblk
NAME                                                                                                  MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sr0                                                                                                    11:0    1 10.1G  0 rom  
nvme0n1                                                                                               259:0    0  500G  0 disk 
|-nvme0n1p1                                                                                           259:1    0    1G  0 part /rootfs/boot
`-nvme0n1p2                                                                                           259:2    0  499G  0 part 
  |-cl-root                                                                                           253:0    0  494G  0 lvm  /rootfs
  `-cl-swap                                                                                           253:1    0    5G  0 lvm  [SWAP]
nvme0n2                                                                                               259:3    0  500G  0 disk 
`-ceph--63ca61f0--20ba--4246--92de--97daa50067a4-osd--block--0334d9de--bb00--46c5--ba4f--b1d3d97e30fa 253:3    0  500G  0 lvm  
nvme0n3                                                                                               259:4    0  500G  0 disk 
`-ceph--ebb6e66a--72a4--47bc--972c--1529dc7b522f-osd--block--1a66b118--0f36--4de9--a85e--34706bdce3d0 253:2    0  500G  0 lvm  
nvme0n4                                                                                               259:5    0  500G  0 disk 
`-ceph--3937117f--7c4d--4e93--b5d6--07f71a10b35a-osd--block--0582284b--19d8--412b--b3d2--4449a21df72d 253:4    0  500G  0 lvm  
nvme0n5                                                                                               259:6    0  500G  0 disk 
```

### 删除OSD

```bash
[ceph: root@host2 /]# ceph orch osd rm 10
Scheduled OSD(s) for removal
```

```bash
[ceph: root@host2 /]# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME       STATUS  REWEIGHT  PRI-AFF
-1         4.88297  root default                             
-5         1.95319      host host1                           
 1    ssd  0.48830          osd.1       up   1.00000  1.00000
 8    ssd  0.48830          osd.8       up   1.00000  1.00000
11    ssd  0.48830          osd.11      up   1.00000  1.00000
12    ssd  0.48830          osd.12      up   1.00000  1.00000
-7         1.46489      host host2                           
 2    ssd  0.48830          osd.2       up   1.00000  1.00000
 4    ssd  0.48830          osd.4       up   1.00000  1.00000
 7    ssd  0.48830          osd.7       up   1.00000  1.00000
-3         1.46489      host host3                           
 0    ssd  0.48830          osd.0       up   1.00000  1.00000
 3    ssd  0.48830          osd.3       up   1.00000  1.00000
 5    ssd  0.48830          osd.5       up   1.00000  1.00000
```

### 确认OSD Secret已删除

```bash
[ceph: root@host2 /]# ceph auth list | grep osd.10
installed auth entries:
```

如果还有残留，会导致下次创建osd失败，通过以下方式删除

```bash
[ceph: root@host2 /]# ceph auth del osd.10
```

# 创建和配置池

## 了解池的含义

1. 池是存储对象的逻辑分区，Ceph客户端将对象写入池

2. Ceph客户机需要集群名称(默认情况下是Ceph)和一个mon地址来连接到集群

3. Ceph客户端使用集群map检索到的池列表来确定存储新对象的位置

4. Ceph客户端创建一个I/O上下文到一个特定的池，Ceph集群使用CRUSH算法将这些池映射到放置组，然后放置组映射到特定的osd

5. 池为集群提供了一层弹性，因为池定义了可以在不丢失数据的情况下发生故障的osd的数量

## 池类型

可用的池类型有复制池和纠删代码池

**复制池**是默认的池类型，通过将各个对象复制到多个 OSD 来发挥作用，它们需要更多的存储空间， 因为会创建多个对象副本，但读取操作不受副本丢失的影响

**纠删代码**池需要的存储空间和网络带宽较小，但因为奇偶校验计算，计算开销会更高一些

对于不需要频繁访问且不需要低延迟的数据，纠删代码池通常是更好的选择。对于经常访问并且需要快速读取性能的数据，复制池通常都是更好的选择。

**创建池后，不能修改池的类型**

## 创建副本池

Ceph通过为每个对象创建多个副本来保护复制池中的数据，Ceph使用CRUSH故障域来确定作用集的主要osd来存储数据，然后，Ceph使用CRUSH 故障域来决定主OSD来存储数据，然后主OSD查找当前的池副本数量并计算辅助OSD来写入数据，当主OSD收到写响应并完成写操作后，主OSD确认写成功到Ceph客户端，这样可以在一个或多个osd失效时保护对象中的数据

使用以下命令创建一个复制池

一般不需要指定pg等数量，新版本集群会自动在200以内管理

```bash
[ceph: root@host1 /]# ceph osd pool create lixiaohuipool 200 100 replicated 
pool 'lixiaohuipool' created
[ceph: root@host1 /]# ceph osd pool ls
.mgr
.rgw.root
default.rgw.log
default.rgw.control
default.rgw.meta
lixiaohuipool
```

其中：

1. lixiaohuipool是新池的名称

2. 200 是为这个池配置的放置组 (PG) 总数

3. 100 是这个池的有效放置组数量，一般将它设置为与 pg_num 相等，而且尽量不要设置pg等数量，新集群会自动管理

4. replicated 指定这是复制池，如果命令中未包含此参数，这是默认值

5. 可以把replicated换成crush-rule-name⽤于这个池的 CRUSH 规则集的名称

## 查看池详细属性

可以看到lixiaohuipool的副本数量为3，也就3副本存储，osd_pool_default_size配置参数定义了副本的数量，默认值为3，osd_pool_default_min_size参数设置必须可以接受I/O的请求的副本数，默认值为2

```bash
[ceph: root@host1 /]# ceph osd pool ls detail       
pool 1 '.mgr' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 31 flags hashpspool stripe_width 0 pg_num_max 32 pg_num_min 1 application mgr
pool 2 '.rgw.root' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 62 lfor 0/0/58 flags hashpspool stripe_width 0 application rgw
pool 3 'default.rgw.log' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 203 lfor 0/0/59 flags hashpspool stripe_width 0 application rgw
pool 4 'default.rgw.control' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 62 lfor 0/0/59 flags hashpspool stripe_width 0 application rgw
pool 5 'default.rgw.meta' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 62 lfor 0/0/60 flags hashpspool stripe_width 0 pg_autoscale_bias 4 application rgw
pool 6 'lixiaohuipool' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 162 pgp_num 124 pg_num_target 32 pgp_num_target 100 pg_num_pending 161 autoscale_mode on last_change 786 lfor 0/786/786 flags hashpspool stripe_width 0 
```

## 修改池的副本数

```bash
[ceph: root@host1 /]# ceph osd pool set lixiaohuipool size 4
set pool 6 size to 4
```

## 配置池的应用程序类型

将lixiaohuipool类型设置为rbd

```bash
[ceph: root@host1 /]# ceph osd pool application enable lixiaohuipool rbd
enabled application 'rbd' on pool 'lixiaohuipool'
[ceph: root@host1 /]# ceph osd pool ls detail
pool 1 '.mgr' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 31 flags hashpspool stripe_width 0 pg_num_max 32 pg_num_min 1 application mgr
pool 2 '.rgw.root' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 62 lfor 0/0/58 flags hashpspool stripe_width 0 application rgw
pool 3 'default.rgw.log' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 203 lfor 0/0/59 flags hashpspool stripe_width 0 application rgw
pool 4 'default.rgw.control' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 62 lfor 0/0/59 flags hashpspool stripe_width 0 application rgw
pool 5 'default.rgw.meta' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 62 lfor 0/0/60 flags hashpspool stripe_width 0 pg_autoscale_bias 4 application rgw
pool 6 'lixiaohuipool' replicated size 4 min_size 2 crush_rule 0 object_hash rjenkins pg_num 100 pgp_num 100 pg_num_target 32 autoscale_mode on last_change 1191 lfor 0/1043/1041 flags hashpspool stripe_width 0 application rbd
```

## 修改池名称

lixiaohuipool太长了，我们修改一下

```bash
[ceph: root@host1 /]# ceph osd pool rename lixiaohuipool lxhpool
pool 'lixiaohuipool' renamed to 'lxhpool'
[ceph: root@host1 /]# ceph osd pool ls
.mgr
.rgw.root
default.rgw.log
default.rgw.control
default.rgw.meta
lxhpool
```

## 配置Erasure code池

Erasure code池使用Erasure code代替副本来保护对象数据，存储在Erasure code池中的对象被划分为多个数据块，这些数据块存储在单独的osd中，Erasure code块的数量是根据数据块计算出来的，并存储在不同的osd中，当OSD出现故障时，Erasure code块用于重建对象的数据，主OSD接收到写操作后，将写载荷编码成K+M块，通过Erasure code池发送给备OSD

Erasure code池使用这种方法来保护它们的对象，并且与复制池不同，它不依赖于存储每个对象的多个副本

总结Erasure code池的工作原理:

1. 每个对象的数据被划分为k个数据块

2. 计算出m个coding chunk

3. coding chunk块大小与data chunk大小相同

4. 该对象总共存储在k + m个osd上

Erasure编码比副本更有效地利用存储容量，副本池维护一个对象的n个副本，而纠删编码只维护k + m块，例如，3副本的副本池使用3倍的存储空间，k=4和m=2的Erasure编码池只使用1.5倍的存储空间

Ceph支持以下k+m值：

4 + 2(1:1.5比率)

8 + 3(1:1.375比率)

8 + 4(1:1.5比率)

erasure code开销的计算公式为nOSD * k / (k+m) * 0SD大小，例如，如果有64个4TB的osd(总共256 TB)， k=8, m=4，那么公式是64 * 8 /(8+4)* 4 = 170.67，然后将原始存储容量除以开销，得到这个比率。256TB /170.67 TB = 1.5

与副本池相比，Erasure编码池需要更少的存储空间才能获得类似级别的数据保护，从而降低存储集群的成本和规模。但是，计算编码块会增加Erasure编码池的CPU处理和内存开销，从而降低整体性能

使用以下命令创建Erasure编码池

```bash
[ceph: root@host1 /]# ceph osd pool create lxhecpool 200 100 erasure default
pool 'lxhecpool' created
```

其中：

1. lxhecpool 是新池的名称

2. 200 是这个池的放置组 (PG) 总数

3. 100 是这个池的有效放置组数量，通常而言，这应当与 PG 总数相等

4. erasure 指定这是纠删代码池

5. default 要使⽤的配置文件的名称，可以使用ceph osd erasure-code-profile set 命令创建新的配置⽂件，配置文件定义 k 和 m 值，以及要使用的纠删代码池插件。默认情况下，Ceph 使用default 配置文件

6. 在配置文件后还可以使用一个要用于这个池的 CRUSH 规则集的名称。如果不设置，Ceph 将使用纠删代码池配置文件中定义的规则集

## 配置PG自动缩放

在旧的Ceph中，pg等数量是非常有考究的，但是现在新版本的集群中，可以根据需要自动缩放了

查询自动缩放参数

我们发现pg自动缩放已经开启了

```bash
[ceph: root@host1 /]# ceph mgr module ls | grep auto
pg_autoscaler         on (always on)
mds_autoscaler        -             
```

如果万一没开，用下面的方法开启，并在相应的池上面开起来

```bash
[ceph: root@host1 /]# ceph mgr module enable pg_autoscaler
module 'pg_autoscaler' is already enabled (always-on)
[ceph: root@host1 /]# ceph osd pool set lixiaohuipool pg_autoscale_mode on
set pool 6 pg_autoscale_mode to on
[ceph: root@host1 /]# ceph osd pool get lixiaohuipool pg_autoscale_mode
pg_autoscale_mode: on  
[ceph: root@host1 /]# ceph osd pool autoscale-status
POOL                   SIZE  TARGET SIZE  RATE  RAW CAPACITY   RATIO  TARGET RATIO  EFFECTIVE RATIO  BIAS  PG_NUM  NEW PG_NUM  AUTOSCALE  BULK   
.mgr                 598.0k                3.0         4999G  0.0000                                  1.0       1              on         False  
.rgw.root             1327                 3.0         4999G  0.0000                                  1.0      32              on         False  
default.rgw.log       3702                 3.0         4999G  0.0000                                  1.0      32              on         False  
default.rgw.control      0                 3.0         4999G  0.0000                                  1.0      32              on         False  
default.rgw.meta      4128                 3.0         4999G  0.0000                                  4.0      32              on         False  
lixiaohuipool            0                 3.0         4999G  0.0000                                  1.0      32              on         False  
```

## Erasure Code配置文件

Erasure Code配置文件配置你的Erasure Code池用来存储对象的数据块和编码块的数量，以及使用哪些Erasure Code插件和算法

可以创建配置文件来定义不同的纠删编码参数集，Ceph在安装过程中自动创建默认配置文件，这个配置文件被配置为将对象分为两个数据块和一个编码块

查看default配置文件详情

```bash
[ceph: root@host1 /]# ceph osd erasure-code-profile get default
k=2
m=2
plugin=jerasure
technique=reed_sol_van
```

使用以下命令创建一个新的配置文件

```bash
[ceph: root@host1 /]# ceph osd erasure-code-profile get lxhecprofile
crush-device-class=
crush-failure-domain=host
crush-root=default
jerasure-per-chunk-alignment=false
k=4
m=2
plugin=jerasure
technique=reed_sol_van
w=8
```

以下是可用的参数:

**k** 跨osd分割的数据块的数量，缺省值为2

**m** 数据不可用前可能发生故障的osd数量，缺省值为1

**directory** 这个可选参数是插件库的位置，默认值为/usr/lib64/ceph/erasure-code

**plugin** 此可选参数定义要使用的纠删编码算法

**crush-failure-domain** 这个可选参数定义了CRUSH故障域，它控制块的放置，默认情况下，它被设置为host，这确保一个对象的块被放置在不同主机的osd上，如果设置为osd，那么一个对象的chunk可以放置在同一主机上的osd上，将故障域设置为osd，会导致主机上所有的osd故障，弹性较差，主机失败，可以定义并使用故障域，以确保块放置在不同数据中心机架或其他指定的主机上的osd上

**crush -device-class** 此可选参数仅为池选择由该类设备支持的osd，典型的类可能包括hdd、ssd或nvme

**crush-root** 该可选参数设置CRUSH规则集的根节点

**key=value** 插件可能具有该插件特有的键值参数

**technique** 每个插件提供一组不同的技术，用于实现不同的算法

**不能修改已存在存储池的erasure code配置文件**

使用ceph osd erasure-code-profile ls 命令列出已存在的配置文件

使用ceph osd erasure-code-profile get命令查看已创建配置文件的详细信息

使用ceph osd erasure-code-profile rm删除已存在的配置文件

## 创建Erasure池

既然不能修改池的配置文件，那我们创建一个新的池，并使用我们的EC配置文件

```bash
[ceph: root@host1 /]# ceph osd pool ls detail| grep ecpool
pool 9 'ecpool' erasure profile lxhecprofile size 6 min_size 5 crush_rule 2 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 1194 flags hashpspool,creating stripe_width 16384
```

## 配置多用途池

我们将池设置为rgw用途

```bash
[ceph: root@host1 /]# ceph osd pool application enable ecpool rgw
enabled application 'rgw' on pool 'ecpool'
[ceph: root@host1 /]# ceph osd pool ls detail | grep ecpool
pool 9 'ecpool' erasure profile lxhecprofile size 6 min_size 5 crush_rule 2 object_hash rjenkins pg_num 1 pgp_num 1 pg_num_target 32 pgp_num_target 32 autoscale_mode on last_change 1197 flags hashpspool,creating stripe_width 16384 application rgw
```

同时允许RBD和CephFS也可以使用此池

```bash
[ceph: root@host1 /]# ceph osd pool set ecpool allow_ec_overwrites true
set pool 9 allow_ec_overwrites to true
[ceph: root@host1 /]# ceph osd pool ls detail | grep ecpool
pool 9 'ecpool' erasure profile lxhecprofile size 6 min_size 5 crush_rule 2 object_hash rjenkins pg_num 1 pgp_num 1 pg_num_target 32 pgp_num_target 32 autoscale_mode on last_change 1198 flags hashpspool,ec_overwrites,creating stripe_width 16384 application rgw
```

## 删除池

删除的时候还非要加一个我非常确定的参数，因为这将失去所有参数

```bash
[ceph: root@host1 /]# ceph osd pool delete lxhpool lxhpool --yes-i-really-relly-mean-it
Invalid command: Unexpected argument '--yes-i-really-relly-mean-it'
```

加上确定参数之后，发现还是不能删除，有一个允许删除池的参数需要改一下，但是要注意，这个参数会影响所有删除池的操作，不行你就使用tell命令临时修改

```bash
[ceph: root@host1 /]# ceph osd pool delete lxhpool lxhpool --yes-i-really-really-mean-it                                                           
Error EPERM: pool deletion is disabled; you must first set the mon_allow_pool_delete config option to true before you can destroy a pool
```

```bash
[ceph: root@host1 /]# ceph tell mon.* config set mon_allow_pool_delete true
mon.host1.xiaohui.cn: {
    "success": "mon_allow_pool_delete = 'true' "
}
mon.host2: {
    "success": "mon_allow_pool_delete = 'true' "
}
mon.host3: {
    "success": "mon_allow_pool_delete = 'true' "
}
[ceph: root@host1 /]# ceph osd pool delete lxhpool lxhpool --yes-i-really-really-mean-it
pool 'lxhpool' removed
```

## Pool Namespace

名称空间是池中对象的逻辑组，可以限制对池的访问，以便用户只能在特定的名称空间中存储或检索对象，名称空间的一个优点是限制用户访问池的一部分，名称空间对于限制应用程序的存储访问非常有用，它们允许对池进行逻辑分区，并将应用程序限制到池中的特定名称空间，使用名称空间，可以保持池的数量相同，而不必为每个应用程序专用整个池

要在名称空间中存储对象，客户机应用程序必须提供池和名称名称空间。默认情况下，每个池包含一个名称为空的名称空间，称为默认名称空间。

使用rados命令从池中存储和检索对象。使用-N name和--namespace=name选项指定要使用的池和命名空间

下面以将/etc/hosts文件作为hosts对象存储在lxhpool池中lixiaohui命名空间下为例

```bash
[ceph: root@host1 /]# rados -p lxhpool -N lixiaohui put hosts /etc/hosts
[ceph: root@host1 /]# rados -p lxhpool ls --all
lixiaohui       hosts
```

# 管理Ceph认证

## 用户身份验证

Ceph使用cephx协议对集群中客户端、应用程序和守护进程之间的通信进行授权。cephx协议基于共享密钥，安装过程默认启用cephx，因此集群需要所有客户端应用程序进行用户身份验证和授权，Ceph守护进程使用的帐户名称与其关联的守护进程osd.1或mgr.serverc相匹配

使用librados的客户端应用程序所使用的帐户具有client.名称前缀，例如，在集成OpenStack和Ceph时，通常会创建一个专用的client.openstack用户帐户。对于Ceph对象网关，安装会创建一个专用的client.rgw.hostname用户帐号，在librados之上创建定制软件的开发人员应该创建具有适当功能的专用帐户

管理员帐户名也具有client.前缀。在运行ceph、rados等命令时使用，安装程序创建超级用户帐户client.admin，具有允许帐户访问所有内容和修改集群配置的功能。Ceph使用client.admin帐户用于运行管理命令，除非使用--name或--id选项明确指定用户名

可以设置CEPH_ARGS环境变量来定义诸如集群名称或用户ID等参数，下例中，我们指定了一个不存在的用户，立刻发现管理角色无法再获取集群状态

```bash
[ceph: root@host1 /]# export CEPH_ARGS="--id cephuser"
[ceph: root@host1 /]# ceph -s
2022-09-15T07:08:58.417+0000 7f4eaddee700 -1 monclient(hunting): handle_auth_bad_method server allowed_methods [2] but i only support [2,1]
2022-09-15T07:08:58.418+0000 7f4eacdec700 -1 monclient(hunting): handle_auth_bad_method server allowed_methods [2] but i only support [2,1]
2022-09-15T07:08:58.418+0000 7f4ead5ed700 -1 monclient(hunting): handle_auth_bad_method server allowed_methods [2] but i only support [2,1]
[errno 13] RADOS permission denied (error connecting to the cluster)
[ceph: root@host1 /]# unset CEPH_ARGS
[ceph: root@host1 /]# ceph -s
  cluster:
    id:     6dbbdf24-33f1-11ed-84ff-000c29759605
    health: HEALTH_WARN
            1 pool(s) do not have an application enabled

  services:
    mon: 3 daemons, quorum host1.xiaohui.cn,host2,host3 (age 2h)
    mgr: host2.eielwz(active, since 2h), standbys: host1.xiaohui.cn.jlcojp
    osd: 10 osds: 10 up (since 94m), 10 in (since 14h)

  data:
    pools:   6 pools, 161 pgs
    objects: 194 objects, 454 KiB
    usage:   342 MiB used, 4.9 TiB / 4.9 TiB avail
    pgs:     161 active+clean 
```

## Keyring 文件

对于身份验证，客户端配置一个Ceph用户名和一个包含用户安全密钥的密钥环文件，Ceph在创建每个用户帐户时为其生成密匙环文件，密钥环文件默认值为`/etc/ceph/$cluster.$name.keyring`密匙环。例如，对于client.openstack帐户，密钥环文件/etc/ceph/ceph.client.openstack.keyring密匙环

密钥环文件以纯文本的形式存储密钥，对文件进行相应的Linux文件权限保护，仅允许Linux授权用户访问，只在需要Ceph用户的密匙环文件进行身份验证的系统上部署它

客户机从Monitor请求一个会话密钥，Monitor使用客户机的共享密钥加密会话密钥，并向客户机提供会话密钥，客户机解密会话密钥并从Monitor请求票据，以对集群守护进程进行身份验证。这类似于Kerberos协议，cephx密钥环文件类似于Kerberos keytab文件

```bash
[root@host1 ~]# cat /etc/ceph/ceph.client.admin.keyring 
[client.admin]
        key = AQB4bCFjrIHZGhAAXI2cM9aPLiPUynHWq6+buA==
        caps mds = "allow *"
        caps mgr = "allow *"
        caps mon = "allow *"
        caps osd = "allow *" 
```

## 配置用户身份验证

使用命令行工具，如ceph、rados和rbd，管理员可以使用--id和--keyring选项指定用户帐户和密钥环文件。如果没有指定，命令作为client.admin进行身份验证

在使用--id 的时候不使用client.的前缀，--id会自动使用client.前缀，而使用--name的时候就需要使用client.的前缀

如果将密钥环文件存储在默认位置，则不需要--keyring选项。cephadm shell自动从/etc/ceph/目录挂载密钥环

```bash
[ceph: root@host1 /]# ceph -s --id admin
  cluster:
    id:     6dbbdf24-33f1-11ed-84ff-000c29759605
    health: HEALTH_WARN
            1 pool(s) do not have an application enabled

  services:
    mon: 3 daemons, quorum host1.xiaohui.cn,host2,host3 (age 2h)
    mgr: host2.eielwz(active, since 2h), standbys: host1.xiaohui.cn.jlcojp
    osd: 10 osds: 10 up (since 101m), 10 in (since 14h)

  data:
    pools:   6 pools, 161 pgs
    objects: 194 objects, 454 KiB
    usage:   342 MiB used, 4.9 TiB / 4.9 TiB avail
    pgs:     161 active+clean 
```

## 配置用户授权

创建新用户帐户时，授予群集权限，以授权用户的群集任务，cephx中的权限被称为能力，可以通过守护进程类型(mon、osd、mgr或mds)授予它们，使用能力来根据应用程序标记限制或提供对池、池的名称空间或一组池中的数据的访问 

## Cephx Caps

在cephx中，对于每个守护进程类型，有几个可用的能力：

**R** 授予读访问权限，每个用户帐户至少应该对监视器具有读访问权限，以便能够检索CRUSH map

**W** 授予写访问权限，客户端需要写访问来存储和修改osd上的对象。对于manager (MGRs)， w授予启用或禁用模块的权限

**X** 授予执行扩展对象类的授权，这允许客户端对对象执行额外的操作，比如用rados lock get或list列出RBD图像

***** 授予完全访问权

**class - read和class -write** 是x的子集，通常在RBD池中使用它们

本例创建了lixiaohui用户帐户，并赋予了从任意池中存储和检索对象的能力：

```bash
[ceph: root@host1 /]# ceph auth get-or-create client.lixiaohui mon 'allow r' osd 'allow r'
[client.lixiaohui]
        key = AQB80yJjzH0gBhAAFAzLHmYBzXElegXlvTo5Dg== 
```

使用配置文件设置Caps

Cephx提供预定义的功能配置文件，在创建用户帐户时，利用配置文件简化用户访问权限的配置

本例通过rbd配置文件定义新的lxh用户帐号的访问权限，客户端应用程序可以使用该帐户使用RADOS块设备对Ceph存储进行基于块的访问

```bash
[ceph: root@host1 /]# ceph auth get-or-create client.lxh mon 'profile rbd' osd 'profile rbd'
[client.lxh]
        key = AQDj0yJjSW8NHRAA3mm59N4TeIV30S/K4dgchg==
```

rbd-read-only配置文件的工作方式相同，但授予只读访问权限，Ceph利用其他现有的配置文件在守护进程之间进行内部通信，Ceph在内部定义它们，不能创建自己的配置文件

下表列出了默认安装下Ceph的功能

| 能力                    | 描述                                                 |
| --------------------- | -------------------------------------------------- |
| allow                 | 授予允许能力                                             |
| r                     | 赋予用户读访问权限，需要监视器来检索CRUSH map                        |
| w                     | 赋予用户对对象的写访问权                                       |
| x                     | 使用户能够调用类方法(即读取和写入)并在监视器上执行身份验证操作                   |
| class-read            | 赋予用户调用类读取方法的能力，x的子集                                |
| class-write           | 赋予用户调用类写入方法的能力，x的子集                                |
| *                     | 为用户提供特定守护进程或池的读、写和执行权限，以及执行管理命令的能力                 |
| profile osd           | 允许用户作为OSD连接到其他OSD或监视器，授予osd权限，使osd能够处理复制心跳流量和状态报告。 |
| profile bootstrap-osd | 允许用户引导一个OSD，这样用户在引导一个OSD时就有了添加key的权限               |
| profile rbd           | 允许用户对Ceph块设备进行读写访问                                 |
| profile rbd-read-only | 为用户提供对Ceph块设备的只读访问权限                               |

## 限制访问

下面的例子创建了user1用户，并给了他对lxhpool池的读写权限:

```bash
[ceph: root@host1 /]# ceph auth get-or-create client.user1 mon 'allow r' osd 'allow rw pool=lxhpool'
[client.user1]
        key = AQAU2yJjKC36IxAAAuXYWDLZ44g/iQVOx9Kmug==
[ceph: root@host1 /]# ceph auth get client.user1
[client.user1]
        key = AQAU2yJjKC36IxAAAuXYWDLZ44g/iQVOx9Kmug==
        caps mon = "allow r"
        caps osd = "allow rw pool=lxhpool"
exported keyring for client.user1
```

如果在配置功能时没有指定池，那么Ceph将在所有现有的池上设置它们，cephx机制可以通过其他方式限制对对象的访问:

**通过对象名称前缀**，下面的示例限制对任何池中名称以lxh开头的对象的访问

```bash
[ceph: root@host1 /]# ceph auth get-or-create client.user2 mon 'allow r' osd 'allow rw object_prefix lxh'
[client.user2]
        key = AQBh2yJjPTOeIBAABWgPQwZlDlOIqjTHCz1tLA==
[ceph: root@host1 /]# ceph auth get client.user2
[client.user2]
        key = AQBh2yJjPTOeIBAABWgPQwZlDlOIqjTHCz1tLA==
        caps mon = "allow r"
        caps osd = "allow rw object_prefix lxh"
exported keyring for client.user2
```

**通过namespace**，实现namespace来对池中的对象进行逻辑分组，然后可以将用户帐户限制为属于特定namespace的对象：

```bash
[ceph: root@host1 /]# ceph auth get-or-create client.user3 mon 'allow r' osd 'allow rw namespace=lixiaohui'
[client.user3]
        key = AQCa2yJjXG2oKhAAM/HL1GHc/gWSxtsB/dKs/g==
[ceph: root@host1 /]# ceph auth get client.user3
[client.user3]
        key = AQCa2yJjXG2oKhAAM/HL1GHc/gWSxtsB/dKs/g==
        caps mon = "allow r"
        caps osd = "allow rw namespace=lixiaohui"
exported keyring for client.user3
```

**通过路径**，Ceph文件系统(cepphfs)利用这种方法来限制对特定目录的访问，下面的例子创建了一个新的用户帐户user4，它只能访问/webcontent目录及其内容:

```bash
[ceph: root@host1 /]# ceph fs volume create lxhfs
[ceph: root@host1 /]# ceph fs authorize lxhfs client.user4 /webcontent rw
[client.user4]
        key = AQC43CJjXiFtCBAAwcJQFfY5bHktAOUyZj3uvQ==
[ceph: root@host1 /]# ceph auth get client.user4    
[client.user4]
        key = AQC43CJjXiFtCBAAwcJQFfY5bHktAOUyZj3uvQ==
        caps mds = "allow rw fsname=lxhfs path=/webcontent"
        caps mon = "allow r fsname=lxhfs"
        caps osd = "allow rw tag cephfs data=lxhfs"
exported keyring for client.user4
```

**通过monitor命令**，这种方法将管理员限制在特定的命令列表中，创建user5用户帐户并限制其访问两个命令的示例如下:

```bash
[ceph: root@host1 /]# ceph auth get-or-create client.user5 mon 'allow r, allow command "auth get-or-create", allow command "auth list"'
[client.user5]
        key = AQAo3SJjdyonGxAAMZAPKh7GUlRuXWenNezRYg==
[ceph: root@host1 /]# ceph auth get client.user5
[client.user5]
        key = AQAo3SJjdyonGxAAMZAPKh7GUlRuXWenNezRYg==
        caps mon = "allow r, allow command \"auth get-or-create\", allow command \"auth list\""
exported keyring for client.user5
```

## 用户管理命令

ceph auth list 查询现有用户

```bash
[ceph: root@host1 /]# ceph auth list
installed auth entries:

mds.host2.xiaohui.cn.host1.drgiaf
        key: AQDWcyFj7Y5/CBAAGubJ5L7AFJcmzJ0Hj4lJfA==
        caps: [mds] allow
        caps: [mon] profile mds
        caps: [osd] allow rw tag cephfs *=*
```

ceph auth get 获取特定帐户的详细信息

```bash
[ceph: root@host1 /]# ceph auth get client.user1
[client.user1]
        key = AQAU2yJjKC36IxAAAuXYWDLZ44g/iQVOx9Kmug==
        caps mon = "allow r"
        caps osd = "allow rw pool=lxhpool"
exported keyring for client.user1
```

ceph auth print-key 显示用户密钥

```bash
[ceph: root@host1 /]# ceph auth print-key client.user1
AQAU2yJjKC36IxAAAuXYWDLZ44g/iQVOx9Kmug==
```

ceph auth export、ceph auth import导出以及导入账号

```bash
[ceph: root@host1 /]# ceph auth export client.user1 > user.txt
export auth(key=AQAU2yJjKC36IxAAAuXYWDLZ44g/iQVOx9Kmug==)
[ceph: root@host1 /]# ceph auth import -i user.txt
imported keyring
[ceph: root@host1 /]# cat user.txt 
[client.user1]
        key = AQAU2yJjKC36IxAAAuXYWDLZ44g/iQVOx9Kmug==
        caps mon = "allow r"
        caps osd = "allow rw pool=lxhpool"
```

ceph auth get-or-create 创建一个新用户帐户并生成它的密钥，通常会添加-o选项保存密钥，不然只是显示不保存

```bash
[ceph: root@host1 /]# ceph auth get-or-create client.user6 mon 'allow r' osd 'allow rw' -o /etc/ceph/ceph.client.app1.keyring
[ceph: root@host1 /]# cat /etc/ceph/ceph.client.app1.keyring
[client.user6]
        key = AQDg3iJjTUBfIRAAAZZXr8zu7vOyK5AoPeXlSw== 
```

ceph auth caps 修改用户Caps

将user6的osd中w权限去掉了，定义一个空字符串可以删除所有功能

```bash
[ceph: root@host1 /]# ceph auth get client.user6
[client.user6]
        key = AQDg3iJjTUBfIRAAAZZXr8zu7vOyK5AoPeXlSw==
        caps mon = "allow r"
        caps osd = "allow rw"
exported keyring for client.user6
[ceph: root@host1 /]# ceph auth caps client.user6 mon 'allow r' osd 'allow r'
updated caps for client.user6
[ceph: root@host1 /]# ceph auth get client.user6
[client.user6]
        key = AQDg3iJjTUBfIRAAAZZXr8zu7vOyK5AoPeXlSw==
        caps mon = "allow r"
        caps osd = "allow r"
exported keyring for client.user6
```

ceph auth del 删除用户帐号

```bash
[ceph: root@host1 /]# ceph auth del client.user6
updated
```
