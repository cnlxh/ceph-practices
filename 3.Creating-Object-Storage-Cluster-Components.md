| 编号  | 系统         | 角色                    | IP             | 主机名              | Ceph版本 |
| --- | ---------- | --------------------- | -------------- | ---------------- | ------ |
| 1   | Centos 8.5 | bootstrap，mon，mgr，osd | 192.168.30.130 | host1.xiaohui.cn | Quincy |
| 2   | Centos 8.5 | mon，mgr，osd，rgw，mds   | 192.168.30.131 | host2.xiaohui.cn | Quincy |
| 3   | Centos 8.5 | mon，mgr，rgw，mds       | 192.168.30.132 | host3.xiaohui.cn | Quincy |

作者：李晓辉

联系方式：

1. 微信：Lxh_Chat

2. QQ：939958092

3. 邮箱：[xiaohui_li@foxmail.com](mailto:xiaohui_li@foxmail.com)

# 使用逻辑卷创建 BlueStore OSD

## BlueStore 简介

BlueStore 取代 FileStore 作为 OSD 的存储后端，FileStore 现已弃用，FileStore 将对象存储为块设备基础上的文件系统（通常是 XFS）中的文件。BlueStore 将对象直接存储在原始块设备上，免除了对文件系统层的需要，从而提高了读写操作速度

## 停止自动应用osd

前面我们已经添加了所有硬盘到集群中，而且添加了--all-available-devices选项，将会导致我们删除osd后再次重新创建，我们先停止集群自动创建osd

```bash
[ceph: root@host1 /]# ceph orch apply osd --all-available-devices --unmanaged=true
Scheduled osd.all-available-devices update...
```

## 添加OSD到集群

先查看现有osd情况

```bash
[ceph: root@host1 /]# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME       STATUS  REWEIGHT  PRI-AFF
-1         4.88297  root default                             
-5         1.95319      host host1                           
 1    ssd  0.48830          osd.1       up   1.00000  1.00000
 8    ssd  0.48830          osd.8       up   1.00000  1.00000
11    ssd  0.48830          osd.11      up   1.00000  1.00000
12    ssd  0.48830          osd.12      up   1.00000  1.00000
-7         1.95319      host host2                           
 2    ssd  0.48830          osd.2       up   1.00000  1.00000
 4    ssd  0.48830          osd.4       up   1.00000  1.00000
 7    ssd  0.48830          osd.7       up   1.00000  1.00000
10    ssd  0.48830          osd.10      up   1.00000  1.00000
-3         0.97659      host host3                           
 0    ssd  0.48830          osd.0       up   1.00000  1.00000
 3    ssd  0.48830          osd.3       up   1.00000  1.00000
```

将host3上的nvme0n4盘加进来

```bash
[ceph: root@host1 /]# ceph orch daemon add osd host3.xiaohui.cn:/dev/nvme0n4
Created osd(s) 5 on host 'host3.xiaohui.cn'
```

再次查看osd情况，发现在host3上多了一个osd

```bash
[ceph: root@host1 /]# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME       STATUS  REWEIGHT  PRI-AFF
-1         5.37126  root default                             
-5         1.95319      host host1                           
 1    ssd  0.48830          osd.1       up   1.00000  1.00000
 8    ssd  0.48830          osd.8       up   1.00000  1.00000
11    ssd  0.48830          osd.11      up   1.00000  1.00000
12    ssd  0.48830          osd.12      up   1.00000  1.00000
-7         1.95319      host host2                           
 2    ssd  0.48830          osd.2       up   1.00000  1.00000
 4    ssd  0.48830          osd.4       up   1.00000  1.00000
 7    ssd  0.48830          osd.7       up   1.00000  1.00000
10    ssd  0.48830          osd.10      up   1.00000  1.00000
-3         1.46489      host host3                           
 0    ssd  0.48830          osd.0       up   1.00000  1.00000
 3    ssd  0.48830          osd.3       up   1.00000  1.00000
 5    ssd  0.48830          osd.5       up   1.00000  1.00000
```

## 从集群中删除OSD

这里有一个很重要的问题，一般情况下我们不会主动删除某个osd，通常是它坏了，主动报告状态为down，这个问题是我们如何指定哪个osd是哪块盘，例如我明确指定host2上的/dev/nvme0n3坏了，那这是哪个osd呢，或者反过来，我就想删除某个osd，我如何知道它是哪块盘呢

从上面的信息我们看到host2上的osd分别为2 4 7 10，一共4个osd，现在我想删除osd 10，并把盘用来干别的

### 查询osd所在的硬盘名称

在host2上，一共有5个硬盘

```bash
[ceph: root@host2 /]# lsblk
NAME                                                                                                  MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sr0                                                                                                    11:0    1 10.1G  0 rom  
nvme0n1                                                                                               259:0    0  500G  0 disk 
|-nvme0n1p1                                                                                           259:1    0    1G  0 part /rootfs/boot
`-nvme0n1p2                                                                                           259:2    0  499G  0 part 
  |-cl-root                                                                                           253:0    0  494G  0 lvm  /rootfs
  `-cl-swap                                                                                           253:1    0    5G  0 lvm  [SWAP]
nvme0n2                                                                                               259:3    0  500G  0 disk 
`-ceph--63ca61f0--20ba--4246--92de--97daa50067a4-osd--block--0334d9de--bb00--46c5--ba4f--b1d3d97e30fa 253:4    0  500G  0 lvm  
nvme0n3                                                                                               259:4    0  500G  0 disk 
`-ceph--ebb6e66a--72a4--47bc--972c--1529dc7b522f-osd--block--1a66b118--0f36--4de9--a85e--34706bdce3d0 253:3    0  500G  0 lvm  
nvme0n4                                                                                               259:5    0  500G  0 disk 
`-ceph--3937117f--7c4d--4e93--b5d6--07f71a10b35a-osd--block--0582284b--19d8--412b--b3d2--4449a21df72d 253:2    0  500G  0 lvm  
nvme0n5                                                                                               259:6    0  500G  0 disk 
`-ceph--989f20d4--37c8--49fa--91bd--e503e052874f-osd--block--657250d1--cf76--415d--a187--b73280e5e695 253:5    0  500G  0 lvm  
```

```bash
[ceph: root@host2 /]# ceph-volume inventory

Device Path               Size         rotates available Model name
/dev/nvme0n1              500.00 GB    False   False     VMware Virtual NVMe Disk
/dev/nvme0n2              500.00 GB    False   False     VMware Virtual NVMe Disk
/dev/nvme0n3              500.00 GB    False   False     VMware Virtual NVMe Disk
/dev/nvme0n4              500.00 GB    False   False     VMware Virtual NVMe Disk
/dev/nvme0n5              500.00 GB    False   False     VMware Virtual NVMe Disk
[ceph: root@host2 /]# 
```

查询硬盘上对应了哪个osd，看到/dev/nvme0n5对应了osd10

```bash
[ceph: root@host2 /]# for disk in `ceph-volume inventory | grep /dev | awk '{print $1}'`;do ceph-volume inventory $disk | grep -e path -e "osd id";done
     path                      /dev/nvme0n1
     path                      /dev/nvme0n2
     osd id                    2
     path                      /dev/nvme0n3
     osd id                    4
     path                      /dev/nvme0n4
     osd id                    7
     path                      /dev/nvme0n5
     osd id                    10
```

### OSD暂停服务

```bash
[ceph: root@host2 /]# ceph osd out 10
marked out osd.10. 
```

### 停止osd

```bash
[ceph: root@host2 /]# ceph osd stop osd.10
stop osd.10. 
```

### 擦除osd以重用

```bash
[ceph: root@host2 /]# ceph orch device zap host2.xiaohui.cn /dev/nvme0n5 --force
zap successful for /dev/nvme0n5 on host2.xiaohui.cn
```

此时在硬盘上的所有lvs都会消失

```bash
[ceph: root@host2 /]# lsblk
NAME                                                                                                  MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sr0                                                                                                    11:0    1 10.1G  0 rom  
nvme0n1                                                                                               259:0    0  500G  0 disk 
|-nvme0n1p1                                                                                           259:1    0    1G  0 part /rootfs/boot
`-nvme0n1p2                                                                                           259:2    0  499G  0 part 
  |-cl-root                                                                                           253:0    0  494G  0 lvm  /rootfs
  `-cl-swap                                                                                           253:1    0    5G  0 lvm  [SWAP]
nvme0n2                                                                                               259:3    0  500G  0 disk 
`-ceph--63ca61f0--20ba--4246--92de--97daa50067a4-osd--block--0334d9de--bb00--46c5--ba4f--b1d3d97e30fa 253:3    0  500G  0 lvm  
nvme0n3                                                                                               259:4    0  500G  0 disk 
`-ceph--ebb6e66a--72a4--47bc--972c--1529dc7b522f-osd--block--1a66b118--0f36--4de9--a85e--34706bdce3d0 253:2    0  500G  0 lvm  
nvme0n4                                                                                               259:5    0  500G  0 disk 
`-ceph--3937117f--7c4d--4e93--b5d6--07f71a10b35a-osd--block--0582284b--19d8--412b--b3d2--4449a21df72d 253:4    0  500G  0 lvm  
nvme0n5                                                                                               259:6    0  500G  0 disk 
```

### 删除osd10

```bash
[ceph: root@host2 /]# ceph orch osd rm 10
Scheduled OSD(s) for removal
```

```bash
[ceph: root@host2 /]# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME       STATUS  REWEIGHT  PRI-AFF
-1         4.88297  root default                             
-5         1.95319      host host1                           
 1    ssd  0.48830          osd.1       up   1.00000  1.00000
 8    ssd  0.48830          osd.8       up   1.00000  1.00000
11    ssd  0.48830          osd.11      up   1.00000  1.00000
12    ssd  0.48830          osd.12      up   1.00000  1.00000
-7         1.46489      host host2                           
 2    ssd  0.48830          osd.2       up   1.00000  1.00000
 4    ssd  0.48830          osd.4       up   1.00000  1.00000
 7    ssd  0.48830          osd.7       up   1.00000  1.00000
-3         1.46489      host host3                           
 0    ssd  0.48830          osd.0       up   1.00000  1.00000
 3    ssd  0.48830          osd.3       up   1.00000  1.00000
 5    ssd  0.48830          osd.5       up   1.00000  1.00000
```

### 确认osd secret已删除

```bash
[ceph: root@host2 /]# ceph auth list | grep osd.10
installed auth entries:
```

如果还有残留，会导致下次创建osd失败，通过以下方式删除

```bash
[ceph: root@host2 /]# ceph auth rm osd.10
```

 

# 创建和配置池

## 了解池的含义

1. 池是存储对象的逻辑分区，Ceph客户端将对象写入池

2. Ceph客户机需要集群名称(默认情况下是Ceph)和一个mon地址来连接到集群

3. Ceph客户端使用集群map检索到的池列表来确定存储新对象的位置

4. Ceph客户端创建一个I/O上下文到一个特定的池，Ceph集群使用CRUSH算法将这些池映射到放置组，然后放置组映射到特定的osd

5. 池为集群提供了一层弹性，因为池定义了可以在不丢失数据的情况下发生故障的osd的数量

## 池类型

可用的池类型有复制池和纠删代码池

**复制池**是默认的池类型，通过将各个对象复制到多个 OSD 来发挥作用，它们需要更多的存储空间， 因为会创建多个对象副本，但读取操作不受副本丢失的影响

**纠删代码**池需要的存储空间和网络带宽较小，但因为奇偶校验计算，计算开销会更高一些

对于不需要频繁访问且不需要低延迟的数据，纠删代码池通常是更好的选择。对于经常访问并且需要快速读取性能的数据，复制池通常都是更好的选择。

**创建池后，不能修改池的类型**

 

## 创建复制池

Ceph通过为每个对象创建多个副本来保护复制池中的数据，Ceph使用CRUSH故障域来确定作用集的主要osd来存储数据，然后，Ceph使用CRUSH 故障域来决定主OSD来存储数据，然后主OSD查找当前的池副本数量并计算辅助OSD来写入数据，当主OSD收到写响应并完成写操作后，主OSD确认写成功到Ceph客户端，这样可以在一个或多个osd失效时保护对象中的数据

使用以下命令创建一个复制池

一般不需要指定pg等数量，新版本集群会自动在200以内管理

```bash
[ceph: root@host1 /]# ceph osd pool create lixiaohuipool 200 100 replicated 
pool 'lixiaohuipool' created
[ceph: root@host1 /]# ceph osd pool ls
.mgr
.rgw.root
default.rgw.log
default.rgw.control
default.rgw.meta
lixiaohuipool
```

其中：

1. lixiaohuipool是新池的名称

2. 200 是为这个池配置的放置组 (PG) 总数

3. 100 是这个池的有效放置组数量，将它设置为与 pg_num 相等

4. replicated 指定这是复制池，如果命令中未包含此参数，这是默认值

5. 可以把replicated换成crush-rule-name⽤于这个池的 CRUSH 规则集的名称

## 查看池详细属性

```bash
[ceph: root@host1 /]# ceph osd pool ls detail       
pool 1 '.mgr' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 31 flags hashpspool stripe_width 0 pg_num_max 32 pg_num_min 1 application mgr
pool 2 '.rgw.root' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 62 lfor 0/0/58 flags hashpspool stripe_width 0 application rgw
pool 3 'default.rgw.log' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 203 lfor 0/0/59 flags hashpspool stripe_width 0 application rgw
pool 4 'default.rgw.control' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 62 lfor 0/0/59 flags hashpspool stripe_width 0 application rgw
pool 5 'default.rgw.meta' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on last_change 62 lfor 0/0/60 flags hashpspool stripe_width 0 pg_autoscale_bias 4 application rgw
pool 6 'lixiaohuipool' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 162 pgp_num 124 pg_num_target 32 pgp_num_target 100 pg_num_pending 161 autoscale_mode on last_change 786 lfor 0/786/786 flags hashpspool stripe_width 0
```
