| 编号  | 系统         | 角色                           | IP             | 主机名                 | Ceph版本 |
| --- | ---------- | ---------------------------- | -------------- | ------------------- | ------ |
| 1   | Centos 8.5 | bootstrap，mon，mgr，osd，iscsi  | 192.168.31.185 | ceph01.xiaohui.cn   | Quincy |
| 2   | Centos 8.5 | mon，mgr，osd，rgw，mds          | 192.168.31.129 | ceph02.xiaohui.cn   | Quincy |
| 3   | Centos 8.5 | mon，mgr，rgw，mds              | 192.168.31.25  | ceph03.xiaohui.cn   | Quincy |
| 4   | Centos 8.5 | second ceph cluster，rgwsite2 | 192.168.31.34  | rgwsite2.xiaohui.cn | Quincy |

作者：李晓辉

联系方式：

1. 微信：Lxh_Chat

2. QQ：939958092

3. 邮箱：[xiaohui_li@foxmail.com](mailto:xiaohui_li@foxmail.com) 

# 部署共享文件存储

## Ceph文件系统和MDS

Ceph文件系统(cepphfs)是一个posix兼容的文件系统，构建在RADOS (Ceph的分布式对象存储)之上。基于文件的存储像传统的文件系统一样组织数据，具有目录树层次结构。Ceph文件系统的实现需要一个运行的Ceph存储集群和至少一个MDS (Ceph Metadata Server)来管理Ceph文件系统的元数据，与文件数据分开管理，降低了复杂性，提高了可靠性。与RBD和RGW类似，CephFS守护进程是作为librados的本地接口实现的

### 文件、块和对象存储

基于文件的存储像传统的文件系统一样组织数据。数据保存为具有名称和相关元数据(如修改时间戳、所有者和访问权限)的文件。基于文件的存储使用目录树层次结构来组织文件的存储方式

基于块的存储提供了一个存储卷，它的操作类似于磁盘设备，组织成大小相同的块。通常，基于块的存储卷要么用文件系统格式化，要么由数据库等应用程序直接访问和写入

使用基于对象的存储，可以将任意数据和元数据作为一个单元存储在平面存储池中，该单元使用惟一标识符进行标记。与以块或文件系统层次结构访问数据不同，您使用API存储和检索对象。基本上，Red Hat Ceph Storage RADOS集群是一个对象存储

### 元数据服务器

元数据服务器(MDS)为CephFS客户端管理元数据。这个守护进程提供cepphfs客户端访问RADOS对象所需的信息，例如提供文件系统树中的文件位置。MDS用于在RADOS集群中管理目录层次结构，存储文件元数据，如所有者、时间戳、权限模式等。MDS还负责访问缓存和管理客户端缓存，以保持缓存一致性。

MDS进程有主备两种运行模式。一个主MDS管理cephfs文件系统的元数据。备MDS作为备份，当主MDS无响应时切换为主MDS。CephFS共享文件系统需要主MDS服务。应该在集群中至少部署一个备MDS，以确保高可用性

如果没有创建足够的MDS池来匹配配置的备用守护进程的数量，那么Ceph集群将显示WARN健康状态。推荐的解决方案是创建更多的MDS池，为每个守护进程提供一个池。但是，一个临时的解决方案是将备用池的数量设置为0，通过Ceph fs set fs-name standby_count_wanted 0命令禁用Ceph MDS备用检查

CephFS客户端首先联系一个MON来验证和检索集群映射。然后客户端向一个主MDS查询文件元数据。客户端使用元数据通过直接与osd通信来访问组成请求文件或目录的对象。

MDS的特性和配置项说明如下:

**MDS Ranks**

MDS级别定义如何在MDS守护进程上分布元数据工作负载。rank的数量由max_mds配置设置定义，它是一次可以活动的mds守护进程的最大数量。MDS守护进程启动时没有级别，MON守护进程负责为它们分配级别

**Subvolume和Subvolume group**

CephFS子卷是独立的CephFS文件系统目录树的抽象。在创建子卷时，可以指定更细粒度的权限管理，如UID、GID、文件模式、大小和子卷组。子卷组是跨一组子卷的目录级别的抽象

可以创建子卷的快照，但Red Hat Ceph $torage 5不支持创建子卷组的快照。可以列出和删除子卷组的现有快照

**文件系统的亲和力**

将CephFS文件系统配置为使用一个MDS而不是另一个MDS。例如，可以配置为更喜欢运行在更快的服务器上的MDS，而不是运行在旧服务器上的另一个MDS。这个文件系统亲和性是通过mds_join_fs选项配置的

**MDS缓存大小限制**

通过MDS_cache_ memory_limit选项来限制可以使用的最大内存，或者通过mds_cache_size选项定义最大索引节点数来限制MDS缓存的大小

**配额**

配置cepphfs文件系统，通过使用配额来限制存储的字节或文件的数量。FUSE和内核客户机都支持在挂载cepphfs文件系统时检查配额。当用户达到配额限制时，这些客户端还负责停止向CephFS文件系统写入数据。使用setfatr命令的ceph.quota.max_bytes和ceph.quota.max_files选项设置限制。

### 新的CephFS能力

Red Hat Ceph Storage 5消除了早期版本的限制。

Red Hat Ceph Storage 5支持集群内多个主MDS，提升元数据性能。为了保持高可用性，您可以配置额外的备MDS，当主MDS出现故障时，可以接管其工作。Red Hat Ceph Storage 5支持在集群中创建多个cephfs文件系统。部署多个cephfs文件系统需要运行更多的MDS守护进程。

## 部署CephFS

要实现cepphfs文件系统，需要创建所需的池、创建cepphfs文件系统、部署MDS守护进程，然后挂载文件系统。可以手动创建池，创建ceph fs文件系统，并部署MDS守护进程，或者使用ceph fs卷创建命令，它自动执行所有这些步骤。第一个选项为系统管理员提供了对进程的更多控制，但是比更简单的ceph fs卷创建命令有更多的步骤

### 使用Volume方法创建CephFS

使用ceph fs卷直接创建ceph fs卷。该命令创建与CephFS相关联的池，创建CephFS卷，并在主机上启动MDS服务，在ceph01上创建4个进程

```bash
[root@ceph01 ~]# ceph fs volume create lxhfsname --placement='4 ceph01.xiaohui.cn'
```

### 创建带有放置规范的CephFS

要对部署过程进行更多控制，请手动创建与CephFS关联的池，在主机上启动MDS服务，并创建CephFS文件系统

#### 创建数据池和元数据池

一个cepphfs文件系统至少需要两个池，一个存储cepphfs数据，另一个存储cepphfs元数据。这两个池的默认名称是cephfs_data和cephfs_metadata。要创建cepphfs文件系统，首先创建两个池

```bash
[ceph: root@server /]# ceph osd pool create cephfs_data 
[ceph: root@server /]# ceph osd pool create cephfs_metadata 
```

这个例子创建了两个带有标准参数的池。由于元数据池存储文件位置信息，因此请考虑为该池设置更高的复制级别，以避免导致数据不可访问的数据错误。

默认情况下，Ceph使用复制的数据池。但是，cepphfs文件系统现在也支持erasure-coded的数据池。使用ceph osd pool命令创建一个erasure-coded池

```bash
[ceph: root@server /]# ceph osd pool create pool-name erasure
```

#### 创建CephFS和部署MDS服务

当有可用的数据池和元数据池时，使用ceph fs new命令创建文件系统，如下所示

```bash
[ceph: root@server /]# ceph fs new fs-name metadata-pool data-pool
```

要在ceph fs文件系统中添加一个现有的erasure pool作为数据池，请使用ceph fs add_data_pool

```bash
[ceph: root@server /]# ceph fs add_ data_pool fs-name data-pool
```

然后可以部署MDS服务

```bash
[ceph: root@server /]# ceph orch apply mds fs-name --placement="number-of-hosts list-of-hosts"
```

#### 使用服务规范创建CephFS

使用Ceph Orchestrator来使用服务规范部署MDS服务。首先，手动创建两个所需的池。然后，创建一个包含服务细节的YAML文件:

```yaml
service_type: mds 
service_id: fs-name 
placements: 
  hosts: 
    - host-name-1 
    - host-name-2
    - ...
```

使用YAML服务规范和ceph orch apply命令部署MDS服务

```bash
[ceph: root@server /]# ceph orch apply -i file-name.yml 
```

最后，使用ceph fs new命令创建ceph fs文件系统

## 使用CephFS挂载文件系统

可以使用任一可用的客户机挂载CephFS文件系统:

1. 核心客户端

2. FUSE客户机

内核客户端需要Linux内核版本4或更高版本，从RHEL 8开始就可以使用，对于以前的内核版本，应该使用FUSE客户机

这两个客户端各有优缺点。并不是两个客户机都支持所有特性。例如，内核客户机不支持配额，但是可以更快。FUSE客户端支持配额和ACLs。必须允许acl与FUSE客户机挂载的CephFS文件系统一起使用它们

### 通用CephFS客户端配置

要在任意客户机上挂载基于cephfs的文件系统，请在客户机主机上验证以下先决条件：

1. 在客户端主机安装ceph-common包，对于FUSE客户端，还需要安装ceph-fuse包

2. 检查Ceph配置文件是否存在(/etc/ceph/ceph.cfg)

3. 授权客户端访问cepphfs文件系统

4. 使用ceph auth get命令提取新的授权密钥，并将其复制到客户端主机上的/etc/ceph文件夹

5. 当使用FUSE客户端为非root用户时，需要在/etc/fuse.conf中添加user_allow_other

### 使用FUSE客户端挂载cephfs

在满足前提条件的情况下，使用FUSE客户端挂载和卸载cephfs文件系统:

```bash
[root@node -]# ceph-fuse [mount-point] [options] 
```

要为特定用户提供密匙环，可以使用--id选项

需要授权客户端访问ceph fs文件系统，使用ceph fs authorize命令

```bash
[ceph: root@server /]# ceph fs authorize fs-name client-name path permissions 
```

使用ceph fs authorize命令，可以为cepphfs文件系统中的不同用户和文件夹提供细粒度的访问控制。可以为cepphfs文件系统中的文件夹设置不同的选项:

**r:** 指定文件夹的读访问权限。如果没有指定其他限制，则还将对子文件夹授予读访问权限

**w:** 指定文件夹的写访问权限。如果没有指定其他限制，则对子文件夹也授予写访问权限

**p:** 除了r和w功能之外，客户端还需要p选项来使用布局或配额

**s:** 除了r和w功能外，客户端还需要s选项来创建快照

此示例允许一个用户读取根文件夹，并提供对/directory文件夹的读、写和快照权限

```bash
[ceph: root@server /]# ceph fs authorize mycephfs client.user / r /directory rws
```

默认情况下，CephFS FUSE客户端挂载所访问文件系统的根目录(/)。可以使用ceph-fuse - r directory命令挂载特定的目录

在挂载指定目录时，如果该目录在CephFS卷中不存在，则该操作将失败

当配置多个CephFS文件系统时，CephFS FUSE客户端将挂载缺省的CephFS文件系统。要使用不同的文件系统，请使用- -client_fs选项

要使用FUSE客户机准确地挂载CephFS文件系统，可以在/etc/fstab文件中添加以下条目

```bash
host-name:_port_ mount-point fuse.ceph ceph.id=myuser,ceph.client_mountpoint=mountpoint,_netdev 0 0
```

使用umount命令卸载文件系统

```bash
[root@node -]# umount mount-point
```

### 用内核客户端安装CephFS

在使用CephFS内核客户机时，使用以下命令挂载文件系统

```bash
[root@node -)# mount -t ceph [device]:[path] [mount-point] -o [key-value] [other-options] 
```

需要使用ceph fs authorize命令授权客户端访问ceph fs文件系统。使用ceph auth get命令提取客户端密钥，然后将密钥复制到客户端主机的/etc/ceph文件夹

使用CephFS内核客户机，可以从CephFS文件系统挂载特定的子目录

这个示例从CephFS文件系统的根目录挂载一个名为/dir/dir2的目录

```bash
[root@node ~]# mount -t ceph mon1:/dir1/dir2 mount-point
```

可以指定一个由多个以逗号分隔的MONs组成的列表来挂载设备。标准端口(6789)是默认的，或者可以在每个MON的名称后面添加一个冒号和一个非标准的端口号。建议的做法是指定多个MON，以防在挂载文件系统时有些MON处于离线状态

当使用CephFS内核客户端时，其他选项是可用的:

cepphfs内核客户端挂载选项如下：

| 选项                         | 描述                                 |
| -------------------------- | ---------------------------------- |
| name=name                  | 要使用的Cephx客户机ID。默认为guest            |
| fs=fs-name                 | 挂载的cepphfs文件系统的名称。如果不提供值，则使用默认文件系统 |
| secret=secret_value        | 此客户端密钥的值                           |
| secretfile=secret_key_file | 这个客户机的带有秘钥的文件的路径                   |
| rsize=bytes                | 以字节为单位指定最大读取大小                     |
| wsize=bytes                | 以字节为单位指定最大写大小。默认为none              |

要使用内核客户端持久化挂载CephFS文件系统，可以在/etc/fstab文件中添加以下条目

```bash
mon1,mon2:/ mount._point ceph name=user1,secretfile=/root/secret,_netdev 0 0
```

使用umount命令卸载文件系统

```bash
[root@node ~]# umount mount_point 
```

### 删除CephFS

如果需要，可以删除CephFS。但是，首先要备份所有数据，因为删除CephFS文件系统会破坏该文件系统上存储的所有数据。

删除cepphfs的步骤首先是将其标记为down，如下所示

```bash
[ceph: root@server /]# ceph fs set fs-name down true 
```

然后，可以使用下一个命令删除它

```bash
[ceph: root@server /]# ceph fs rm fs-name --yes-i-really-mean-it
```

### NFS服务器的用户空间实现

Red Hat Ceph Storage 5通过NFS Ganesha从NFS客户端提供对Ceph存储的访问

NFS Ganesha是一个用户空间NFS文件服务器，支持多种协议，如NFSv3、NFSv4.0、NFSv4.1和pNFS。NFS Ganesha使用文件系统抽象层(FSAL)架构，来支持和共享来自多个文件系统或较低级别存储的文件，例如Ceph、Samba、Gluster和Linux文件系统，如XFS

在Red Hat Ceph Storage中，NFS Ganesha使用NFS 4.0或更高协议共享文件。对于cepphfs客户端、OpenStack Manila文件共享服务和其他配置为访问NFS Ganesha服务的Red Hat产品，这个需求是必要的

以下列出了用户空间NFS服务器的优点:

1. 服务器不实现系统调用

2. 更有效地定义和使用缓存

3. 服务故障转移和重新启动更快更容易实现

4. 可以轻松地对用户空间服务进行集群，以获得高可用性

5. 可以使用分布式锁管理(DLM)来支持多个客户机协议

6. 服务器问题的调试更简单，因此不需要创建内核转储

7. 资源管理和性能监控更加简单

可以通过入口服务在现有的cepphfs文件系统上以active-active配置部署NFS Ganesha。这种active-active配置的主要目标是实现负载平衡，并扩展到许多处理更高负载的实例。因此，如果一个节点发生故障，那么集群将所有工作负载重定向到其他节点

系统管理员可以通过CLI部署NFS Ganesha守护进程，或者在启用Cephadm或Rook协调器的情况下自动管理它们

下面列出了在现有NFS服务之上拥有入口服务的优点:

1. 用于访问NFS服务器的虚拟IP

2. 当一个节点故障时，将NFS服务迁移到其他节点，以缩短故障切换时间

3. NFS节点间负载均衡

ingress 实现还没有完全开发完成。它可以部署多个Ganesha实例，并在它们之间平衡负载，但主机之间的故障转移尚未完全实现。这个特性有望在未来的版本中使用

可以使用多个双活NFS Ganesha服务与Pacemaker的高可用性。Pacemaker组件负责所有与集群相关的活动，比如监控集群成员关系、管理服务和资源以及保护集群成员

前提条件是，创建一个ceph文件系统，并在ceph MGR节点上安装nfs-ganesha、nfs-ganesha-ceph、nfs-ganesha-rados-grace和nfs-ganesha-rados-urls

满足先决条件后，启用Ceph MGR NFS模块

```bash
[ceph: root@server /]# ceph mgr module enable nfs 
```

然后，创建NFS Ganesha集群

```bash
[ceph: root@server /]# ceph nfs cluster create cluster-name "node-list" 
```

节点列表是一个以逗号分隔的列表，其中部署了守护进程容器。

接下来，导出cephfs文件系统

```bash
[ceph: root@server /]# ceph nfs export create cephfs fs-name cluster-name pseudo-path 
```

伪路径参数是伪根路径

最后，在客户机节点上挂载导出的CephFS文件系统

```bash
[root@node -]# mount -t nfs -o port=ganesha-port node-name:_pseudo-path_path
```

### MDS自动扩容

CephFS共享文件系统需要至少一个主MDS服务才能正常运行，需要至少一个备MDS服务才能保证高可用性。MDS自动扩展模块可以确保有足够的MDS守护进程可用

此模块监视级别的数量和备用守护进程的数量，并调整协调器生成的MDS守护进程的数量

使用实例启用MDS自动扩展模块

```bash
[ceph: root@server /]# ceph mgr module enable mds_autoscaler
```

### 在另一个Ceph集群上复制CephFS

Red Hat Ceph Storage 5支持cepphfs多站点配置，用于两地三中心复制。因此，可以在另一个Red Hat Ceph存储集群上复制cepphfs文件系统。有了这个特性，可以故障转移到辅助CephFS文件系统，并重新启动使用它的应用程序。CephFS文件系统镜像特性需要使用cephfs-mirror

源集群和目标集群都必须使用Red Hat Ceph Storage版本5或更高版本

CephFS镜像特性是基于快照的。第一次快照同步需要将数据从源集群批量传输到远程集群。然后，对于下面的同步，镜像守护进程识别本地快照之间修改的文件，并在远程集群中同步这些文件。这种同步方式不需要查询远端集群(根据本地快照计算文件差异)，只需要将更新后的文件传输到远端集群，比其他需要批量向远端集群传输数据的同步方式速度更快。默认禁用CephFS镜像模块。要配置CephFS的快照镜像，必须在源集群和远程集群上启用mirroring模块:

```bash
[ceph: root@server /]# ceph mgr module enable mirroring 
```

然后，可以在源集群上部署CephFS镜像守护进程:

```bash
[ceph: root@source /]# ceph orch apply cephfs-mirror [node-name]
```

前面的命令在节点名称上部署CephFS镜像守护进程，并创建Ceph用户CephFS -mir镜像。对于每个CephFS对等体，必须在目标集群上创建一个用户

```bash
[ceph: root@target /]# ceph fs authorize fs-name client_ / rwps
```

此时，可以在源集群上启用镜像。指定文件系统必须启用镜像

```bash
[ceph: root@source /]# ceph fs snapshot mirror enable fs-name
```

下一步是准备目标peer。可以使用下一个命令在目标节点中创建对等引导程序

```bash
[ceph: root@target /]# ceph fs snapshot mirror peer_bootstrap create fs-name peer-name site-name
```

可以使用site-name字符串来标识目标存储集群。当目标peer被创建时，你必须从在目标集群上创建对等体导入引导令牌到源集群

```bash
[ceph: root@source /]# ceph fs snapshot mirror peer_bootstrap import fs-name bootstrap-token 
```

最后，使用如下命令在源集群上配置一个快照镜像目录:

```bash
[ceph: root@source /]# ceph fs snapshot mirror add fs-name path 
```

# 管理共享文件存储

## CephFS管理

使用以下命令管理CephFS文件系统

| 动作                 | 命令                                               |
| ------------------ | ------------------------------------------------ |
| 创建文件系统             | ceph fs new fs-name meta-pool data-pool          |
| 列出现有的文件系统          | ceph fs ls                                       |
| 删除文件系统             | ceph fs rm fs-name [ - -yes -i- really-mean -it] |
| 强制MDS进入故障状态        | ceph mds fail gid/name/role                      |
| 声明MDS修复，触发failback | ceph mds repaired role                           |

CephFS提供了检查和修复MDS日志(cephfs-journal-tool)或MDS表(cephfs-table-tool)的工具，以及检查和重建元数据(cephfs-data-scan)的工具

## 将文件映射到对象

对于故障排除来说，确定存储文件对象的osd是很有用的。目录或0 长度的文件可能在数据池中有任何关联的对象。

这个例子为Ceph中的一个文件检索对象映射信息:

检索文件的inode编号

```bash
[ceph: root@server /]# stat -c %i filepath 
1099511627776
```

将索引节点数转换为十六进制数。使用printf命令的%x格式化输出

```bash
[ceph: root@server /]# printf '%x\n' 1099511627776 
10000000000 
```

这个示例结合了前两个步骤

```bash
[ceph: root@server /]# printf '%x\n' $(stat -c %i filepath) 
```

在RADOS对象列表中搜索十六进制ID。一个大文件可能返回多个对象

```bash
[ceph: root@server /]# rados -p cephfs_data ls | grep 10000000000 
10000000000.00000000
```

检索返回对象的映射信息

```bash
[ceph: root@server /]# ceph osd map cephfs_data 10000000000.00000000 
osdmap e95 pool ' cephfs_data ' {3) object ' 10000000000.00000000 ' -> pg 3.f0b56f30 
(3.30) -> up ( [1,2], pl) acting ( [1,2], pl)
```

将这个输出解释为cephfs_data池(ID 3)的OSD映射的e95映射epoch映射了10000000000.00000000对象放置组3.30，在OSD 1和OSD 2上，OSD 1为主。如果处于up和acting状态的osd不相同，那么这意味着集群正在重新平衡或存在其他问题

## 控制RADOS文件布局

RADOS布局控制文件如何映射到对象。这些设置存储在CephFS中的虚拟扩展属性(xattrs)中。可以调整设置来控制使用对象的大小和存储它们的位置

布局属性最初是在CephFS文件系统顶部的目录中设置的。您可以手动设置其他目录或文件的布局属性。创建文件时，它会从父目录继承布局属性。如果没有在其父目录中设置布局属性，则使用最近的具有布局属性的祖先目录

文件的布局属性(比如这些例子)使用ceph.file.layout前缀

文件布局属性

| 属性                              | 描述                                       |
| ------------------------------- | ---------------------------------------- |
| ceph.file.layout.pool           | Ceph存储文件数据对象的池(通常是cephfs_data)           |
| ceph.file.layout.stripe_unit    | 用于文件RAID 0分布的数据块的大小(以字节为单位)              |
| ceph.file.layout.stripe_count   | 文件数据组成RAID 0“分条”的连续分条单元的个数               |
| ceph.file.layout.object_size    | 文件数据以字节为单位分割成RADOS对象(默认为4194304字节，即4mib) |
| ceph.file.layout.pool_namespace | 使用的名称空间(如果有的话)                           |

ceph.dir.layout前缀标识目录的布局属性

目录布局属性

| 属性                             | 描述                                  |
| ------------------------------ | ----------------------------------- |
| ceph.di.layout.pool            | 此属性指定Ceph存储目录数据对象的池(通常为cephfs_data) |
| ceph.dir.layout.stripe_unit    | 此属性指定一个目录的RAID 0分布数据块的大小(以字节为单位)    |
| ceph.dir.layout.stripe_count   | 该属性指定目录数据组成RAID 0分条的连续分条单元的个数       |
| ceph.dir.layout.object_size    | 目录数据拆分为RADOS对象，默认为4194304字节，即4mib   |
| ceph.dir.layout.pool_namespace | 这个属性指定使用的名称空间(如果有的话)                |

getfattr命令显示文件或目录的布局属性

```bash
[ceph: root@server /]# getfattr -n ceph.file.layout file-path 
# file: file-path ceph.file.layout="stripe unit=4194304 stripe count=1 object size=4194304 pool=cephfs_ data" 
[ceph: root@server /]# getfattr -n ceph.dir.layout directory-path 
# file : directory-path ceph.dir.layout="stripe unit=4194304 stripe count=1 object_size=4194304 pool=cephfs_data"
```

setfattr命令修改布局属性:

```bash
[ceph: root@server /]# setfattr -n ceph.file.layout.attribute -v value file 
[ceph: root@server /]# setfattr -n ceph.dir.layout.attribute -v value directory 
```

布局属性在数据最初保存到文件时设置。如果父目录的布局属性在文件创建后改变，那么文件的布局属性不会改变。此外，只有当文件为空时，才能更改文件的布局属性

### 使用情况和统计数据

可以使用虚拟扩展属性来获取有关CephFS文件系统使用的信息。当与目录上的ceph属性名称空间一起使用getfattr命令时，将返回该目录的递归统计信息列表

```bash
[ceph: root@server /]# getfattr -d -m ceph.dir.* directory-path 
file: directory-path 
ceph .dir.entries="l" 
ceph.dir.files="0" 
ceph.dir.rbytes="424617209" 
ceph.dir. rctime="1341146808.804098000" 
ceph.dir.rentries="39623" 
ceph.dir. rfiles="37362" 
ceph.dir.rsubdirs="2261" 
ceph .dir. subdirs="l" 
```

统计数据提供了详细的信息

CephFS统计:

| 属性                | 描述                                              |
| ----------------- | ----------------------------------------------- |
| ceph.dir.entries  | 直接分支的数量                                         |
| ceph.dir.files    | 目录中常规文件的数量                                      |
| ceph.dir.rbytes   | 子树中的总文件大小(目录及其所有子目录)                            |
| ceph.dir.rctime   | 子树中最近的创建时间(从epoch开始的秒数，1970-01-01 00:00:00 UTC) |
| ceph.dir.rentries | 子树的后代数                                          |
| ceph.dir.rfiles   | 子树中常规文件的数量                                      |
| ceph.dir.rsubdirs | 子树中的目录数                                         |
| ceph.dir.subdirs  | 目录中的目录数                                         |

## 管理快照

部署Red Hat Ceph Storage 5时，CephFS默认启用异步快照。这些快照存储在一个名为.snap。在早期的Red Hat Ceph Storage版本中，快照是默认禁用的，因为它们是一个实验特性

### 创建快照

使用cephfs set为已有的cephfs文件系统创建快照

```bash
[ceph: root@server /]# ceph fs set fs-name allow_new_snaps true 
```

要创建快照，首先在客户机节点上挂载CephFS文件系统。当有多个cepfs文件系统时，使用-o fs=_f s- name选项挂载一个cepfs文件系统。然后，在.snap目录。快照名称为新的子目录名称。该快照包含cepphfs文件系统中所有当前文件的副本

```bash
[root@node -]# mount.ceph server.example.com:/ /mnt/mycephfs 
[root@node -]# mkdir /mnt/mycephfs/.snap/snap-name
```

使用s选项授权客户端为CephFS文件系统创建快照

```bash
[ceph: root@target /)# ceph fs authorize fs-name client path rws
```

如果需要恢复一个文件，请将该文件从快照目录复制到另一个正常目录

```bash
[root@node -]# cp -a .snap/snap-name/file-name .
```

完全恢复快照。快照目录树，将普通条目替换为所选快照的副本

```bash
[root@node -]# rm -rf * 
[root@node -]# cp -a .snap/snap-name/* .
```

如果要丢弃一个快照，请删除对应的目录.snap。即使快照目录不为空，执行rmdir命令也会成功，无需递归执行rm命令

```bash
[root@node -)# rmdir .snap/snap-name
```

### 调度快照

可以使用CephFS调度快照。nap_schedule模块管理定时快照。可以使用此模块创建和删除快照计划。快照时间表信息存储在cepphfs元数据池中。要创建快照计划，首先在MGR节点上启用快照计划模块

```bash
[ceph: root@server /]# ceph mgr module enable snap_schedule
```

然后，添加新的快照时间表

```bash
[ceph: root@server /)# ceph fs snap-schedule add fs-path time-period [start-time] 
```

如果安装的版本低于Python 3.7，则开始时间字符串必须使用格式%Y-%m -%dT%H: %m: %S。对于Python 3.7或更高版本，可以使用更灵活的日期解析。为使用实例为“/volume”文件夹创建定时快照，使用ceph fs snap- schedule add命令，定时快照的时间间隔为每小时

在客户机节点上，查看.snap文件夹在安装的CephFS

```bash
[root@node -]# ls /mnt/mycephfs/.snap 
scheduled-2021-10-06-08_00_00 
scheduled-2021-10-06-09_00_00 
scheduled-2021-10-06-10_00_00
```

可以使用list选项列出某个路径的快照时间表:

```bash
[ceph: root@server /]# ceph fs snap-schedule list fs-path
```

使用status选项来验证快照计划的详细信息

```bash
[ceph: root@server /]# ceph fs snap-schedule status fs-path 
```

通过指定路径删除快照计划

```bash
[ceph: root@server /]# ceph fs snap-schedule remove fs-path 
```

通过激活和禁用选项可以激活和禁用快照调度。当添加快照时间表时，如果路径存在，则默认激活快照时间表。但是，如果该路径不存在，则将其设置为不活动的，以便稍后在创建该路径时激活它

```bash
[ceph: root@server /]# ceph fs snap-schedule activate/deactivate 
```
