| 编号  | 系统         | 角色                    | IP             | 主机名               | Ceph版本 |
| --- | ---------- | --------------------- | -------------- | ----------------- | ------ |
| 1   | Centos 8.5 | bootstrap，mon，mgr，osd | 192.168.30.130 | host1.xiaohui.cn  | Quincy |
| 2   | Centos 8.5 | mon，mgr，osd，rgw，mds   | 192.168.30.131 | host2.xiaohui.cn  | Quincy |
| 3   | Centos 8.5 | mon，mgr，rgw，mds       | 192.168.30.132 | host3.xiaohui.cn  | Quincy |
| 4   | Centos 8.5 | client                | 192.168.30.133 | client.xiaohui.cn | Quincy |

作者：李晓辉

联系方式：

1. 微信：Lxh_Chat

2. QQ：939958092

3. 邮箱：[xiaohui_li@foxmail.com](mailto:xiaohui_li@foxmail.com)

# 管理RADOS块设备

## RBD的块存储介绍

RBD (RADOS Block Device)提供Ceph存储集群的块存储。RADOS在Ceph存储集群的池中为RBD映像的虚拟块设备提供了存储

## 提供RDB块存储

### 创建存储池

首先需要创建一个池用于存放RDB镜像，然后将该池设置为RBD的应用程序类型

```bash
[ceph: root@host1 /]# ceph osd pool create rbdpool
pool 'rbdpool' created
[ceph: root@host1 /]# rbd pool init rbdpool
```

查询池的大小以及可用信息，发现我们目前rbdpool有32个PG，最大为475GiB

```bash
[ceph: root@host1 /]# ceph df
--- RAW STORAGE ---
CLASS     SIZE    AVAIL     USED  RAW USED  %RAW USED
ssd    4.9 TiB  4.9 TiB  274 MiB   274 MiB          0
TOTAL  4.9 TiB  4.9 TiB  274 MiB   274 MiB          0

--- POOLS ---
POOL                 ID  PGS   STORED  OBJECTS     USED  %USED  MAX AVAIL
.mgr                  1    1  449 KiB        2  1.3 MiB      0    475 GiB
.rgw.root             2   32  1.3 KiB        4   48 KiB      0    475 GiB
default.rgw.log       3   32  3.6 KiB      177  408 KiB      0    475 GiB
default.rgw.control   4   32      0 B        8      0 B      0    475 GiB
default.rgw.meta      5   32  4.0 KiB        2   23 KiB      0    475 GiB
cephfs.lxhfs.meta    13   16   24 KiB       22  154 KiB      0    475 GiB
cephfs.lxhfs.data    14   32      0 B        0      0 B      0    475 GiB
rbdpool              18   32      0 B        0      0 B      0    475 GiB
[ceph: root@host1 /]# 
```

### 客户端安装ceph-common

 客户端需要安装ceph-common来操作ceph，安装前，需要完成ceph的repository

```bash
cat > /etc/yum.repos.d/ceph.repo <<EOF
[cephrepo]
name=ceph
baseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/8-stream/storage/x86_64/ceph-quincy/
enabled=1
gpgcheck=0
EOF
```

安装ceph-common

```bash
[root@client ~]# yum install ceph-common -y
```

### 创建客户端账号

创建一个名为rbduse的账号以供客户端来连接并使用此存储池，要注意需要把keyring和ceph.conf发给客户端

```bash
[ceph: root@host1 /]# ceph auth get-or-create client.rbduse mon 'profile rbd' osd 'profile rbd' -o /etc/ceph/ceph.client.rbduse.keyring
```

### 分发配置文件和keyring

```bash
[ceph: root@host1 /]# scp /etc/ceph/ceph.conf /etc/ceph/ceph.client.rbduse.keyring root@192.168.30.133:/etc/ceph/
root@192.168.30.133's password: 
ceph.conf                                                                           100%  339   533.3KB/s   00:00    
ceph.client.rbduse.keyring                                                          100%   64   110.0KB/s   00:00    
```

### 创建RBD镜像

现在客户端已经有了足够的权限，可以自主操作了，我们在客户端上完成所需的镜像操作

创建一个500M的镜像，为了避免每次都要输入id，可以临时用变量来设置ID

```bash
[root@client ~]# export CEPH_ARGS='--id=rbduse'
```

```bash
[root@client ~]# rbd create rbdpool/test --size=500M
[root@client ~]# rbd list -p rbdpool
test
```

### 查询RBD镜像参数

```bash
[root@client ~]# rbd info rbdpool/test
rbd image 'test':
        size 500 MiB in 125 objects
        order 22 (4 MiB objects)
        snapshot_count: 0
        id: 82856fceb2f83
        block_name_prefix: rbd_data.82856fceb2f83
        format: 2
        features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
        op_features: 
        flags: 
        create_timestamp: Fri Sep 16 17:05:23 2022
        access_timestamp: Fri Sep 16 17:05:23 2022
        modify_timestamp: Fri Sep 16 17:05:23 2022
```

### 挂载RBD到本地使用

可以看到多了一个名为rbd0的500M的磁盘

```bash
[root@client ~]# rbd map rbdpool/test
/dev/rbd0
[root@client ~]# lsblk
NAME        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
rbd0        252:0    0  500M  0 disk 
```

查看RBD挂载情况

```bash
[root@client ~]# rbd showmapped
id  pool     namespace  image  snap  device   
0   rbdpool             test   -     /dev/rbd0
```

### 格式化挂载RBD设备

```bash
[root@client ~]# mkfs.xfs /dev/rbd0
meta-data=/dev/rbd0              isize=512    agcount=8, agsize=16000 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=1, sparse=1, rmapbt=0
         =                       reflink=1
data     =                       bsize=4096   blocks=128000, imaxpct=25
         =                       sunit=16     swidth=16 blks
naming   =version 2              bsize=4096   ascii-ci=0, ftype=1
log      =internal log           bsize=4096   blocks=1872, version=2
         =                       sectsz=512   sunit=16 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
Discarding blocks...Done.
```

我们挂载到/mnt上

```bash
[root@client ~]# mount /dev/rbd0 /mnt/
[root@client ~]# df -h | grep rbd0
/dev/rbd0            493M   29M  464M   6% /mnt
```

### 永久挂载

rbd的设备永久挂载一定要注意，这不是本地设备，需要ceph先操作把rbd0的设备映射成本地设备，然后才能本地挂载，不然你启动的时候挂载会卡住无法启动系统

先把rbd映射写好，并启动服务，如果不启用并启动服务，就需要每次系统启动后执行rbd map命令映射成本地设备，关机的时候执行rbd unmap

```bash
[root@client ~]# vim /etc/ceph/rbdmap 
[root@client ~]# tail -n1 /etc/ceph/rbdmap
rbdpool/test id=rbduse,keyring=/etc/ceph/ceph.client.rbduse.keyring
[root@client ~]# systemctl enable rbdmap --now
```

写入fstab，并重启测试

```bash
[root@client ~]# vim /etc/fstab 
[root@client ~]# tail -n1 /etc/fstab
/dev/rbd0 /mnt xfs defaults,noauto 0 0 
[root@client ~]# reboot
```

### 写入数据

```bash
[root@client ~]# dd if=/dev/zero of=/mnt/test.data bs=10M count=10
10+0 records in
10+0 records out
104857600 bytes (105 MB, 100 MiB) copied, 0.0647059 s, 1.6 GB/s
```

### 查询rados数据

```bash
[root@client ~]# export CEPH_ARGS='--id=rbduse'
[root@client ~]# rados -p rbdpool ls
rbd_data.82856fceb2f83.0000000000000014
rbd_data.82856fceb2f83.0000000000000012
rbd_data.82856fceb2f83.000000000000000f
rbd_data.82856fceb2f83.0000000000000023
rbd_data.82856fceb2f83.0000000000000013
rbd_data.82856fceb2f83.0000000000000019
rbd_id.test
rbd_data.82856fceb2f83.0000000000000000
rbd_data.82856fceb2f83.000000000000006d
rbd_data.82856fceb2f83.000000000000001a
rbd_data.82856fceb2f83.0000000000000015
rbd_data.82856fceb2f83.0000000000000028
```

### 查询RBD镜像实际使用量

```bash
[ceph: root@host1 /]# rbd disk-usage --pool rbdpool test
NAME        PROVISIONED  USED   
test            500 MiB  0 B
<TOTAL>         500 MiB  0 MiB
```

### RADOS块设备镜像布局

RBD镜像中的所有对象都有一个名称，如上一个步骤所示，以每个RBD镜像的“RBD Block name Prefix”字段的值开头，通过RBD info命令显示。在这个前缀之后，有一个句点(.)，后面跟着对象编号。对象编号字段的值是一个12个字符的十六进制数

```bash
[root@client ~]# rbd -p rbdpool info test
rbd image 'test':
        size 500 MiB in 125 objects
        order 22 (4 MiB objects)
        snapshot_count: 0
        id: 82856fceb2f83
        block_name_prefix: rbd_data.82856fceb2f83
        format: 2
        features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
        op_features: 
        flags: 
        create_timestamp: Fri Sep 16 17:05:23 2022
        access_timestamp: Fri Sep 16 17:05:23 2022
        modify_timestamp: Fri Sep 16 17:05:23 2022
```

### RBD镜像格式

每个RBD镜像都有三个相关参数:，

**image_format** RBD镜像格式版本。默认值为2，即最新版本。版本1已被弃用，因为不支持克隆和镜像等特性

**stripe_unit** 一个对象中存储的连续字节数，默认为object_size

**stripe_count** 条带跨越的RBD映像对象数目，默认为1

# 管理RADOS块设备快照与克隆

## RBD 快照

RBD快照是在特定时间创建的RBD映像的只读副本。RBD快照使用COW技术来减少维护快照所需的存储空间。当集群对RBD快照镜像发起写I/O请求时，需要先将原始数据复制到该RBD快照镜像所在放置组的其他区域。快照在创建时不消耗任何存储空间，而是随着它们所包含的对象的大小而增长改变，而且RBD映像支持增量快照

在创建快照之前，使用fsfreeze命令暂停对文件系统的访问。fsfreeze --freeze命令停止对文件系统的访问，并在磁盘上创建一个稳定的映像。当文件系统未冻结时，不要进行文件系统快照，因为这会破坏快照的文件系统。快照完成后，使用fsfreeze - -unfreeze命令恢复对文件系统的操作和访问

创建Ceph设备的快照，给rbdpool中的test创建一个名为snap1的快照

先冻结文件系统访问，然后拍摄快照，然后再接触冻结

```bash
[root@client ~]# fsfreeze --freeze /mnt 
[root@client ~]# rbd snap create rbdpool/test@snap1
Creating snap: 100% complete...done.
[root@client ~]# rbd snap ls rbdpool/test
SNAPID  NAME   SIZE     PROTECTED  TIMESTAMP               
     6  snap1  500 MiB             Fri Sep 16 20:22:40 2022
[root@client ~]# fsfreeze --unfreeze /mnt 
```

## 快照恢复

先接触挂载，先把数据删除，然后挂载测试恢复后数据是否回来

```bash
[root@client ~]# umount /mnt
[root@client ~]# rbd unmap rbdpool/test
[root@client ~]# rbd snap rollback rbdpool/test@snap1
Rolling back to snapshot: 100% complete...done.
```

挂载后发现数据已经恢复成功

```bash
[root@client ~]# rbd map rbdpool/test
/dev/rbd0
[root@client ~]# mount /dev/rbd0 /mnt/
[root@client ~]# ls /mnt/ | more
abc
adjtime
aliases
anacrontab
bashrc
```

## 克隆与快照技术对比

快照就是针对RBD某个时间点状态的复制，以后出了问题，可以回到那个时间点，这么说来，快照是只读的，而克隆一般是基于某个快照，默认启用的是写时复制技术，克隆可以看做是一个可读可写的RBD镜像，其创建之初，并不占用任何物理空间，你在写入或修改某些东西时，从其基于的快照复制过来，然后修改并保存，此时才占用空间，当然，也可以用命令直接斩断其和上级的关系，成为一个独立的RBD镜像，此时会把所有缺失的内容从上级拷贝过来。

## 创建克隆

创建克隆一般有三个步骤

1. 创建快照

2. 保护快照不被删除

3. 创建克隆

我们已经有有一个名为snap1的快照，基于它，我们创建一个clone1的克隆

```bash
[root@client ~]# rbd snap protect rbdpool/test@snap1
[root@client ~]# rbd clone rbdpool/test@snap1 rbdpool/clone1
[root@client ~]# rbd ls -p rbdpool
clone1
test
```

查询快照占用的空间，我们发现其实际上没有占用空间

```bash
[root@client ~]# rbd disk-usage -p rbdpool
NAME        PROVISIONED  USED   
clone1          500 MiB      0 B
test@snap1      500 MiB  136 MiB
test            500 MiB  136 MiB
<TOTAL>        1000 MiB  272 MiB
```

我们可以用info来看到它的父级关系

```bash
[root@client ~]# rbd info rbdpool/clone1
rbd image 'clone1':
        size 500 MiB in 125 objects
        order 22 (4 MiB objects)
        snapshot_count: 0
        id: 84f92ea4087d0
        block_name_prefix: rbd_data.84f92ea4087d0
        format: 2
        features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
        op_features: 
        flags: 
        create_timestamp: Fri Sep 16 21:44:42 2022
        access_timestamp: Fri Sep 16 21:44:42 2022
        modify_timestamp: Fri Sep 16 21:44:42 2022
        parent: rbdpool/test@snap1
        overlap: 500 MiB
```

## 使用克隆

我们将克隆映射到客户端本地，完成读写验证，并确认其磁盘占用空间变大

要挂克隆到本地，你需要卸载前面挂载的数据卷，挂载克隆后，我们发现其数据还在

```bash
[root@client ~]# umount /mnt
[root@client ~]# rbd map rbdpool/clone1
/dev/rbd1
[root@client ~]# mount /dev/rbd1 /opt
[root@client ~]# ls /opt | more
abc
adjtime
aliases
anacrontab
bashrc
```

```bash
[root@client ~]# rbd disk-usage -p rbdpool
NAME        PROVISIONED  USED   
clone1          500 MiB   12 MiB
test@snap1      500 MiB  136 MiB
test            500 MiB  476 MiB
<TOTAL>        1000 MiB  624 MiB
```

## 合并克隆

将数据从快照中合并到我们的克隆

```bash
[root@client ~]# rbd flatten rbdpool/clone1
Image flatten: 100% complete...done.
[root@client ~]# rbd disk-usage -p rbdpool
NAME        PROVISIONED  USED   
clone1          500 MiB   36 MiB
test@snap1      500 MiB  136 MiB
test            500 MiB  476 MiB
<TOTAL>        1000 MiB  648 MiB
```

此时再看，就发现已经不再存在父级关系，成为了独立的镜像

```bash
[root@client ~]# rbd info rbdpool/clone1
rbd image 'clone1':
        size 500 MiB in 125 objects
        order 22 (4 MiB objects)
        snapshot_count: 0
        id: 84f92ea4087d0
        block_name_prefix: rbd_data.84f92ea4087d0
        format: 2
        features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
        op_features: 
        flags: 
        create_timestamp: Fri Sep 16 21:44:42 2022
        access_timestamp: Fri Sep 16 21:44:42 2022
        modify_timestamp: Fri Sep 16 21:44:42 2022
```

# 导入和导出RBD镜像

## 导入和导出RBD镜像

RBD导出和导入机制允许维护RBD映像的可操作副本，在同一个集群中或通过使用单独的集群实现功能完整且可访问的。可以将这些副本用于各种用例，包括:

1. 使用真实的数据量测试新版本

2. 使用真实的数据量运行质量保证流程

3. 使用真实的数据量实现业务连续性场景

4. 将备份流程与生产块设备解耦

RADOS块设备特性提供了导出和导入整个RBD映像或仅RBD映像在两个时间点之间变化的能力

### 导出RBD镜像

Ceph Storage提供了rbd export命令来导出rbd镜像到文件中

```bash
[root@client ~]# rbd export rbdpool/test export.dat
Exporting image: 100% complete...done.
```

### 导入RBD镜像

Ceph Storage提供了rbd import命令，用于从文件中导入rbd镜像。该命令创建一个test2新映像

```bash
[root@client ~]# rbd import export.dat rbdpool/test2
Importing image: 100% complete...done.
[root@client ~]# rbd ls -p rbdpool
clone1
test
test2
```

## 导出和导入RBD镜像的变化部分

Ceph Storage提供rbd export-diff和rbd import-diff命令来导出和导入rbd镜像上两个时间点之间的更改。语法与rbd export、rbd import命令相同

时间端点可以是:

1. RBD镜像的当前内容，如poolname/imagename

2. RBD镜像的快照，例如poolname/imagename@snapname

开始时间包括:

1. RBD映像的创建日期和时间。例如，不使用- -from- snap选项

2. RBD镜像的快照，例如使用--from-snap snapname选项获取

如果指定了起始点快照，该命令将在创建该快照后导出所做的更改，如果不指定快照，该命令将导出自创建以来的所有更改RBD镜像，与常规的RBD镜像导出操作相同

import-diff操作执行以下有效性检查:

1. 如果export-diff是相对于一个开始快照，那么这个快照也必须存在于目标RBD映像中

2. 如果使用export-diff命令时指定了一个结束快照，则导入数据后在目标RBD镜像中创建相同的快照名称

### 导出两个快照的不同之处

我们已经有一个包含数据的snap1，我们再向test镜像中写入点内容，然后重做一个新快照

```bash
[root@client ~]# rbd snap ls rbdpool/test
SNAPID  NAME   SIZE     PROTECTED  TIMESTAMP               
     6  snap1  500 MiB  yes        Fri Sep 16 20:22:40 2022
[root@client ~]# rbd map rbdpool/test
/dev/rbd0
[root@client ~]# mount /dev/rbd0 /mnt
[root@client ~]# dd if=/dev/zero of=/mnt/test2.data bs=10M count=1
1+0 records in
1+0 records out
10485760 bytes (10 MB, 10 MiB) copied, 0.0103503 s, 1.0 GB/s
[root@client ~]# fsfreeze --freeze /mnt 
[root@client ~]# rbd snap create rbdpool/test@snap2
Creating snap: 100% complete...done.
[root@client ~]# fsfreeze --unfreeze /mnt 
[root@client ~]# rbd snap ls rbdpool/test
SNAPID  NAME   SIZE     PROTECTED  TIMESTAMP               
     6  snap1  500 MiB  yes        Fri Sep 16 20:22:40 2022
     9  snap2  500 MiB             Fri Sep 16 22:07:43 2022
```

导出其不同之处，这就相当于做了一个增量备份

```bash
[root@client ~]# rbd export-diff --from-snap snap1 rbdpool/test@snap2 diff.dat
Exporting image: 100% complete...done.
```

### 导入这个增量数据到镜像

不要随便导入这个增量数据，除非你发生了数据丢失，比如snap2不见了

```bash
[root@client ~]# rbd snap rm rbdpool/test@snap2
Removing snap: 100% complete...done.
[root@client ~]# rbd import-diff ./diff.dat rbdpool/test
Importing image diff: 100% complete...done.
```


