| 编号  | 系统         | 角色                    | IP             | 主机名               | Ceph版本 |
| --- | ---------- | --------------------- | -------------- | ----------------- | ------ |
| 1   | Centos 8.5 | bootstrap，mon，mgr，osd | 192.168.30.130 | host1.xiaohui.cn  | Quincy |
| 2   | Centos 8.5 | mon，mgr，osd，rgw，mds   | 192.168.30.131 | host2.xiaohui.cn  | Quincy |
| 3   | Centos 8.5 | mon，mgr，rgw，mds       | 192.168.30.132 | host3.xiaohui.cn  | Quincy |
| 4   | Centos 8.5 | client                | 192.168.30.133 | client.xiaohui.cn | Quincy |

作者：李晓辉

联系方式：

1. 微信：Lxh_Chat

2. QQ：939958092

3. 邮箱：[xiaohui_li@foxmail.com](mailto:xiaohui_li@foxmail.com)

# 管理和定制CRUSH Map

# 管理RADOS块设备

## RBD的块存储介绍

RBD (RADOS Block Device)提供Ceph存储集群的块存储。RADOS在Ceph存储集群的池中为RBD映像的虚拟块设备提供了存储

## 提供RDB块存储

### 创建存储池

首先需要创建一个池用于存放RDB镜像，然后将该池设置为RBD的应用程序类型

```bash
[ceph: root@host1 /]# ceph osd pool create rbdpool
pool 'rbdpool' created
[ceph: root@host1 /]# rbd pool init rbdpool
```

查询池的大小以及可用信息，发现我们目前rbdpool有32个PG，最大为475GiB

```bash
[ceph: root@host1 /]# ceph df
--- RAW STORAGE ---
CLASS     SIZE    AVAIL     USED  RAW USED  %RAW USED
ssd    4.9 TiB  4.9 TiB  274 MiB   274 MiB          0
TOTAL  4.9 TiB  4.9 TiB  274 MiB   274 MiB          0

--- POOLS ---
POOL                 ID  PGS   STORED  OBJECTS     USED  %USED  MAX AVAIL
.mgr                  1    1  449 KiB        2  1.3 MiB      0    475 GiB
.rgw.root             2   32  1.3 KiB        4   48 KiB      0    475 GiB
default.rgw.log       3   32  3.6 KiB      177  408 KiB      0    475 GiB
default.rgw.control   4   32      0 B        8      0 B      0    475 GiB
default.rgw.meta      5   32  4.0 KiB        2   23 KiB      0    475 GiB
cephfs.lxhfs.meta    13   16   24 KiB       22  154 KiB      0    475 GiB
cephfs.lxhfs.data    14   32      0 B        0      0 B      0    475 GiB
rbdpool              18   32      0 B        0      0 B      0    475 GiB
[ceph: root@host1 /]# 
```

### 客户端安装ceph-common

 客户端需要安装ceph-common来操作ceph，安装前，需要完成ceph的repository

```bash
cat > /etc/yum.repos.d/ceph.repo <<EOF
[cephrepo]
name=ceph
baseurl=https://mirrors.tuna.tsinghua.edu.cn/centos/8-stream/storage/x86_64/ceph-quincy/
enabled=1
gpgcheck=0
EOF
```

安装ceph-common

```bash
[root@client ~]# yum install ceph-common -y
```

### 创建客户端账号

创建一个名为rbduse的账号以供客户端来连接并使用此存储池，要注意需要把keyring和ceph.conf发给客户端

```bash
[ceph: root@host1 /]# ceph auth get-or-create client.rbduse mon 'profile rbd' osd 'profile rbd' -o /etc/ceph/ceph.client.rbduse.keyring
```

### 分发配置文件和keyring

```bash
[ceph: root@host1 /]# scp /etc/ceph/ceph.conf /etc/ceph/ceph.client.rbduse.keyring root@192.168.30.133:/etc/ceph/
root@192.168.30.133's password: 
ceph.conf                                                                           100%  339   533.3KB/s   00:00    
ceph.client.rbduse.keyring                                                          100%   64   110.0KB/s   00:00    
```

### 创建RBD镜像

现在客户端已经有了足够的权限，可以自主操作了，我们在客户端上完成所需的镜像操作

创建一个500M的镜像，为了避免每次都要输入id，可以临时用变量来设置ID

```bash
[root@client ~]# export CEPH_ARGS='--id=rbduse'
```

```bash
[root@client ~]# rbd create rbdpool/test --size=500M
[root@client ~]# rbd list -p rbdpool
test
```

### 查询RBD镜像参数

```bash
[root@client ~]# rbd info rbdpool/test
rbd image 'test':
        size 500 MiB in 125 objects
        order 22 (4 MiB objects)
        snapshot_count: 0
        id: 82856fceb2f83
        block_name_prefix: rbd_data.82856fceb2f83
        format: 2
        features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
        op_features: 
        flags: 
        create_timestamp: Fri Sep 16 17:05:23 2022
        access_timestamp: Fri Sep 16 17:05:23 2022
        modify_timestamp: Fri Sep 16 17:05:23 2022
```

### 挂载RBD到本地使用

可以看到多了一个名为rbd0的500M的磁盘

```bash
[root@client ~]# rbd map rbdpool/test
/dev/rbd0
[root@client ~]# lsblk
NAME        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
rbd0        252:0    0  500M  0 disk 
```

查看RBD挂载情况

```bash
[root@client ~]# rbd showmapped
id  pool     namespace  image  snap  device   
0   rbdpool             test   -     /dev/rbd0
```

### 格式化挂载RBD设备

```bash
[root@client ~]# mkfs.xfs /dev/rbd0
meta-data=/dev/rbd0              isize=512    agcount=8, agsize=16000 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=1, sparse=1, rmapbt=0
         =                       reflink=1
data     =                       bsize=4096   blocks=128000, imaxpct=25
         =                       sunit=16     swidth=16 blks
naming   =version 2              bsize=4096   ascii-ci=0, ftype=1
log      =internal log           bsize=4096   blocks=1872, version=2
         =                       sectsz=512   sunit=16 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
Discarding blocks...Done.
```

我们挂载到/mnt上

```bash
[root@client ~]# mount /dev/rbd0 /mnt/
[root@client ~]# df -h | grep rbd0
/dev/rbd0            493M   29M  464M   6% /mnt
```

### 永久挂载

rbd的设备永久挂载一定要注意，这不是本地设备，需要ceph先操作把rbd0的设备映射成本地设备，然后才能本地挂载，不然你启动的时候挂载会卡住无法启动系统

先把rbd映射写好，并启动服务，如果不启用并启动服务，就需要每次系统启动后执行rbd map命令映射成本地设备，关机的时候执行rbd unmap

```bash
[root@client ~]# vim /etc/ceph/rbdmap 
[root@client ~]# tail -n1 /etc/ceph/rbdmap
rbdpool/test id=rbduse,keyring=/etc/ceph/ceph.client.rbduse.keyring
[root@client ~]# systemctl enable rbdmap --now
```

写入fstab，并重启测试

```bash
[root@client ~]# vim /etc/fstab 
[root@client ~]# tail -n1 /etc/fstab
/dev/rbd0 /mnt xfs defaults,noauto 0 0 
[root@client ~]# reboot
```

### 写入数据

```bash
[root@client ~]# dd if=/dev/zero of=/mnt/test.data bs=10M count=10
10+0 records in
10+0 records out
104857600 bytes (105 MB, 100 MiB) copied, 0.0647059 s, 1.6 GB/s
```

### 查询rados数据

```bash
[root@client ~]# export CEPH_ARGS='--id=rbduse'
[root@client ~]# rados -p rbdpool ls
rbd_data.82856fceb2f83.0000000000000014
rbd_data.82856fceb2f83.0000000000000012
rbd_data.82856fceb2f83.000000000000000f
rbd_data.82856fceb2f83.0000000000000023
rbd_data.82856fceb2f83.0000000000000013
rbd_data.82856fceb2f83.0000000000000019
rbd_id.test
rbd_data.82856fceb2f83.0000000000000000
rbd_data.82856fceb2f83.000000000000006d
rbd_data.82856fceb2f83.000000000000001a
rbd_data.82856fceb2f83.0000000000000015
rbd_data.82856fceb2f83.0000000000000028
```


