| 编号  | 系统         | 角色                    | IP             | 主机名              | Ceph版本 |
| --- | ---------- | --------------------- | -------------- | ---------------- | ------ |
| 1   | Centos 8.5 | bootstrap，mon，mgr，osd | 192.168.30.130 | host1.xiaohui.cn | Quincy |
| 2   | Centos 8.5 | mon，mgr，osd，rgw，mds   | 192.168.30.131 | host2.xiaohui.cn | Quincy |
| 3   | Centos 8.5 | mon，mgr，rgw，mds       | 192.168.30.132 | host3.xiaohui.cn | Quincy |

作者：李晓辉

联系方式：

1. 微信：Lxh_Chat

2. QQ：939958092

3. 邮箱：[xiaohui_li@foxmail.com](mailto:xiaohui_li@foxmail.com) 

# 管理和定制CRUSH Map

## CRUSH和目标放置策略

Ceph通过一种称为CRUSH(可伸缩哈希下的受控复制)的放置算法来计算哪些osd应该持有哪些对象，对象被分配到放置组(pg)， CRUSH决定这些放置组应该使用哪个osd来存储它们的对象

### CRUSH的算法

CRUSH算法使Ceph客户端能够直接与osd通信，这避免了集中式服务瓶颈，Ceph客户端和osd使用CRUSH算法高效地计算对象位置的信息，而不是依赖于一个中央查找服务器。Ceph客户端检索集群映射，并使用CRUSH映射从算法上确定如何存储和检索数据，通过避免单点故障和性能瓶颈，这为Ceph集群提供了大规模的可伸缩性

CRUSH算法的作用是将数据统一分布在对象存储中，管理复制，并响应系统增长和硬件故障，当新增OSD或已有OSD或OSD主机故障时，Ceph通过CRUSH在主OSD间实现集群对象的再平衡

### CRUSH Map 组件

从概念上讲，一个CRUSH map包含两个主要组件：

**CRUSH层次结构**

这将列出所有可用的osd，并将它们组织成树状的Bucket结构

CRUSH层次结构通常用来表示osd的位置，默认情况下，有一个root Bucket代表整个层次结构，其中包含每个OSD主机的一个主机Bucket

OSD是树的叶子节点，默认情况下，同一个OSD主机上的所有OSD都放在该主机的桶中，也可以自定义树状结构，重新排列，增加层次，将OSD主机分组到不同的桶中，表示其在不同的服务器机架或数据中心的位置

**至少有一条CRUSH规则**

CRUSH 规则决定了如何从这些桶中分配放置组的osd，这决定了这些放置组的对象的存储位置。不同的池可能会使用不同的CRUSH规则

### CRUSH Bucket类型

CRUSH层次结构将osd组织成一个由不同容器组成的树，称为桶(Bucket)。对于大型安装，可以创建特定的层次结构来描述存储基础设施：数据中心、机架、主机和OSD设备。通过创建一个CRUSH map规则，可以使Ceph将一个对象的副本放在独立服务器上的osd上，放在不同机架的服务器上，甚至放在不同数据中心的服务器上

总而言之，桶是CRUSH层次结构中的容器或分支。osd设备是CRUSH等级中的叶子

一些最重要的桶属性有:

1. 桶ID，这些id为负数，以便与存储设备的id区分开来

2. 桶的名称

3. 桶的类型，默认映射定义了几种类型，可以使用ceph osd crush dump命令检索这些类型

桶类型包括root、region、datacenter、room、pod、pdu、row、rack、chassis和host，但你也可以添加自己的类型、位于层次结构根的桶属于根类型

Ceph 使用一定的算法将PG副本映射到osd，有几种算法可用:uniform、list、tree和straw2。每种算法都代表了性能和重组效率之间的权衡。缺省算法为straw2

# 自定义故障和性能域

CRUSH映射是CRUSH算法的中心配置机制，可以编辑此map以影响数据放置并自定义CRUSH算法

配置CRUSH映射和创建单独的故障域允许osd和集群节点发生故障，而不会发生任何数据丢失。在问题解决之前，集群只是以降级状态运行

配置CRUSH映射并创建单独的性能域可以减少使用集群存储和检索数据的客户机和应用程序的性能瓶颈。例如，CRUSH可以为hdd创建一个层次结构，为ssd创建另一个层次结构

定制CRUSH映射的一个典型用例是针对硬件故障提供额外的保护。可以配置CRUSH映射以匹配底层物理基础设施，这有助于减轻硬件故障的影响

默认情况下，CRUSH算法将复制的对象放置在不同主机上的osd上。可以定制CRUSH map，这样对象副本就可以跨osd放置在不同的机架上，或者放置在不同机房的主机上，或者放置在具有不同电源的不同机架上

另一个用例是将带有SSD驱动器的osd分配给需要快速存储的应用程序使用的池，而将带有传统hdd的osd分配给支持要求较低的工作负载的池

CRUSH map可以包含多个层次结构，你可以通过不同的CRUSH规则进行选择。通过使用单独的CRUSH层次结构，可以建立单独的性能域。配置单独性能域的用例示例如下：

1. 分离虚拟机使用的块存储和应用使用的对象存储

2. 将包含不经常访问的数据的“冷”存储区与包含经常访问的数据的“热”存储区分开

一个CRUSH map定义，它包含：

1. 所有可用物理存储设备的列表

2. 所有基础设施桶的列表，以及每个桶中存储设备或其他桶的id。bucket是基础结构树中的容器或分支，例如，它可能表示一个位置或一块物理硬件

3. 将pg映射到osd的CRUSH规则列表

4. 其他CRUSH可调参数及其设置的列表

集群安装过程部署一个默认的CRUSH映射，可以使用ceph osd crush dump命令打印JSON格式的crush map。你也可以导出映射的二进制副本，并将其反编译为文本文件:

```bash
[ceph: root@host1 /]# ceph osd getcrushmap -o ./map.bin
51
[ceph: root@host1 /]# crushtool -d ./map.bin -o ./map.txt 
```

## 自定义OSD CRUSH设置

CRUSH映射包含集群中所有存储设备的列表。对于每台存储设备，已获取如下信息:

1. 存储设备的ID

2. 存储设备的名称

3. 存储设备的权重，通常以tb为单位。例如，4tb的存储设备重量约为4.0。这是设备可以存储的相对数据量，CRUSH算法使用这一数据来帮助确保对象的均匀分布

可以通过ceph osd crush reweight命令设置OSD的权重。CRUSH的树桶权重应该等于它们的叶子权重的总和。如果手动编辑CRUSH映射权重，那么应该执行以下命令来确保CRUSH树桶的权重准确地反映了桶内叶片osd的总和

```bash
[ceph: root@host1 /]# ceph osd crush reweight-all 
reweighted crush hierarchy
```

4. 存储设备的类别，存储集群支持多种存储设备，如hdd、ssd、NVMe ssd等。存储设备的类反映了这些信息，可以使用这些信息创建针对不同应用程序工作负载优化的池。osd自动检测和设置它们的设备类。ceph osd crush set-device-class命令用于显式设置OSD的设备类。使用ceph osd crush rm device-class从osd中删除一个设备类

ceph osd crush tree命令显示crush map 当前的层级：

```bash
[ceph: root@host1 /]# ceph osd crush tree
ID  CLASS  WEIGHT   TYPE NAME     
-1         4.88297  root default  
-5         1.95319      host host1
 1    ssd  0.48830          osd.1 
 6    ssd  0.48830          osd.6 
 8    ssd  0.48830          osd.8 
11    ssd  0.48830          osd.11
-7         1.46489      host host2
 2    ssd  0.48830          osd.2 
 4    ssd  0.48830          osd.4 
 7    ssd  0.48830          osd.7 
-3         1.46489      host host3
 0    ssd  0.48830          osd.0 
 3    ssd  0.48830          osd.3 
 5    ssd  0.48830          osd.5 
```

使用ceph osd crush class create命令创建一个新的设备类

使用ceph osd crush class rm命令删除一个设备类

使用ceph osd crush class ls命令列出已配置的设备类

## 使用CRUSH规则

CRUSH map还包含数据放置规则，决定如何将pg映射到osd，以存储对象副本或erasure coded块，ceph osd crush rule ls命令在已有的规则基础上，打印规则详细信息。ceph osd crush rule dump rule_name命令打印规则详细信息，编译后的CRUSH map也包含规则，直接cat map.txt阅读

还可以使用可调参数修改CRUSH算法的行为。可调项可以调整、禁用或启用CRUSH算法的特性。Ceph在反编译的CRUSH映射的开始部分定义了可调参数，你可以使用下面的命令获取它们的当前值:

```bash
[ceph: root@host1 /]# ceph osd crush show-tunables
{
    "choose_local_tries": 0,
    "choose_local_fallback_tries": 0,
    "choose_total_tries": 50,
    "chooseleaf_descend_once": 1,
    "chooseleaf_vary_r": 1,
    "chooseleaf_stable": 1,
    "straw_calc_version": 1,
    "allowed_bucket_algs": 54,
    "profile": "jewel",
    "optimal_tunables": 1,
    "legacy_tunables": 0,
    "minimum_required_version": "jewel",
    "require_feature_tunables": 1,
    "require_feature_tunables2": 1,
    "has_v2_rules": 1,
    "require_feature_tunables3": 1,
    "has_v3_rules": 0,
    "has_v4_buckets": 1,
    "require_feature_tunables5": 1,
    "has_v5_rules": 0
}
```

调整CRUSH可调项可能会改变CRUSH将放置组映射到osd的方式。当这种情况发生时，集群需要将对象移动到集群中的不同osd，以反映重新计算的映射。在此过程中，集群性能可能会下降

## CRUSH Map 管理

集群有一个编译后的CRUSH map，你可以通过以下方式修改它:

1. 使用ceph osd crush命令

2. 提取二进制CRUSH映射并将其编译为纯文本，编辑文本文件，将其重新编译为二进制格式，然后将其导入到集群中

通常使用ceph osd crush命令更新CRUSH地图会更容易。但是，还有一些不太常见的场景只能通过使用第二种方法来实现

### 使用Ceph命令定制CRUSH地图

下面的例子创建了几个不同类型的新桶

```bash
[ceph: root@host1 /]# ceph osd crush add-bucket datacenter root
added bucket datacenter type root to crush map
[ceph: root@host1 /]# ceph osd crush add-bucket rack1 rack     
added bucket rack1 type rack to crush map
[ceph: root@host1 /]# ceph osd crush add-bucket rack2 rack
added bucket rack2 type rack to crush map
```

```bash
[ceph: root@host1 /]# ceph osd crush tree            
ID   CLASS  WEIGHT   TYPE NAME      
-11               0  rack rack2     
-10               0  rack rack1     
 -9               0  root datacenter
 -1         4.88297  root default   
 -5         1.95319      host host1 
  1    ssd  0.48830          osd.1  
  6    ssd  0.48830          osd.6  
  8    ssd  0.48830          osd.8  
 11    ssd  0.48830          osd.11 
 -7         1.46489      host host2 
  2    ssd  0.48830          osd.2  
  4    ssd  0.48830          osd.4  
  7    ssd  0.48830          osd.7  
 -3         1.46489      host host3 
  0    ssd  0.48830          osd.0  
  3    ssd  0.48830          osd.3  
  5    ssd  0.48830          osd.5  
```

将他们整理成结构化：

把rack1和rack2放入到datacenter的root桶，然后把主机拉入到不同的rack

```bash
[ceph: root@host1 /]# ceph osd crush move rack1 root=datacenter
moved item id -10 name 'rack1' to location {root=datacenter} in crush map
[ceph: root@host1 /]# ceph osd crush move host1 rack=rack1
moved item id -5 name 'host1' to location {rack=rack1} in crush map
[ceph: root@host1 /]# ceph osd crush move host2 rack=rack2
moved item id -7 name 'host2' to location {rack=rack2} in crush map
[ceph: root@host1 /]# ceph osd crush move rack2 root=datacenter
moved item id -11 name 'rack2' to location {root=datacenter} in crush map
[ceph: root@host1 /]# 
[ceph: root@host1 /]# ceph osd crush tree
ID   CLASS  WEIGHT   TYPE NAME         
 -9         3.41808  root datacenter   
-10         1.95319      rack rack1    
 -5         1.95319          host host1
  1    ssd  0.48830              osd.1 
  6    ssd  0.48830              osd.6 
  8    ssd  0.48830              osd.8 
 11    ssd  0.48830              osd.11
-11         1.46489      rack rack2    
 -7         1.46489          host host2
  2    ssd  0.48830              osd.2 
  4    ssd  0.48830              osd.4 
  7    ssd  0.48830              osd.7 
 -1         1.46489  root default      
 -3         1.46489      host host3    
  0    ssd  0.48830          osd.0     
  3    ssd  0.48830          osd.3     
  5    ssd  0.48830          osd.5     
```

在创建了自定义桶层次结构之后，自动将osd作为该树的叶子放置，当Ceph启动时，它使用ceph-crush-location工具来自动验证每个OSD都在正确的CRUSH位置。如果OSD不在CRUSH地图中预期的位置，它将被自动移动。默认情况下，这是root=default host=hostname

### 添加CRUSH Map规则

下面的示例创建新的rule1规则来在名为datacenter的数据中心存储副本，将副本分发到各个机架:

```bash
[ceph: root@host1 /]# ceph osd crush rule create-replicated rule1 datacenter rack
[ceph: root@host1 /]# ceph osd crush rule ls
replicated_rule
erasure-code
ecpool
rule1
```

定义规则后，在创建复制池时使用它:

```bash
[ceph: root@host1 /]# ceph osd pool create lxhpool2 rule1
pool 'lxhpool2' created
```

创建erasure池

创建一个和我们刚创建的crush关联的erasure 配置文件，然后在创建池的时候引用

```bash
[ceph: root@host1 /]# ceph osd erasure-code-profile set profile1 k=2 m=1 crush-root=datacenter crush-failure-domain=rack crush-device-class=ssd 
[ceph: root@host1 /]# ceph osd pool create myecpool erasure profile1
pool 'myecpool' created
[ceph: root@host1 /]# ceph osd pool ls detail | grep myecpool
pool 16 'myecpool' erasure profile profile1 size 3 min_size 2 crush_rule 4 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 1373 flags hashpspool stripe_width 8192
```

### 通过编译二进制版本自定义CRUSH地图

你可以用以下命令来反编译和手动编辑CRUSH地图:

| 命令                                   | 动作                         |
| ------------------------------------ | -------------------------- |
| ceph osd getcrushmap -o binfiIe      | 导出当前映射的二进制副本               |
| crushtool -d binfiIe -o textfiIepath | 将一个CRUSH映射二进制文件反编译成一个文本文件  |
| crushtool -c textfiIepath -o binfiIe | 从文本中编译一个CRUSH地图            |
| crushtool -i binfiIe --test          | 在二进制CRUSH地图上执行演练，并模拟放置组的创建 |
| ceph osd setcrushmap -i binfiIe      | 将二进制CRUSH映射导入集群            |

## 优化放置组

CRUSH会确保对象在池中osd之间的均匀分布，但也存在pg变得不平衡的情况。放置组自动缩放器可用于优化PG分发，并在默认情况下打开。如果需要，还可以手动设置每个池的pg数量

对象通常是均匀分布的，前提是池中比osd多一个或两个数量级(十个因子)的放置组。如果没有足够的pg，那么对象的分布可能会不均匀。如果池中存储了少量非常大的对象，那么对象分布可能会变得不平衡

应该配置pg，以便有足够的对象在集群中均匀分布。如果pg的数量设置过高，则会显著增加CPU和内存的使用。建议每个OSD大约100到200个放置组来平衡这些因素

### 计算放置组的数量

对于单个池的集群，可以使用以下公式，每个OSD 100个放置组

```bash
Total PGs = (OSDs * 100)/Number of replicas 
```

推荐使用每个池计算Ceph放置组，https://access.redhat.com/labs/cephpgc/manual
