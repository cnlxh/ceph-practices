| 编号  | 系统         | 角色                               | IP             | 主机名                   | Ceph版本 |
| --- | ---------- | -------------------------------- | -------------- | --------------------- | ------ |
| 1   | Centos 8.5 | bootstrap，mon，mgr，osd，iscsi      | 192.168.31.185 | ceph01.xiaohui.cn     | Quincy |
| 2   | Centos 8.5 | mon，mgr，osd，rgw，mds              | 192.168.31.129 | ceph02.xiaohui.cn     | Quincy |
| 3   | Centos 8.5 | mon，mgr，rgw，mds                  | 192.168.31.25  | ceph03.xiaohui.cn     | Quincy |
| 4   | Centos 8.5 | second ceph cluster，iscsi client | 192.168.31.34  | cephbackup.xiaohui.cn | Quincy |

作者：李晓辉

联系方式：

1. 微信：Lxh_Chat

2. QQ：939958092

3. 邮箱：[xiaohui_li@foxmail.com](mailto:xiaohui_li@foxmail.com)  

# 配置RBD镜像

## RBD镜像

Ceph Storage支持两个存储集群之间的RBD镜像。这允许自动将RBD映像从一个Ceph Storage集群复制到另一个远程集群。这种机制使用异步机制在网络上镜像源(主)RBD映像和目标(次)RBD映像。如果包含主RBD映像的集群不可用，那么可以从远程集群故障转移到辅助RBD映像，并重新启动使用它的应用程序。

当从源RBD镜像故障转移到镜像RBD镜像时，必须降级源RBD镜像，提升目标RBD镜像。一个被降级且被锁定不可用，并且提升后的映像在读/写模式下变得可用和可访问。

RBD镜像特性需要RBD-mirror守护进程，rbd-mirror守护进程从远程对等集群提取映像更新，并将它们应用到本地集群映像

## 支持的镜像配置

RBD镜像支持两种配置:

1. 单向镜像或active-passive 

2. 双向镜像或active-active

## 支持Mirroring模式

RBD镜像支持两种模式:pool模式和image模式

### pool模式

在池模式下，Ceph自动为镜像池中创建的每个RBD映像启用镜像。当在源集群的池中创建映像时，Ceph会在远程集群上创建辅助映像。

### image模式

在镜像模式中，可以有选择地为镜像池中的各个RBD映像启用镜像。在这种模式下，必须显式地选择要在两个集群之间复制的RBD映像。

## image模式

### 基于日志的镜像

这种模式使用RBD日志映像特性来确保两个Red Hat Ceph Storage集群之间的时间点和崩溃一致性复制。在修改实际的镜像之前，对RBD镜像的每次写入都首先记录到关联的日志。远程集群读取此日志，并将更新重播到映像的本地副本

### 基于快照的镜像

基于快照的镜像通过定时调度或手动创建的RBD镜像镜像快照，在两个Red Hat Ceph Storage集群之间复制崩溃一致性RBD镜像。远程集群决定两个镜像之间的任何数据或元数据更新快照并将增量复制到映像的本地副本。RBD fast-diff image特性可以快速确定更新的数据块，而不需要扫描整个RBD图像。在故障转移场景中使用之前，必须同步两个快照之间的完整增量。任何部分应用的增量集都将在故障转移时回滚。

# 实践：配置RBD Mirror

## 创建RBD源池

```bash
[root@ceph01 ~]# ceph osd pool create rbdtest
pool 'rbdtest' created
[root@ceph01 ~]# ceph osd pool application enable rbdtest rbd
enabled application 'rbd' on pool 'rbdtest'

```

## 创建备份集群目标RBD池

```bash
[root@cephbackup ~]# ceph osd pool create rbdtest
pool 'rbdtest' created
[root@cephbackup ~]# ceph osd pool application enable rbdtest rbd
enabled application 'rbd' on pool 'rbdtest'
```

## 创建源镜像

```bash
[root@ceph01 ~]# rbd create --size 1000M -p rbdtest lxhrbdname --image-feature=exclusive-lock,journaling
[root@ceph01 ~]# rbd info -p rbdtest lxhrbdname
rbd image 'lxhrbdname':
        size 1000 MiB in 250 objects
        order 22 (4 MiB objects)
        snapshot_count: 0
        id: 3f7e85fa0570
        block_name_prefix: rbd_data.3f7e85fa0570
        format: 2
        features: exclusive-lock, journaling
        op_features:
        flags:
        create_timestamp: Sun Sep 18 15:02:25 2022
        access_timestamp: Sun Sep 18 15:02:25 2022
        modify_timestamp: Sun Sep 18 15:02:25 2022
        journal: 3f7e85fa0570
        mirroring state: disabled

```

## 启用池模式mirror

```bash
[root@ceph01 ~]# rbd mirror pool enable rbdtest pool
[root@ceph01 ~]# rbd info -p rbdtest lxhrbdname
rbd image 'lxhrbdname':
        size 1000 MiB in 250 objects
        order 22 (4 MiB objects)
        snapshot_count: 0
        id: 3f7e85fa0570
        block_name_prefix: rbd_data.3f7e85fa0570
        format: 2
        features: exclusive-lock, journaling
        op_features:
        flags:
        create_timestamp: Sun Sep 18 15:02:25 2022
        access_timestamp: Sun Sep 18 15:02:25 2022
        modify_timestamp: Sun Sep 18 15:02:25 2022
        journal: 3f7e85fa0570
        mirroring state: enabled
        mirroring mode: journal
        mirroring global id: 165a0e83-8e8b-43cb-8d93-6b1812e3c58c
        mirroring primary: true

```

## 准备源同步token

```bash
[root@ceph01 ~]# rbd mirror pool peer bootstrap create --site-name site1 rbdtest > /ceph/site1.key
[root@ceph01 ~]# cat /ceph/site1.key
eyJmc2lkIjoiMjMyZDA4MmUtMzZmOS0xMWVkLTllOTEtMDA1MDU2OTRiOWExIiwiY2xpZW50X2lkIjoicmJkLW1pcnJvci1wZWVyIiwia2V5IjoiQVFBTHhDWmpERWlYQ2hBQUFnRHg3Nnd6Ty8xMnlTTkJ6dndPekE9PSIsIm1vbl9ob3N0IjoiW3YyOjE5Mi4xNjguMzEuMTg1OjMzMDAvMCx2MToxOTIuMTY4LjMxLjE4NTo2Nzg5LzBdIFt2MjoxOTIuMTY4LjMxLjEyOTozMzAwLzAsdjE6MTkyLjE2OC4zMS4xMjk6Njc4OS8wXSBbdjI6MTkyLjE2OC4zMS4yNTozMzAwLzAsdjE6MTkyLjE2OC4zMS4yNTo2Nzg5LzBdIn0=

```

```bash
[root@ceph01 ~]# scp -p /ceph/site1.key root@cephbackup:/root
root@cephbackup's password:
site1.key                                                                          100%  393   217.6KB/s   00:00
[root@ceph01 ~]#

```

目标集群部署rbd-mirror进程

```bash
[root@cephbackup ~]# ceph orch apply rbd-mirror --placement=cephbackup.xiaohui.cn
Scheduled rbd-mirror update...[root@cephbackup ~]# ceph orch ps | grep mirror
rbd-mirror.cephbackup.fpwjkw      cephbackup.xiaohui.cn               running (42s)    31s ago  41s    13.2M        -  17.2.3   0912465dcea5  7960db7df380


```

## 导入源集群token密钥

```bash
[root@cephbackup ~]# rbd mirror pool peer bootstrap import --site-name backupceph --direction rx-only rbdtest /root/site1.key
[root@cephbackup ~]#
[root@cephbackup ~]# rbd -p rbdtest ls
lxhrbdname
[root@cephbackup ~]# rbd -p rbdtest info lxhrbdname
rbd image 'lxhrbdname':
        size 1000 MiB in 250 objects
        order 22 (4 MiB objects)
        snapshot_count: 0
        id: 37a88b1c5510
        block_name_prefix: rbd_data.37a88b1c5510
        format: 2
        features: exclusive-lock, journaling
        op_features:
        flags:
        create_timestamp: Sun Sep 18 15:18:32 2022
        access_timestamp: Sun Sep 18 15:18:32 2022
        modify_timestamp: Sun Sep 18 15:18:32 2022
        journal: 37a88b1c5510
        mirroring state: enabled
        mirroring mode: journal
        mirroring global id: 165a0e83-8e8b-43cb-8d93-6b1812e3c58c
        mirroring primary: false

```

## 查询同步状态

```bash
[root@cephbackup ~]# rbd mirror pool info rbdtest
Mode: pool
Site Name: backupceph

Peer Sites:

UUID: 6b9fb207-4555-4637-8954-f637453081c9
Name: site1
Direction: rx-only
Client: client.rbd-mirror-peer

```

```bash
[root@ceph01 ~]# rbd -p rbdtest info lxhrbdname
rbd image 'lxhrbdname':
        size 1000 MiB in 250 objects
        order 22 (4 MiB objects)
        snapshot_count: 0
        id: 3f7e85fa0570
        block_name_prefix: rbd_data.3f7e85fa0570
        format: 2
        features: exclusive-lock, journaling
        op_features:
        flags:
        create_timestamp: Sun Sep 18 15:02:25 2022
        access_timestamp: Sun Sep 18 15:02:25 2022
        modify_timestamp: Sun Sep 18 15:02:25 2022
        journal: 3f7e85fa0570
        mirroring state: enabled
        mirroring mode: journal
        mirroring global id: 165a0e83-8e8b-43cb-8d93-6b1812e3c58c
        mirroring primary: true
[root@ceph01 ~]# rbd mirror pool info rbdtest
Mode: pool
Site Name: site1

Peer Sites:

UUID: de0c1b32-7b04-4885-b321-5a4d609e9477
Name: backupceph
Mirror UUID: 0a968055-277a-4a1b-96cf-c79d78e15ae4
Direction: tx-only

```

## 创建新镜像测试是否同步

```bash
[root@ceph01 ~]# rbd create --size 1000M -p rbdtest image --image-feature=exclusive-lock,journaling
[root@ceph01 ~]# rbd info image -p rbdtest
rbd image 'image':
        size 1000 MiB in 250 objects
        order 22 (4 MiB objects)
        snapshot_count: 0
        id: 671bbccd15ef
        block_name_prefix: rbd_data.671bbccd15ef
        format: 2
        features: exclusive-lock, journaling
        op_features:
        flags:
        create_timestamp: Sun Sep 18 15:24:44 2022
        access_timestamp: Sun Sep 18 15:24:44 2022
        modify_timestamp: Sun Sep 18 15:24:44 2022
        journal: 671bbccd15ef
        mirroring state: enabled
        mirroring mode: journal
        mirroring global id: 68ec360e-d029-44cd-9747-fbc5556fb883
        mirroring primary: true

```

```bash
[root@cephbackup ~]# rbd -p rbdtest ls
image
lxhrbdname
[root@cephbackup ~]# rbd -p rbdtest info image
rbd image 'image':
        size 1000 MiB in 250 objects
        order 22 (4 MiB objects)
        snapshot_count: 0
        id: 37a81190624a
        block_name_prefix: rbd_data.37a81190624a
        format: 2
        features: exclusive-lock, journaling
        op_features:
        flags:
        create_timestamp: Sun Sep 18 15:24:46 2022
        access_timestamp: Sun Sep 18 15:24:46 2022
        modify_timestamp: Sun Sep 18 15:24:46 2022
        journal: 37a81190624a
        mirroring state: enabled
        mirroring mode: journal
        mirroring global id: 68ec360e-d029-44cd-9747-fbc5556fb883
        mirroring primary: false

```

一些额外的mirror命令：

如果两个peer集群之间的状态不一致，rbd-mirror守护进程不会尝试mirror不一致的映像，使用rbd mirror image resync重新同步映像

```bash
rbd mirror image resync mypool/myimage
```

使用rbd mirror image enable或rbd mirror image disable在两个对端存储集群上的整个池image模式中启用或禁用mirroring模式

```bash
rbd mirror image enable mypool/myimage
rbd mirror image disable mypool/myimage
```

使用基于快照的镜像，通过禁用镜像和启用快照，将基于日志的镜像转换为基于快照的镜像

```bash
rbd mirror image disable mypool/myimage
rbd mirror image enable mypool/myimage snapshot
```

## 故障转移过程

如果主RBD镜像不可用，可以通过以下步骤启用对备RBD镜像的访问:

1. 停止对主RBD映像的访问。这意味着停止使用映像的所有应用程序和虚拟机

2. 使用rbd mirror image demote pool-name/image-name命令降级rbd主镜像

3. 使用rbd mirror image promote pooI-name/image-name命令提升rbd副镜像

4. 恢复对RBD映像的访问。重新启动应用程序和虚拟机

当发生非有序关闭后的故障转移时，必须从备份存储集群中的Ceph Monitor节点提升非主映像。使用- -force选项，因为降级无法传播到主存储集群

# 实践：配置对象存储

## 部署RGW服务

部署rgw服务，发现监听在80端口

```bash
[root@ceph01 ~]# ceph orch apply rgw svcname placement='ceph01.xiaohui.cn'
Scheduled rgw.svcname update...
[root@ceph01 ~]# ceph orch ps
NAME                                 HOST               PORTS        STATUS        REFRESHED  AGE  MEM USE  MEM LIM  VERSION  IMAGE ID      CONTAINER ID
alertmanager.ceph01                  ceph01.xiaohui.cn  *:9093,9094  running (5h)     6m ago   5h    23.4M        -           ba2b418f427c  f9159a065935
crash.ceph01                         ceph01.xiaohui.cn               running (5h)     6m ago   5h    6991k        -  17.2.3   0912465dcea5  3415f10f03e3
crash.ceph02                         ceph02.xiaohui.cn               running (5h)     7m ago   5h    7134k        -  17.2.3   0912465dcea5  432f7117eb80
crash.ceph03                         ceph03.xiaohui.cn               running (5h)     8m ago   5h    7134k        -  17.2.3   0912465dcea5  7728d86e79b0
grafana.ceph01                       ceph01.xiaohui.cn  *:3000       running (5h)     6m ago   5h    62.8M        -  8.3.5    dad864ee21e9  f82a985843a6
mds.ceph02.xiaohui.cn.ceph01.mvyrsv  ceph01.xiaohui.cn               running (5h)     6m ago   5h    23.3M        -  17.2.3   0912465dcea5  ab21636bfb9f
mds.ceph02.xiaohui.cn.ceph02.smtnbp  ceph02.xiaohui.cn               running (5h)     7m ago   5h    24.1M        -  17.2.3   0912465dcea5  ccef0507b7fc
mds.ceph03.xiaohui.cn.ceph01.zigsts  ceph01.xiaohui.cn               running (5h)     6m ago   5h    24.1M        -  17.2.3   0912465dcea5  192d0d03131e
mds.ceph03.xiaohui.cn.ceph02.jkdoxc  ceph02.xiaohui.cn               running (5h)     7m ago   5h    24.0M        -  17.2.3   0912465dcea5  6b3e12ec4365
mgr.ceph01.xiaohui.cn.fylsir         ceph01.xiaohui.cn  *:9283       running (5h)     6m ago   5h     503M        -  17.2.3   0912465dcea5  437ff72d07fe
mgr.ceph02.apxfvk                    ceph02.xiaohui.cn  *:8443,9283  running (5h)     7m ago   5h     436M        -  17.2.3   0912465dcea5  725e0666335c
mon.ceph01.xiaohui.cn                ceph01.xiaohui.cn               running (5h)     6m ago   5h     189M    2048M  17.2.3   0912465dcea5  d82f526e7f8b
mon.ceph02                           ceph02.xiaohui.cn               running (5h)     7m ago   5h     181M    2048M  17.2.3   0912465dcea5  0b70006ffc40
mon.ceph03                           ceph03.xiaohui.cn               running (5h)     8m ago   5h     176M    2048M  17.2.3   0912465dcea5  687c135f2143
node-exporter.ceph01                 ceph01.xiaohui.cn  *:9100       running (5h)     6m ago   5h    23.9M        -           1dbe0e931976  41a7edbcfe44
node-exporter.ceph02                 ceph02.xiaohui.cn  *:9100       running (5h)     7m ago   5h    21.9M        -           1dbe0e931976  be6bd6f08d6c
node-exporter.ceph03                 ceph03.xiaohui.cn  *:9100       running (5h)     8m ago   5h    21.8M        -           1dbe0e931976  b1c16a4022f2
osd.0                                ceph01.xiaohui.cn               running (5h)     6m ago   5h     104M    4096M  17.2.3   0912465dcea5  3dff6b23b5e0
osd.1                                ceph01.xiaohui.cn               running (5h)     6m ago   5h     100M    4096M  17.2.3   0912465dcea5  edb54c845cd7
osd.2                                ceph02.xiaohui.cn               running (5h)     7m ago   5h     100M    4096M  17.2.3   0912465dcea5  b096cbd3cae8
osd.3                                ceph02.xiaohui.cn               running (5h)     7m ago   5h    97.9M    4096M  17.2.3   0912465dcea5  a85afc49596c
osd.4                                ceph03.xiaohui.cn               running (5h)     8m ago   5h     105M    4096M  17.2.3   0912465dcea5  ce68ee165655
osd.5                                ceph03.xiaohui.cn               running (5h)     8m ago   5h     105M    4096M  17.2.3   0912465dcea5  6e0260a8d5ad
prometheus.ceph01                    ceph01.xiaohui.cn  *:9095       running (5h)     6m ago   5h     159M        -           514e6a882f6e  388825ab9d7b
rgw.svcname.ceph01.yjfygq            ceph01.xiaohui.cn  *:80         running (7m)     6m ago   7m    17.2M        -  17.2.3   0912465dcea5  731051efc883

[root@ceph01 ~]# curl localhost
<?xml version="1.0" encoding="UTF-8"?><ListAllMyBucketsResult xmlns="http://s3.amazonaws.com/doc/2006-03-01/"><Owner><ID>anonymous</ID><DisplayName></DisplayName></Owner><Buckets></Buckets></ListAllMyBucketsResult>[root@ceph01 ~]#


```

自动创建了所需的池

```bash
[root@ceph01 ~]# ceph df
--- RAW STORAGE ---
CLASS     SIZE    AVAIL     USED  RAW USED  %RAW USED
hdd    2.9 TiB  2.9 TiB  515 MiB   515 MiB       0.02
TOTAL  2.9 TiB  2.9 TiB  515 MiB   515 MiB       0.02

--- POOLS ---
POOL                 ID  PGS   STORED  OBJECTS     USED  %USED  MAX AVAIL
.mgr                  1    1  897 KiB        2  2.6 MiB      0    950 GiB
ops-image             2   32   16 MiB        8   47 MiB      0    950 GiB
ops-block             3   32   81 MiB       79  244 MiB      0    950 GiB
ops-object            4   32      0 B        0      0 B      0    950 GiB
ops-fsstore           5   32      0 B        0      0 B      0    950 GiB
rbdtest               6   32  1.1 KiB       21  156 KiB      0    950 GiB
.rgw.root             7   32  1.3 KiB        4   48 KiB      0    950 GiB
default.rgw.log       8   32  3.6 KiB      177  408 KiB      0    950 GiB
default.rgw.control   9   32      0 B        8      0 B      0    950 GiB
default.rgw.meta     10   32    382 B        2   24 KiB      0    950 GiB

```

## 创建用户

创建一个用户

```bash
[root@ceph01 ~]# radosgw-admin user create --uid="xiaohui" --display-name="Xiaohui Li" --access=full
{
    "user_id": "xiaohui",
    "display_name": "Xiaohui Li",
    "email": "",
    "suspended": 0,
    "max_buckets": 1000,
    "subusers": [],
    "keys": [
        {
            "user": "xiaohui",
            "access_key": "LSLG9ID55Y6ZHNGSVB4O",
            "secret_key": "EBBCE76qQINV7kBXi97aDBB5ljP89Tw8FRsmxXpH"
        }
    ],
    "swift_keys": [],
    "caps": [],
    "op_mask": "read, write, delete",
    "default_placement": "",
    "default_storage_class": "",
    "placement_tags": [],
    "bucket_quota": {
        "enabled": false,
        "check_on_raw": false,
        "max_size": -1,
        "max_size_kb": 0,
        "max_objects": -1
    },
    "user_quota": {
        "enabled": false,
        "check_on_raw": false,
        "max_size": -1,
        "max_size_kb": 0,
        "max_objects": -1
    },
    "temp_url_keys": [],
    "type": "rgw",
    "mfa_ids": []
}


```

## 安装配置s3客户端

你也可以使用其他任何第三方的对象存储客户端，这里尝试使用s3

```bash
[root@ceph01 ~]# yum install s3cmd

```

配置S3存储访问凭据

```bash
[root@ceph01 ~]# s3cmd --configure

Enter new values or accept defaults in brackets with Enter.
Refer to user manual for detailed description of all options.

Access key and Secret key are your identifiers for Amazon S3. Leave them empty for using the env variables.
Access Key: LSLG9ID55Y6ZHNGSVB4O
Secret Key: EBBCE76qQINV7kBXi97aDBB5ljP89Tw8FRsmxXpH
Default Region [US]:

Use "s3.amazonaws.com" for S3 Endpoint and not modify it to the target Amazon S3.
S3 Endpoint [s3.amazonaws.com]: ceph01.xiaohui.cn

Use "%(bucket)s.s3.amazonaws.com" to the target Amazon S3. "%(bucket)s" and "%(location)s" vars can be used
if the target S3 system supports dns based buckets.
DNS-style bucket+hostname:port template for accessing a bucket [%(bucket)s.s3.amazonaws.com]:

Encryption password is used to protect your files from reading
by unauthorized persons while in transfer to S3
Encryption password:
Path to GPG program [/usr/bin/gpg]:

When using secure HTTPS protocol all communication with Amazon S3
servers is protected from 3rd party eavesdropping. This method is
slower than plain HTTP, and can only be proxied with Python 2.7 or newer
Use HTTPS protocol [Yes]: no

On some networks all internet access must go through a HTTP proxy.
Try setting it here if you can't connect to S3 directly
HTTP Proxy server name:

New settings:
  Access Key: LSLG9ID55Y6ZHNGSVB4O
  Secret Key: EBBCE76qQINV7kBXi97aDBB5ljP89Tw8FRsmxXpH
  Default Region: US
  S3 Endpoint: ceph01.xiaohui.cn
  DNS-style bucket+hostname:port template for accessing a bucket: %(bucket)s.s3.amazonaws.com
  Encryption password:
  Path to GPG program: /usr/bin/gpg
  Use HTTPS protocol: False
  HTTP Proxy server name:
  HTTP Proxy server port: 0

Test access with supplied credentials? [Y/n] y
Please wait, attempting to list all buckets...
Success. Your access key and secret key worked fine :-)

Now verifying that encryption works...
Not configured. Never mind.

Save settings? [y/N] y
Configuration saved to '/root/.s3cfg'

```

默认的格式是连接亚马逊，所以我们修改一下/root/.s3cfg

```bash
[root@ceph01 ~]# vim .s3cfg
host_bucket = 192.168.31.185/%(bucket)

```



新建一个存储桶

```bash
[root@ceph01 ~]# s3cmd mb s3://lxhname
Bucket 's3://lxhname/' created
[root@ceph01 ~]# s3cmd ls
2022-09-18 08:04  s3://lxhname


```

# 实践：提供iscsi

osd和MONs不需要任何与iscsi相关的服务器设置。主要是为了限制客户端SCSI超时时间，减少集群用于检测故障OSD的延迟超时设置

```bash
[root@ceph01 ~]# ceph config set osd osd_heartbeat_interval 5
[root@ceph01 ~]# ceph config set osd osd_heartbeat_grace 20
[root@ceph01 ~]# ceph config set osd osd_client_watch_timeout 15

```

## 创建iscsi池

```bash
[root@ceph01 ~]# ceph osd pool create iscsi
pool 'iscsi' created[ceph: root@ceph01 /]# ceph osd pool application enable iscsi rbd
enabled application 'rbd' on pool 'iscsi'


```

## 部署iscsi进程

```bash
[ceph: root@ceph01 /]# ceph orch apply iscsi iscsi admin admin 192.168.31.185 --placement='ceph01.xiaohui.cn'
Scheduled iscsi.iscsi update... 
[ceph: root@ceph01 /]# ceph orch ps | grep iscsi
iscsi.iscsi.ceph01.uwwvti            ceph01.xiaohui.cn               running (78s)    69s ago  78s    83.5M        -  3.5      0912465dcea5  6219b77d44e1



```

## 配置iscsi后端

从上面的步骤中，可以看到iscsi容器的id，用podman exec进入配置

```bash
[root@ceph01 ~]# podman exec -it 6219b77d44e1 gwcli
/> ls /
o- / ......................................................................................................................... [...]
  o- cluster ......................................................................................................... [Clusters: 1]
  | o- ceph ............................................................................................................ [HEALTH_OK]
  |   o- pools ......................................................................................................... [Pools: 12]
  |   | o- .mgr ................................................................. [(x3), Commit: 0.00Y/995916608K (0%), Used: 2712K]
  |   | o- .rgw.root .............................................................. [(x3), Commit: 0.00Y/995916608K (0%), Used: 48K]
  |   | o- default.rgw.buckets.index ............................................ [(x3), Commit: 0.00Y/995916608K (0%), Used: 0.00Y]
  |   | o- default.rgw.control .................................................. [(x3), Commit: 0.00Y/995916608K (0%), Used: 0.00Y]
  |   | o- default.rgw.log ....................................................... [(x3), Commit: 0.00Y/995916608K (0%), Used: 408K]
  |   | o- default.rgw.meta ....................................................... [(x3), Commit: 0.00Y/995916608K (0%), Used: 72K]
  |   | o- iscsi .................................................................. [(x3), Commit: 0.00Y/995916608K (0%), Used: 24K]
  |   | o- ops-block .......................................................... [(x3), Commit: 0.00Y/995916608K (0%), Used: 249660K]
  |   | o- ops-fsstore .......................................................... [(x3), Commit: 0.00Y/995916608K (0%), Used: 0.00Y]
  |   | o- ops-image ........................................................... [(x3), Commit: 0.00Y/995916608K (0%), Used: 47808K]
  |   | o- ops-object ........................................................... [(x3), Commit: 0.00Y/995916608K (0%), Used: 0.00Y]
  |   | o- rbdtest ............................................................... [(x3), Commit: 0.00Y/995916608K (0%), Used: 156K]
  |   o- topology ................................................................................................ [OSDs: 6,MONs: 3]
  o- disks ....................................................................................................... [0.00Y, Disks: 0]
  o- iscsi-targets ............................................................................... [DiscoveryAuth: None, Targets: 0]
/>

```

### 创建iscsi target

```bash
/> cd /iscsi-targets
/iscsi-targets> create iqn.2022-09.cn.xiaohui:test
ok

```

### 创建iscsi gateway

```bash
/iscsi-targets> cd /iscsi-targets/iqn.2022-09.cn.xiaohui:test/gateways
/iscsi-target...test/gateways> create ceph01.xiaohui.cn 192.168.31.185
Adding gateway, sync'ing 0 disk(s) and 0 client(s)
ok

```

### 添加客户端

```bash
/iscsi-target...ui:test/hosts> create iqn.1994-05.com.redhat:1ba034dc3645
ok

```

上面的iqn这部分是客户端的iqn号，要和客户端完全一致，如果cat时文件不存在，就需要安装iscsi客户端

```bnf
[root@cephbackup ~]# yum install iscsi-initiator-utils -y
[root@cephbackup ~]# cat /etc/iscsi/initiatorname.iscsi
InitiatorName=iqn.1994-05.com.redhat:1ba034dc3645

```

### 在Ceph中创建并存储lun

```bash
/iscsi-target...:1ba034dc3645> cd /disks
/disks> create pool=iscsi image=testiscsi size=100G
ok


```

### 添加存储Lun到iscsi

```bash
/disks> cd /iscsi-targets/iqn.2022-09.cn.xiaohui:test/hosts/iqn.1994-05.com.redhat:1ba034dc3645
/iscsi-target...:1ba034dc3645> disk add iscsi/testiscsi
ok

```

### 创建客户端使用的账号密码

```bash
/iscsi-target...:1ba034dc3645> auth username=lixiaohui password=lixiaohui0608
ok

```

### 预览iscsi后端配置

```bash
/iscsi-target...:1ba034dc3645> cd /
/> ls /
o- / ......................................................................................................................... [...]
  o- cluster ......................................................................................................... [Clusters: 1]
  | o- ceph ............................................................................................................ [HEALTH_OK]
  |   o- pools ......................................................................................................... [Pools: 12]
  |   | o- .mgr ................................................................. [(x3), Commit: 0.00Y/995912000K (0%), Used: 2712K]
  |   | o- .rgw.root .............................................................. [(x3), Commit: 0.00Y/995912000K (0%), Used: 48K]
  |   | o- default.rgw.buckets.index ............................................ [(x3), Commit: 0.00Y/995912000K (0%), Used: 0.00Y]
  |   | o- default.rgw.control .................................................. [(x3), Commit: 0.00Y/995912000K (0%), Used: 0.00Y]
  |   | o- default.rgw.log ....................................................... [(x3), Commit: 0.00Y/995912000K (0%), Used: 408K]
  |   | o- default.rgw.meta ....................................................... [(x3), Commit: 0.00Y/995912000K (0%), Used: 72K]
  |   | o- iscsi .................................................................. [(x3), Commit: 100G/995912000K (10%), Used: 24K]
  |   | o- ops-block .......................................................... [(x3), Commit: 0.00Y/995912000K (0%), Used: 299580K]
  |   | o- ops-fsstore .......................................................... [(x3), Commit: 0.00Y/995912000K (0%), Used: 0.00Y]
  |   | o- ops-image ........................................................... [(x3), Commit: 0.00Y/995912000K (0%), Used: 47808K]
  |   | o- ops-object ........................................................... [(x3), Commit: 0.00Y/995912000K (0%), Used: 0.00Y]
  |   | o- rbdtest ............................................................... [(x3), Commit: 0.00Y/995912000K (0%), Used: 156K]
  |   o- topology ................................................................................................ [OSDs: 6,MONs: 3]
  o- disks ........................................................................................................ [100G, Disks: 1]
  | o- iscsi ........................................................................................................ [iscsi (100G)]
  |   o- testiscsi ................................................................................ [iscsi/testiscsi (Online, 100G)]
  o- iscsi-targets ............................................................................... [DiscoveryAuth: None, Targets: 1]
    o- iqn.2022-09.cn.xiaohui:test ....................................................................... [Auth: None, Gateways: 1]
      o- disks .......................................................................................................... [Disks: 1]
      | o- iscsi/testiscsi ...................................................................... [Owner: ceph01.xiaohui.cn, Lun: 0]
      o- gateways ............................................................................................ [Up: 1/1, Portals: 1]
      | o- ceph01.xiaohui.cn ................................................................................. [192.168.31.185 (UP)]
      o- host-groups .................................................................................................. [Groups : 0]
      o- hosts ....................................................................................... [Auth: ACL_ENABLED, Hosts: 1]
        o- iqn.1994-05.com.redhat:1ba034dc3645 ........................................................ [Auth: CHAP, Disks: 1(100G)]
          o- lun 0 ............................................................... [iscsi/testiscsi(100G), Owner: ceph01.xiaohui.cn]

```

确认无误后exit退出

```bash
/> exit
[root@ceph01 ~]#

```

### 开通防火墙

iscsi默认用的3260端口

```bash
[root@ceph01 ~]# firewall-cmd --add-port=3260/tcp
success
[root@ceph01 ~]# firewall-cmd --add-port=3260/tcp --permanent
success

```

### 客户端挂载测试

#### 安装必备软件包

```bash
[root@cephbackup ~]# yum install iscsi-initiator-utils device-mapper-multipath -y

```

#### 创建multipathd默认配置文件

位于/etc/multipath.conf

```bash
[root@cephbackup ~]# mpathconf --enable --with_multipathd y
```

添加以下内容

```bash
cat >> /etc/multipath.conf <<EOF
devices {
        device {
                vendor                 "LIO-ORG"
                product                "TCMU device"
                hardware_handler       "1 alua"
                path_grouping_policy   "failover"
                path_selector          "queue-length 0"
                failback               60
                path_checker           tur
                prio                   alua
                prio_args              exclusive_pref_bit
                fast_io_fail_tmo       25
                no_path_retry          queue
        }
}
EOF

```

#### 重启服务

```bash
[root@cephbackup ~]# systemctl enable multipathd --now
[root@cephbackup ~]# systemctl restart multipathd

```

#### 配置iscsi认证

在文件中找到这几个参数改一下，或者在后面追加

```bash
[root@cephbackup ~]# tail -n3 /etc/iscsi/iscsid.conf
node.session.auth.authmethod = CHAP
node.session.auth.username = lixiaohui
node.session.auth.password = lixiaohui0608


```

#### 执行iscsi发现

```bash
[root@cephbackup ~]# iscsiadm -m discovery -t st -p 192.168.31.185
192.168.31.185:3260,1 iqn.2022-09.cn.xiaohui:test

```

#### 登陆iscsi

```bash
[root@cephbackup ~]# iscsiadm -m node -T iqn.2022-09.cn.xiaohui:test -l
Logging in to [iface: default, target: iqn.2022-09.cn.xiaohui:test, portal: 192.168.31.185,3260]
Login to [iface: default, target: iqn.2022-09.cn.xiaohui:test, portal: 192.168.31.185,3260] successful.

```

#### 查询设备列表

```bash
[root@cephbackup ~]# lsblk
NAME                                                                                                  MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
sdd                                                                                                     8:48   0   100G  0 disk


```

#### 格式化挂载使用

分区

```bash
[root@cephbackup ~]# fdisk /dev/sdd

Welcome to fdisk (util-linux 2.32.1).
Changes will remain in memory only, until you decide to write them.
Be careful before using the write command.

Device does not contain a recognized partition table.
Created a new DOS disklabel with disk identifier 0xdee0adfd.

Command (m for help): n
Partition type
   p   primary (0 primary, 0 extended, 4 free)
   e   extended (container for logical partitions)
Select (default p):

Using default response p.
Partition number (1-4, default 1):
First sector (2048-209715199, default 2048):
Last sector, +sectors or +size{K,M,G,T,P} (2048-209715199, default 209715199):

Created a new partition 1 of type 'Linux' and of size 100 GiB.

Command (m for help): w
The partition table has been altered.
Calling ioctl() to re-read partition table.
Syncing disks.

[root@cephbackup ~]# lsblk | grep sdd
sdd                                                                                                     8:48   0   100G  0 disk
└─sdd1                                                                                                  8:49   0   100G  0 part

```

格式化 

```bash
[root@cephbackup ~]# mkfs.xfs /dev/sdd1
meta-data=/dev/sdd1              isize=512    agcount=4, agsize=6553536 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=1, sparse=1, rmapbt=0
         =                       reflink=1
data     =                       bsize=4096   blocks=26214144, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0, ftype=1
log      =internal log           bsize=4096   blocks=12799, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
Discarding blocks...Done.

```

挂载使用

```bash
[root@cephbackup ~]# mount /dev/sdd1 /mnt
[root@cephbackup ~]# dd if=/dev/zero of=/mnt/out.file bs=1M count=100
100+0 records in
100+0 records out
104857600 bytes (105 MB, 100 MiB) copied, 0.143924 s, 729 MB/s
[root@cephbackup ~]# df -h | grep mnt
/dev/sdd1            100G  846M  100G   1% /mnt

```
