| 编号  | 系统         | 角色                    | IP                      | 主机名              | Ceph版本 |
| --- | ---------- | --------------------- | ----------------------- | ---------------- | ------ |
| 1   | Centos 8.5 | bootstrap，mon，mgr，osd | 192.168.30.130，10.0.0.1 | host1.xiaohui.cn | Quincy |
| 2   | Centos 8.5 | mon，mgr，osd，rgw，mds   | 192.168.30.131，10.0.0.2 | host2.xiaohui.cn | Quincy |
| 3   | Centos 8.5 | mon，mgr，rgw，mds       | 192.168.30.132，10.0.0.3 | host3.xiaohui.cn | Quincy |

作者：李晓辉

联系方式：

1. 微信：Lxh_Chat

2. QQ：939958092

3. 邮箱：[xiaohui_li@foxmail.com](mailto:xiaohui_li@foxmail.com)

# 管理集群配置设置

## Ceph集群配置介绍

Ceph配置的设置名称通常是由小写字母与下划线连接，并且全局唯一，所有的Ceph存储集群配置都包含这些必需的定义:

1. 集群网络配置

2. 集群监视器(MON)配置和bootstrap选项

3. 集群身份验证配置

4. 守护进程的配置选项

每个Ceph守护进程都从以下来源访问它的配置：

1. 编译后的默认值

2. 集中式配置数据库

3. 保存在本地主机上的配置文件

4. 环境变量

5. 命令行参数

6. 运行时覆盖

监视器(MON)节点管理集中的配置数据库，在启动时，Ceph守护进程通过命令行选项解析环境变量和本地集群配置文件提供的配置选项，守护进程然后联系MON集群以检索存储在集中配置数据库中的配置设置，在新版本的Ceph里已经不再强调/etc/ceph/ceph.conf，集中配置数据库成为首选方式

## 修改集群配置文件

每个Ceph存储集群配置文件的默认位置为/etc/ceph/ceph.conf，配置文件使用一种INI文件格式，其中包括对Ceph守护进程和client的配置，每个section都有一个名称，它是用[name]头定义的键值对

```ini
[name] 
parameter1 = value1
parameter2 = value2 
```

使用井号#或分号;来禁用设置或添加注释，设置bootstrap集群时使用集群配置文件以及自定义设置，使用cephadm boostrap带有--config选项的命令来传递配置文件

```bash
[root@host1 ~]# cephadm bootstrap --config ceph-config.yaml
```

### 实例设置

将应用于特定守护进程实例的设置分组在各自的Section中，名称为[daemon-type.instance- id]，对于OSD进程，实例ID总是为数字，例如[osd.0]，对于客户端，实例ID为活动用户名，例如[client.operator3]

```bash
[mon]
# Settings for all mon daemons
[mon.host1]
# Settings that apply to the specific MON daemon running on host1 
```

### 元变量

元变量是ceph定义的变量。使用它们可以简化配置

`$cluster`：集群名称，默认集群名称为ceph

`$type`：守护进程类型，例如monitor使用mon，OSDs使用osd、MDSes使用mds, MGRs使用mgr，client应用程序使用client

`$id`：守护进程实例ID，在host1上的monitor变量的值为的host1，osd 1的id是osd.1，client应用程序是用户名

`$name`  守护进程名称和实例ID，这个变量是`$type.$id`的快捷方式

`$host`：运行守护进程的主机名 

## 使用集中式配置数据库

MON集群在MON节点上管理和存储集中配置数据库，可以临时更改设置(守护进程重新启动后丢失)，也可以配置设置永久保存并存储在数据库中，可以在集群运行时更改大多数配置设置

使用ceph config命令查询数据库，查看配置信息：

`ceph config ls` 列出所有可能的配置设置

`ceph config help setting` 查询特定的配置设置的帮助

`ceph config dump` 显示集群配置数据库设置

`ceph config show $type.$id`显示特定守护进程的数据库设置。使用show-with-defaults包含默认设置

`ceph config get $type.$id` 获得特定的配置设置

`ceph config set $type.$id` 用于设置特定的配置设置

使用assimilate-conf子命令将文件中的配置应用到正在运行的集群，这个过程识别配置文件中的更改设置并将其应用到集中式数据库中

```bash
[ceph: root@host1/]# ceph config assimilate-conf -i ceph.conf 
```

## 在运行时覆盖配置设置

可以在守护进程运行时临时更改配置设置，守护进程重新启动时恢复到原来的设置：

`ceph tell $type.$id config` 命令临时覆盖配置设置

`ceph tell $type.$id config get `获取守护进程的特定运行时设置

`ceph tell $type.$id config set `设置守护进程的特定运行时设置

`ceph tell $type.$id config `命令还可以接受通配符来获取或设置同一类型的所有守护进程的值。例如cceph tell `osd.*` config get debug_ms 显示集群中所有OSD守护进程的该设置的值

`ceph daemon $type.$id config `临时覆盖配置设置，仅在**需要设置**的集群节点上运行此命令，ceph daemon不需要通过MONs进行连接，即使在MONs不运行的情况下，ceph daemon命令仍然可以运行，这对于故障排除很有用

`ceph daemon $type.$id config get `获取守护进程的特定运行时设置

`ceph daemon $type.$id config set` 设置守护进程的特定运行时设置，当守护进程重新启动时，临时设置会恢复到原来的值

## 查看现有配置

查看现有集中式数据库配置

```bash
[ceph: root@host1 /]# ceph orch ps
NAME                         HOST              PORTS        STATUS        REFRESHED  AGE  MEM USE  MEM LIM  VERSION  IMAGE ID      CONTAINER ID  
alertmanager.host1           host1.xiaohui.cn  *:9093,9094  running (2m)   108s ago  28m    41.9M        -           ba2b418f427c  357ae4912a26  
crash.host1                  host1.xiaohui.cn               running (2m)   108s ago  28m    20.0M        -  17.2.3   0912465dcea5  250699174ad6  
grafana.host1                host1.xiaohui.cn  *:3000       running (2m)   108s ago  27m     147M        -  8.3.5    dad864ee21e9  d3c7689e00c1  
mgr.host1.xiaohui.cn.jlcojp  host1.xiaohui.cn  *:9283       running (2m)   108s ago  29m     557M        -  17.2.3   0912465dcea5  f49230bce18a  
mon.host1.xiaohui.cn         host1.xiaohui.cn               running (2m)   108s ago  29m    53.0M    2048M  17.2.3   0912465dcea5  6270162e0395  
node-exporter.host1          host1.xiaohui.cn  *:9100       running (2m)   108s ago  27m    22.2M        -           1dbe0e931976  4835ed0d63e5  
[ceph: root@host1 /]# 
```

查看现有osd.1进程的配置

```basg
[ceph: root@host1 /]# ceph config show osd.1
NAME                                             VALUE                                                                                                                                                        SOURCE    OVERRIDES  IGNORES
container_image                                  quay.io/ceph/ceph@sha256:43f6e905f3e34abe4adbc9042b9d6f6b625dee8fa8d93c2bae53fa9b61c3df1a                                                                    mon                         
daemonize                                        false                                                                                                                                                        override                    
keyring                                          $osd_data/keyring                                                                                                                                            default                     
leveldb_log                                                                                                                                                                                                   default                     
log_to_file                                      false                                                                                                                                                        default                     
log_to_journald                                  true                                                                                                                                                         default                     
log_to_stderr                                    false                                                                                                                                                        default                     
mon_host                                         [v2:192.168.30.130:3300/0,v1:192.168.30.130:6789/0] [v2:192.168.30.131:3300/0,v1:192.168.30.131:6789/0] [v2:192.168.30.132:3300/0,v1:192.168.30.132:6789/0]  file                        
no_config_file                                   false                                                                                                                                                        override                    
osd_delete_sleep                                 0.000000                                                                                                                                                     override                    
osd_delete_sleep_hdd                             0.000000                                                                                                                                                     override                    
osd_delete_sleep_hybrid                          0.000000                                                                                                                                                     override                    
osd_delete_sleep_ssd                             0.000000                                                                                                                                                     override                    
osd_max_backfills                                1000                                                                                                                                                         override                    
osd_mclock_max_capacity_iops_ssd                 23784.332992                                                                                                                                                 mon                         
osd_mclock_scheduler_background_best_effort_lim  999999                                                                                                                                                       override                    
osd_mclock_scheduler_background_best_effort_res  743                                                                                                                                                          override                    
osd_mclock_scheduler_background_best_effort_wgt  2                                                                                                                                                            override                    
osd_mclock_scheduler_background_recovery_lim     2973                                                                                                                                                         override                    
osd_mclock_scheduler_background_recovery_res     743                                                                                                                                                          override                    
osd_mclock_scheduler_background_recovery_wgt     1                                                                                                                                                            override                    
osd_mclock_scheduler_client_lim                  999999                                                                                                                                                       override                    
osd_mclock_scheduler_client_res                  1487                                                                                                                                                         override                    
osd_mclock_scheduler_client_wgt                  2                                                                                                                                                            override                    
osd_memory_target_autotune                       true                                                                                                                                                         mon                         
osd_recovery_max_active                          1000                                                                                                                                                         override                    
osd_recovery_max_active_hdd                      1000                                                                                                                                                         override                    
osd_recovery_max_active_ssd                      1000                                                                                                                                                         override                    
osd_recovery_sleep                               0.000000                                                                                                                                                     override                    
osd_recovery_sleep_hdd                           0.000000                                                                                                                                                     override                    
osd_recovery_sleep_hybrid                        0.000000                                                                                                                                                     override                    
osd_recovery_sleep_ssd                           0.000000                                                                                                                                                     override                    
osd_scrub_sleep                                  0.000000                                                                                                                                                     override                    
osd_snap_trim_sleep                              0.000000                                                                                                                                                     override                    
osd_snap_trim_sleep_hdd                          0.000000                                                                                                                                                     override                    
osd_snap_trim_sleep_hybrid                       0.000000                                                                                                                                                     override                    
osd_snap_trim_sleep_ssd                          0.000000                                                                                                                                                     override                    
rbd_default_features                             61                                                                                                                                                           default                     
rbd_qos_exclude_ops                              0                                                                                                                                                            default                     
setgroup                                         ceph                                                                                                                                                         cmdline                     
setuser                                          ceph                                                                                                                                                         cmdline                     
[ceph: root@host1 /]# 
```

这些值有点多，而且还有一些目前没有生效的未列出，如果想要查看目前的日志级别的值可以用下方的方法

```bash
[ceph: root@host1 /]# ceph config show osd.1 debug_ms
0/0
[ceph: root@host1 /]# 
```

看到目前的日志级别为0，Ceph 的日志级别从 1 到 20 ， 1 是简洁， 20 是详细，从上图来看0/0，第一个0是日志级别，第二个0是内存级别其实也是，日志，只是把其暂存到内存中

## 永久修改数据库值

现在我们需要把两者同时设置为1

```bash
[ceph: root@host1 /]# ceph config set osd.1 debug_ms 1
[ceph: root@host1 /]# ceph config show osd.1 debug_ms
1/1
[ceph: root@host1 /]# ceph config get osd.1 debug_ms
1/1
[ceph: root@host1 /]# 
```

重启一下osd.1的守护进程再次确认，我们发现这个值是被永久修改了

```bash
[ceph: root@host1 /]# ceph orch daemon restart osd.1
Scheduled to restart osd.1 on host 'host1.xiaohui.cn'
[ceph: root@host1 /]# ceph config get osd.1 debug_ms
1/1
[ceph: root@host1 /]# ceph config show osd.1 debug_ms
1/1
[ceph: root@host1 /]# 
```

## 临时修改数据库值

用ceph tell可以临时修改值，daemon重启后会恢复

```bash
[ceph: root@host1 /]# ceph tell osd.1 config set debug_ms 10
{
    "success": ""
}
[ceph: root@host1 /]# ceph tell osd.1 config get debug_ms
{
    "debug_ms": "10/10"
}
[ceph: root@host1 /]# 
```

重启daemon再查看，发现其回到了我们永久修改的1

```bash
[ceph: root@host1 /]# ceph orch daemon restart osd.1
Scheduled to restart osd.1 on host 'host1.xiaohui.cn'
[ceph: root@host1 /]# ceph tell osd.1 config get debug_ms
{
    "debug_ms": "1/1"
}
[ceph: root@host1 /]# ceph config get osd.1 debug_ms
1/1
[ceph: root@host1 /]# 
```

## 临时修改运行时参数

不建议这种方式，但集群故障无法运行时，可以考虑使用这种方式，或者你明白自己在做什么时使用，这种方式不要求集群正常，而且需要你登录到具体daemon所在的节点执行，在daemon重启后，所做的事将会恢复原来的样子

```bash
[ceph: root@host3 /]# ceph daemon osd.0 config set debug_ms 4
{
    "success": "osd_delete_sleep = '0.000000' osd_delete_sleep_hdd = '0.000000' osd_delete_sleep_hybrid = '0.000000' osd_delete_sleep_ssd = '0.000000' osd_max_backfills = '1000' osd_recovery_max_active = '1000' osd_recovery_max_active_hdd = '1000' osd_recovery_max_active_ssd = '1000' osd_recovery_sleep = '0.000000' osd_recovery_sleep_hdd = '0.000000' osd_recovery_sleep_hybrid = '0.000000' osd_recovery_sleep_ssd = '0.000000' osd_scrub_sleep = '0.000000' osd_snap_trim_sleep = '0.000000' osd_snap_trim_sleep_hdd = '0.000000' osd_snap_trim_sleep_hybrid = '0.000000' osd_snap_trim_sleep_ssd = '0.000000' "
}
[ceph: root@host3 /]# ceph daemon osd.0 config get debug_ms  
{
    "debug_ms": "4/4"
}
[ceph: root@host3 /]# 
```

# 配置Monitor（Mons）

Ceph监视器(MONs)存储和维护客户端用来查找MON和OSD节点的集群映射，Ceph客户端在向osd读写任何数据之前，必须连接到一个MON来检索集群映射，MONs通过使用一种变异的Paxos算法组成一个quorum，选出一个leader，在一组分布式计算机之间达成共识，MONs可以是下角色之一：

1. Leader：第一个获得最新版本的cluster map的MON

2. Provider：一个MON，它具有集群映射的最新版本，但不是leader

3. Requester：一个MON，它没有最新版本的集群映射，并且在重新加入仲裁之前必须与权威mon同步

要建立仲裁，集群中的大多数MONs必须处于运行状态，例如，如果部署了5个MONs，那么必须运行3个MONs来建立仲裁，在生产Ceph集群中部署至少三个MON节点，以确保高可用性，支持对运行中的集群添加或移除mon，集群配置文件定义了用于集群操作的MON主机IP地址和端口，mon_host设置可以包含IP地址或DNS名称，不建议在集群部署后修改MON节点IP地址，以下为/etc/ceph/ceph.conf样例

```ini
[global]
        fsid = 6dbbdf24-33f1-11ed-84ff-000c29759605
        mon_host = [v2:192.168.30.130:3300/0,v1:192.168.30.130:6789/0] [v2:192.168.30.131:3300/0,v1:192.168.30.131:6789/0] [v2:192.168.30.132:3300/0,v1:192.168.30.132:6789/0]
```

## 查看Mon Map

Ceph集群map包括MON map、OSD map、PG map、MDS map和CRUSH map

MON映射包含集群fsid(文件系统ID)，以及与每个MON节点通信的名称、IP地址和网口。fsid是一个惟一的、自动生成的标识符(UUID)，用于标识Ceph集群

MON映射还保存映射版本信息，例如最后一次更改的序号(epoch )和时间，MON节点通过同步更改和对当前版本达成一致来维护映射

可以看到一共有3个投票的机器

```bash
[ceph: root@host1 /]# ceph mon dump
epoch 3
fsid 6dbbdf24-33f1-11ed-84ff-000c29759605
last_changed 2022-09-14T06:25:51.618084+0000
created 2022-09-14T05:54:01.447710+0000
min_mon_release 17 (quincy)
election_strategy: 1
0: [v2:192.168.30.130:3300/0,v1:192.168.30.130:6789/0] mon.host1.xiaohui.cn
1: [v2:192.168.30.131:3300/0,v1:192.168.30.131:6789/0] mon.host2
2: [v2:192.168.30.132:3300/0,v1:192.168.30.132:6789/0] mon.host3
dumped monmap epoch 3
```

## 确认仲裁状态

可以看到目前host1.xiaohui.cn在leader

```bash
[ceph: root@host1 /]# ceph mon stat
e3: 3 mons at {host1.xiaohui.cn=[v2:192.168.30.130:3300/0,v1:192.168.30.130:6789/0],host2=[v2:192.168.30.131:3300/0,v1:192.168.30.131:6789/0],host3=[v2:192.168.30.132:3300/0,v1:192.168.30.132:6789/0]}, election epoch 24, leader 0 host1.xiaohui.cn, quorum 0,1,2 host1.xiaohui.cn,host2,host3
```

```json
[ceph: root@host1 /]# ceph quorum_status -f json-pretty

{
    "election_epoch": 24,
    "quorum": [
        0,
        1,
        2
    ],
    "quorum_names": [
        "host1.xiaohui.cn",
        "host2",
        "host3"
    ],
    "quorum_leader_name": "host1.xiaohui.cn",
    "quorum_age": 7098,
    "features": {
        "quorum_con": "4540138320759226367",
        "quorum_mon": [
            "kraken",
            "luminous",
            "mimic",
            "osdmap-prune",
            "nautilus",
            "octopus",
            "pacific",
            "elector-pinging",
            "quincy"
        ]
    },
    "monmap": {
        "epoch": 3,
        "fsid": "6dbbdf24-33f1-11ed-84ff-000c29759605",
        "modified": "2022-09-14T06:25:51.618084Z",
        "created": "2022-09-14T05:54:01.447710Z",
        "min_mon_release": 17,
        "min_mon_release_name": "quincy",
        "election_strategy": 1,
        "disallowed_leaders: ": "",
        "stretch_mode": false,
        "tiebreaker_mon": "",
        "features": {
            "persistent": [
                "kraken",
                "luminous",
                "mimic",
                "osdmap-prune",
                "nautilus",
                "octopus",
                "pacific",
                "elector-pinging",
                "quincy"
            ],
            "optional": []
        },
        "mons": [
            {
                "rank": 0,
                "name": "host1.xiaohui.cn",
                "public_addrs": {
                    "addrvec": [
                        {
                            "type": "v2",
                            "addr": "192.168.30.130:3300",
                            "nonce": 0
                        },
                        {
                            "type": "v1",
                            "addr": "192.168.30.130:6789",
                            "nonce": 0
                        }
                    ]
                },
                "addr": "192.168.30.130:6789/0",
                "public_addr": "192.168.30.130:6789/0",
                "priority": 0,
                "weight": 0,
                "crush_location": "{}"
            },
            {
                "rank": 1,
                "name": "host2",
                "public_addrs": {
                    "addrvec": [
                        {
                            "type": "v2",
                            "addr": "192.168.30.131:3300",
                            "nonce": 0
                        },
                        {
                            "type": "v1",
                            "addr": "192.168.30.131:6789",
                            "nonce": 0
                        }
                    ]
                },
                "addr": "192.168.30.131:6789/0",
                "public_addr": "192.168.30.131:6789/0",
                "priority": 0,
                "weight": 0,
                "crush_location": "{}"
            },
            {
                "rank": 2,
                "name": "host3",
                "public_addrs": {
                    "addrvec": [
                        {
                            "type": "v2",
                            "addr": "192.168.30.132:3300",
                            "nonce": 0
                        },
                        {
                            "type": "v1",
                            "addr": "192.168.30.132:6789",
                            "nonce": 0
                        }
                    ]
                },
                "addr": "192.168.30.132:6789/0",
                "public_addr": "192.168.30.132:6789/0",
                "priority": 0,
                "weight": 0,
                "crush_location": "{}"
            }
        ]
    }
} 
```

## 管理集中式配置数据库

MON节点存储和维护集中式配置数据库，数据库在每个MON节点上的默认位置是`/var/lib/ceph/$fsid/mon.$host/store.db`，不建议更改数据库的位置。

随着时间的推移，数据库可能会变大，运行`ceph tell mon.$id compact`命令用于压缩数据库以提高性能，另外，将mon_compact_on_start配置设置为true，以便在每次daemon启动时压缩数据库:

```bash
[ceph: root@host1 /]# ceph config set mon mon_compact_on_start true 
```

## 集群的身份验证

Ceph默认使用Cephx协议在Ceph组件之间进行加密身份验证，并使用共享密钥进行身份验证，默认启用cephx

```bash
[ceph: root@host1 /]# ceph config get mon auth_service_required
cephx
[ceph: root@host1 /]# ceph config get mon auth_cluster_required
cephx
[ceph: root@host1 /]# ceph config get mon auth_client_required
cephx, none
```

使用ceph auth命令创建、查看和管理集群密钥，使用ceph-authtool命令创建keyring文件，部署⼯具还会在 /etc/ceph ⽬录中创建 client.admin ⽤⼾，管理密钥环默认存储在 /etc/ceph/ceph.admin.client.keyring ⽂件中

```bash
[ceph: root@host1 /]# ceph-authtool --create-keyring /mnt/ceph.mon.keyring --gen-key -n mon. --cap mon 'all *'
creating /mnt/ceph.mon.keyring
[ceph: root@host1 /]# cat /mnt/ceph.mon.keyring 
[mon.]
        key = AQDskiFjbuNaDhAAGD+RKvZgLBdSLw7AKttb1g==
        caps mon = "all *"
```

### 查看现有用户以及其能力、密钥

```bash
[ceph: root@host1 /]# ceph auth list
mds.host2.xiaohui.cn.host1.drgiaf
        key: AQDWcyFj7Y5/CBAAGubJ5L7AFJcmzJ0Hj4lJfA==
        caps: [mds] allow
        caps: [mon] profile mds
        caps: [osd] allow rw tag cephfs *=*
mds.host2.xiaohui.cn.host2.xfigqi
        key: AQDZcyFjYPyvBBAA6bsysWeCEfNiNCZuhhYcOQ==
        caps: [mds] allow
        caps: [mon] profile mds
        caps: [osd] allow rw tag cephfs *=*
mds.host3.xiaohui.cn.host1.misolh
        key: AQDccyFj4AccCRAArWAcm20IocD1RDVyDhe6QA==
        caps: [mds] allow
        caps: [mon] profile mds
        caps: [osd] allow rw tag cephfs *=*
mds.host3.xiaohui.cn.host2.antron
        key: AQDecyFjXSnENxAAtnmmj6jFklP9qbafoF9jEg==
        caps: [mds] allow
        caps: [mon] profile mds
        caps: [osd] allow rw tag cephfs *=*
osd.0
        key: AQARdCFjruIcORAAXrJOqXY707TNfyptcQo85g==
        caps: [mgr] allow profile osd
        caps: [mon] allow profile osd
        caps: [osd] allow *
```

### 导出用户密钥环

```bash
[ceph: root@host1 /]# ceph auth get client.admin -o /tmp/adminkey
exported keyring for client.admin
[ceph: root@host1 /]# cat /tmp/adminkey
[client.admin]
        key = AQB4bCFjrIHZGhAAXI2cM9aPLiPUynHWq6+buA==
        caps mds = "allow *"
        caps mgr = "allow *"
        caps mon = "allow *"
        caps osd = "allow *"
```

### 压缩数据库文件

先查询现有大小

```bash
[root@host1 ~]# du -sch /var/lib/ceph/6dbbdf24-33f1-11ed-84ff-000c29759605/mon.host1.xiaohui.cn/store.db/
61M     /var/lib/ceph/6dbbdf24-33f1-11ed-84ff-000c29759605/mon.host1.xiaohui.cn/store.db/
61M     total
```

将mon 进程启动时压缩选项配置好，并重启mon进程，由于mon进程全部重启会导致集群状态异常，所以重启后要先确认集群状态

发现数据库压缩为60M

```bash
[root@host1 ~]# cephadm shell
Inferring fsid 6dbbdf24-33f1-11ed-84ff-000c29759605
Inferring config /var/lib/ceph/6dbbdf24-33f1-11ed-84ff-000c29759605/mon.host1.xiaohui.cn/config
Using ceph image with id '0912465dcea5' and tag 'v17' created on 2022-08-04 16:09:31 +0000 UTC
quay.io/ceph/ceph@sha256:43f6e905f3e34abe4adbc9042b9d6f6b625dee8fa8d93c2bae53fa9b61c3df1a
[ceph: root@host1 /]# ceph config set mon mon_compact_on_start true
[ceph: root@host1 /]# ceph orch restart mon
Scheduled to restart mon.host1.xiaohui.cn on host 'host1.xiaohui.cn'
Scheduled to restart mon.host2 on host 'host2.xiaohui.cn'
Scheduled to restart mon.host3 on host 'host3.xiaohui.cn' 
[ceph: root@host1 /]# ceph health
HEALTH_OK
[root@host1 ~]# du -sch /var/lib/ceph/6dbbdf24-33f1-11ed-84ff-000c29759605/mon.host1.xiaohui.cn/store.db/
60M     /var/lib/ceph/6dbbdf24-33f1-11ed-84ff-000c29759605/mon.host1.xiaohui.cn/store.db/
60M     total 
```

# 配置集群网络

## 公共网络和集群网络

public网络是所有Ceph集群通信的默认网络，cephadm工具使用第一个MON守护进程IP地址的网络做public网络，新的MON守护进程都部署在public网络中，除非显式地定义了不同的网络

Ceph客户端通过集群的public网络直接向osd发出请求，OSD复制和恢复流量使用public网络，除非为此配置单独的cluster网络，配置单独的cluster网络可以通过减少public网络流量负载和将客户端流量与后端OSD操作流量分离来提高集群性能

执行以下步骤，为单独的集群网络配置节点：

1. 在每个集群节点上配置一个额外的网络接口

2. 在每个节点的新网口上配置相应的cluster网络IP地址

3. 使用cephadm bootstrap命令的--cluster- network选项在集群bootstrap创建cluster网络

当然，如果在bootstrap的时候没有指定，后期也可以修改

## 查询目前网络情况

我们发现就只有一个public网络，所有流量都走这里

```bash
[ceph: root@host1 /]# ceph config get mon public_network
192.168.30.0/24
[ceph: root@host1 /]# ceph config get osd public_network

[ceph: root@host1 /]# ceph config get osd cluster_network

[ceph: root@host1 /]# ceph config get mon cluster_network
```

## 设置集群网络

很明显，mon是需要public网络来同步以及接受客户端请求的，但是osd如果也用这个网络，会有安全问题以及网络负载过高的问题，所以我们打算给他设置一个单独的集群网络，如开头所述，我已经准备好了10.0.0.0/24的第二块网卡，修改网络后需要重启集群生效

```bash
[ceph: root@host1 /]# ceph config set osd cluster_network 10.0.0.0/24
[ceph: root@host1 /]# ceph config set mon cluster_network 10.0.0.0/24
[ceph: root@host1 /]# 
```

以上是为特定的守护进程配置网络，你可以在配置文件中使用以下方式为所有服务配置全局网络

```ini
[global] 
public_network = 192.168.30.0/24, 192.168.40.0/24 
cluster_network = 10.0.0.0/24 
```

验证网络

```bash
[ceph: root@host1 /]# ceph config get osd cluster_network
10.0.0.0/24
[ceph: root@host1 /]# ceph config get mon public_network
192.168.30.0/24
[ceph: root@host1 /]# ceph config get mon cluster_network
10.0.0.0/24
```

```bash
[ceph: root@host1 /]# ceph config dump | grep cluster_network
mon                                             advanced  cluster_network                        10.0.0.0/24                                                                                * 
osd                                             advanced  cluster_network                        10.0.0.0/24                                                                                * 
[ceph: root@host1 /]# 
```

## 启用巨型帧

建议在存储网络上配置网络MTU以支持巨型帧，这可能会提高性能，在集群网口上配置MTU值为9000，以支持巨型帧

同一通信路径下的所有节点和网络设备的MTU值必须相同。对于bound网口，配置bound网口的MTU值后，底层接口继承相同的MTU值 

```bash
[root@host1 ~]# nmcli connection modify 'ens192' 802-3-ethernet.mtu 9000
[root@host1 ~]# nmcli connection down ens192 
Connection 'ens192' successfully deactivated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/2)
[root@host1 ~]# nmcli connection up ens192 
Connection successfully activated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/3)
[root@host1 ~]# 
```

```bash
[root@host1 ~]# ip a s ens192
3: ens192: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9000 qdisc mq state UP group default qlen 1000
    link/ether 00:0c:29:75:96:0f brd ff:ff:ff:ff:ff:ff
    inet 10.0.0.1/24 brd 10.0.0.255 scope global noprefixroute ens192
       valid_lft forever preferred_lft forever
    inet6 fe80::20c:29ff:fe75:960f/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever
[root@host1 ~]# 
```

## 配置网络安全

配置集群网络之后，OSD的数据处理就不再对外开放，将后端OSD流量隔离到自己的网络中可能有助于防止public网络上的数据泄露，但是要注意的是，不要让public和cluster网络之间存在互通的可能性 

## 配置防火墙规则

Ceph OSD和MDS默认绑定的TCP端口范围为6800 ~ 7300。要配置不同的范围，需要修改ms_bind_port_min和ms_bind_port_max设置

下表列出了Quincy的默认端口

| 服务名称                      | 端口                               | 描述                                                                                                                               |
| ------------------------- | -------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- |
| Monitor (MON)             | 6789/TCP (msgr),3300/TCP (msgr2) | Ceph集群内的通信                                                                                                                       |
| OSD                       | 6800-7300/TCP                    | 每个OSD使用3个端口:1个用于通过public与客户端和MONs通信；一个用于通过cluster网络向其他osd发送数据，如果前者不存在，则通过public网络发送数据；另一个用于在cluster网络或public网络上交换心跳数据包，如果前者不存在的话 |
| Metadata Server(MDS)      | 6800-7300/TCP                    | 与Ceph元数据服务器通信                                                                                                                    |
| Dashboard/Manager(MGR)    | 8443/TCP                         | 通过SSL与Ceph管理器仪表板通信                                                                                                               |
| Manager RESTful Module    | 8003/TCP                         | 通过SSL与Ceph Manager RESTful模块通信                                                                                                   |
| Manager Prometheus Module | 9283/TCP                         | 与Ceph Manager Prometheus插件的通信                                                                                                    |
| Prometheus Alertmanager   | 9093/TCP                         | 与Prometheus Alertmanager服务的通信                                                                                                    |
| Prometheus Node Exporter  | 9100/TCP                         | 与Prometheus Node Exporter守护进程通信                                                                                                  |
| Grafana server            | 3000/TCP                         | 与Grafana服务沟通                                                                                                                     |
| Ceph Object Gateway (RGW) | 80/TCP                           | 与Ceph RADOSGW通信。如果client.rgw配置段为空，Cephadm使用默认的80端口                                                                               |
| Ceph iSCSI Gateway        | 9287/TCP                         | 与Ceph iSCSI网关通信                                                                                                                  |

MONs总是在public网络上运行。为了保证的MON节点安全可以启用防火墙规则，需要配置带有public接口和public网络IP地址的规则。可以手动将端口添加到防火墙规则中

```bash
[root@host1 ~]# firewall-cmd --zone=public --add-port=6789/tcp
[root@host1 ~]# firewall-cmd --zone=public --add-port=6789/tcp --permanent 
```

还可以通过将ceph-mon服务添加到防火墙规则中来保护MON节点

```bash
[root@host1 ~]# firewall-cmd --zone=public --add-service=ceph-mon 
[root@host1 ~]# firewall-cmd --zone=public --add-service=ceph-mon --permanent 
```

为了配置cluster网络，osd需要同时配置public网络和cluster网络的规则，客户端通过public连接osd, osd之间通过cluster网络通信

为了保护OSD不受防火墙规则的影响，需要配置相应的规则网口和IP地址

```bash
[root@node ~]# firewall-cmd --zone=public --add-port=6800-7300/tcp 
[root@node ~]# firewall-cmd --zone=public --add-port=6800-7300/tcp --permanent 
```

也可以通过在防火墙规则中添加ceph服务来保护OSD的安全

```bash
[root@node ~]# firewall-cmd --zone=public --add-service=ceph 
[root@node ~]# firewall-cmd --zone=public --add-service=ceph --permanent
```
